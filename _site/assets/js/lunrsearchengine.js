
var documents = [{
    "id": 0,
    "url": "http://localhost:4000/404.html",
    "title": "404",
    "body": "404 Page does not exist!Please use the search bar at the top or visit our homepage! "
    }, {
    "id": 1,
    "url": "http://localhost:4000/about/",
    "title": "About",
    "body": "  My name is Jay Grossman. I have worn a lot of different hats over the years &ndash; husband, father, son, entrepreneur, software developer, manager, mentor, coach, student, tag team partner, etc. All have shaped me in different ways and this site is where I plan to share those experiences on this site. ^ Here is a picture of my family in 2016 - Cindy, Jack, me, and Jenna. Ever since I was 9 years old, I have been fascinated by how people behave related to collecting things and how they decide what they are willing to pay for it. At the time, I was in love with collecting baseball cards (as were many boys in the mid 1980&prime;s). So I would spend more time than reasonable trying to understand why a card that one person would buy for $1 could then be sold to another person for 50 cents or $10. Or why my friend who was a Mets fan was willing to pay 10x more for a Dwight Gooden rookie card than I would. It sparked interests into math, statistics, and basic economics principles. I was completely hooked. My interest in this problem space only grew as I attended college and beyond. That drew me to learning how to program so I could build software to further test my theories and find solutions. After searching for answers and not finding the solutions I was looking for, I have built some interesting projects that I have shared:      Projects:   SportsCollectors. NetDescription:Largest Community for Sports Autograph Collectors. 45K+ members and 10K+ subscribersTimeframe:2000-PresentCollectz. com: Description:Research and Arbitrage Platform for collectibles categories. Timeframe:2012-PresentSportsCardDatabase: Description:Sports card reference provides market values, comparison shopping, deal finder (arbitrage), and collection tracking functions. Timeframe:2009-PresentAuctionCapture: Description:Comparison shopping and product recommendation engine (leveraged item based collaborative filtering) for auction items. &nbsp;Timeframe:2006-2013Hipstir: Description:Social Network and lifestyle site with over 600k+ users. We were the first to offer games, events, and other cool features. Timeframe:2003-2008TrainingMe: Description:Online learning and courseware solution that allowed users to run their own curriculums. Timeframe:2002-2004BigTrades: Description:Online collection management and trading platform for collectibles. Timeframe:2001-2003Personal Auction Manager: Description:Tool set for sellers to list and track product offerings across a variety of marketplaces and formats. Timeframe:2001-2002CodeAndHacks: Description:Code repository and document management platform for development solutions. Contained a custom content management solution. Timeframe:2000-2008&nbsp; All of these projects were built from the ground up and each was created to fill specific need that I could not find in other sites (at that time period). Each has delivered different but valuable opportunities to explore different problem spaces. These have allowed me to get hands-on experience for so many interesting technical and business topics&hellip;. customer service, predictive analytics, collective intelligence, market pricing algorithms, gauging demand, real time data/application integration, data mining, database design, implementing a search platform, content/document management, service based architecture, task automation, marketing, public relations, etc. Through these projects I&rsquo;ve met so many interesting, smart, passionate, and fun people &ndash; with the hope of helping them solve some of their problems/voids along the way. &nbsp;Thanks for taking a look and hopefully some of the topics on this blog will be worth the read for some of you.       TLDR;techie / entrepreneur that&nbsp;enjoys:&nbsp;1) my kids + awesome wife&nbsp;2) building software projects/products&nbsp;3) digging for gold in data sets&nbsp;4) my various day jobs&nbsp;5) playing sports (baseball + hoops)&nbsp;6) rooting for my Boston sports teams:&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Also find me on:&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; "
    }, {
    "id": 2,
    "url": "http://localhost:4000/categories",
    "title": "Categories",
    "body": ""
    }, {
    "id": 3,
    "url": "http://localhost:4000/favorites",
    "title": "Favorite Posts",
    "body": ""
    }, {
    "id": 4,
    "url": "http://localhost:4000/",
    "title": "Home",
    "body": "      Latest Posts:                                                                                                     New JayGrossman. com              :       Welcome to the new version of my blog!!! I migrated over 10+ years of content to a platform and a fresh (cleaner) look. :                                                                               Jay Grossman                21 Oct 2023                                                                                                                                     Creating Singer Target to Send Data to Web Endpoint              :       Please Note: If you are new to Singer, you may want to check out my last post Creating Singer Tap to Capture Ebay Completed Items. It provides a high level. . . :                                                                               Jay Grossman                30 Jun 2023                                                                                                                                     Creating Singer Tap to Capture Ebay Completed Items              :       What is Singer The Singer specification bills itself as  the open-source standard for writing scripts that move data . PipelineWise and Meltano are popular open source platforms that use the Singer. . . :                                                                               Jay Grossman                19 Jun 2023                                                                                                                                     Modern Data Stack in a box (all Open Source)              :       This week I participated in a hackathon for fun with a couple of friends. There were 3 rules for each of us::                                                                               Jay Grossman                14 Jun 2023                                                                                                                                     The 4 Analytics Questions of Subscription Ecommerce              :       I have spent over 20 years building my own subscription based service (SportsCollectors. Net) and working for companies with subscription offerings (Dell, Jupiterimages, Weight Watchers, Rent the Runway,&nbsp;ElysiumHelath). While the business. . . :                                                                               Jay Grossman                07 Jun 2023                                                                                                                                     Optimizing spend in Snowflake              :       Since about 2016, I have introduced Snowflake at two of my day jobs (Rent the Runway and Luma Financial Technologies) and have witnessed the platform's considerable evolution. &nbsp; As with any. . . :                                                                               Jay Grossman                25 Apr 2023                                               &laquo; Prev       1        2        3        4        5        6        7        8      Next &raquo; "
    }, {
    "id": 5,
    "url": "http://localhost:4000/robots.txt",
    "title": "",
    "body": "      Sitemap: {{ “sitemap. xml”   absolute_url }}   "
    }, {
    "id": 6,
    "url": "http://localhost:4000/page2/",
    "title": "Home",
    "body": "{% if site. show_featured == true %}{% if page. url == “/” %}       Featured:       {% for post in site. posts %}    {% if post. featured == true %}      {% include featuredbox. html %}    {% endif %}  {% endfor %}  {% endif %}{% endif %}       Latest Posts:         {% for post in paginator. posts %}    {% include postbox. html %}    {% endfor %}    {% include pagination. html %}"
    }, {
    "id": 7,
    "url": "http://localhost:4000/page3/",
    "title": "Home",
    "body": "{% if site. show_featured == true %}{% if page. url == “/” %}       Featured:       {% for post in site. posts %}    {% if post. featured == true %}      {% include featuredbox. html %}    {% endif %}  {% endfor %}  {% endif %}{% endif %}       Latest Posts:         {% for post in paginator. posts %}    {% include postbox. html %}    {% endfor %}    {% include pagination. html %}"
    }, {
    "id": 8,
    "url": "http://localhost:4000/page4/",
    "title": "Home",
    "body": "{% if site. show_featured == true %}{% if page. url == “/” %}       Featured:       {% for post in site. posts %}    {% if post. featured == true %}      {% include featuredbox. html %}    {% endif %}  {% endfor %}  {% endif %}{% endif %}       Latest Posts:         {% for post in paginator. posts %}    {% include postbox. html %}    {% endfor %}    {% include pagination. html %}"
    }, {
    "id": 9,
    "url": "http://localhost:4000/page5/",
    "title": "Home",
    "body": "{% if site. show_featured == true %}{% if page. url == “/” %}       Featured:       {% for post in site. posts %}    {% if post. featured == true %}      {% include featuredbox. html %}    {% endif %}  {% endfor %}  {% endif %}{% endif %}       Latest Posts:         {% for post in paginator. posts %}    {% include postbox. html %}    {% endfor %}    {% include pagination. html %}"
    }, {
    "id": 10,
    "url": "http://localhost:4000/page6/",
    "title": "Home",
    "body": "{% if site. show_featured == true %}{% if page. url == “/” %}       Featured:       {% for post in site. posts %}    {% if post. featured == true %}      {% include featuredbox. html %}    {% endif %}  {% endfor %}  {% endif %}{% endif %}       Latest Posts:         {% for post in paginator. posts %}    {% include postbox. html %}    {% endfor %}    {% include pagination. html %}"
    }, {
    "id": 11,
    "url": "http://localhost:4000/page7/",
    "title": "Home",
    "body": "{% if site. show_featured == true %}{% if page. url == “/” %}       Featured:       {% for post in site. posts %}    {% if post. featured == true %}      {% include featuredbox. html %}    {% endif %}  {% endfor %}  {% endif %}{% endif %}       Latest Posts:         {% for post in paginator. posts %}    {% include postbox. html %}    {% endfor %}    {% include pagination. html %}"
    }, {
    "id": 12,
    "url": "http://localhost:4000/page8/",
    "title": "Home",
    "body": "{% if site. show_featured == true %}{% if page. url == “/” %}       Featured:       {% for post in site. posts %}    {% if post. featured == true %}      {% include featuredbox. html %}    {% endif %}  {% endfor %}  {% endif %}{% endif %}       Latest Posts:         {% for post in paginator. posts %}    {% include postbox. html %}    {% endfor %}    {% include pagination. html %}"
    }, {
    "id": 13,
    "url": "http://localhost:4000/new-jaygrossman.com/",
    "title": "New JayGrossman.com",
    "body": "2023/10/21 - Welcome to the new version of my blog!!! I migrated over 10+ years of content to a platform and a fresh (cleaner) look. The Old Blog: In 2012, I was doing a lot more development in ASP. NET/C#, so I decided to move my blog to a platform that was more familiar to me. I set up my blog with blogengine. io that I self-hosted. At the time, it had some decent features and I was able to modify a theme I liked (design shown below): There were some things I didn't like about my set up: It was a pain to set up and maintain. It had become slower. The WYSIWYG editor made some really ugly HTML. The widgets were not flexible and easy to customize. (compiling old . NET to make changes was a pain). A white background would be better for reading content. Moving on to something more modern: I remembered that my friend Collin Meyers had mentioned that he set up a blog hosted with Github Pages.  So I took a look and it looked pretty straight forward to set up. Github Pages natively supports the ruby based blog framework - Jekyll. I had never used it before, but it was very easy to set up locally with rbenv, install the gems and get it working. There were some things I immediately liked: There were plenty of themes to choose from (free and paid). I chose the Mediumish because it was clean and simple. The scripting is easy to customize the theme. There are tons of examples on google and ChatGPT. I can use both HTML and markdown. There are many great tutorials online that will walk you through setting up a blog with Github Pages and Jekyll. I found this one to be the most helpful: Build A Blog With Jekyll And GitHub Pages. Here is a nice video I watched by Spencer Pao: The new platfrom will certaily make it easier for me to post new content. Now I just need to find time and motivation to add more to this blog consistently. "
    }, {
    "id": 14,
    "url": "http://localhost:4000/creating-singer-target-to-send-data-to-web-endpoint/",
    "title": "Creating Singer Target to Send Data to Web Endpoint",
    "body": "2023/06/30 -   Please Note:  If you are new to Singer, you may want to check out my last post Creating Singer Tap to Capture Ebay Completed Items. It provides a high level background of the specification and how taps &amp; targets work together.   The Challenge: Last week I created a walk through of&nbsp;Creating Singer Tap to Capture Ebay Completed Items. While it's great to capture data, it's not overly useful without persisting the data to a target destination. There are some useful targets&nbsp;posted on signer. io&nbsp;and&nbsp;posted on meltano&nbsp;for writing to a nice variety of standard destinations (databases, cloud data warehouses, S3, csv, json, etc. ). However my site has an API and I could not find a target to send to a web endpoint. I want to able to pipe data from a Singer Tap to my own API endpoints: Creating target-web_endpoint: Code for this Singer Target is posted on github (click on image below): Project Scope: So the goal is to create a Singer target that will allow us to take data piped from a Singer tap and send it to a web endpoint (via a HTTP GET or HTTP POST). Below is a visual illustration: This target must be able to support the following requirements: Sending record data (piped from a tap) to a url endpoint via HTTP GET or HTTP Post. Configuration of basic auth credentials and HTTP Headers for HTTP Post method. &nbsp;Configuration to map source data field names to target system's data field names. &nbsp;Configuration to specify additional properties (with static values) to send to endpoint. &nbsp;Configuration to specify VERY BASIC filter rules based on the record values. &nbsp;Helpful links to get background on Developing Singer Targets: Singer provides&nbsp;getting started docs&nbsp;on creating targets. &nbsp;Meltano's slack&nbsp;has a dedicated channel&nbsp;#singer-target-development&nbsp;for help developing targets&nbsp;Setting up development for Singer Target: In order to develop a tap, we need to install the Singer library: 1pip install singer-pythonNext we'll install&nbsp;cookiecutter&nbsp;and download the&nbsp;target template&nbsp;to give us a starting point: 1234pip install cookiecuttercookiecutter https://github. com/singer-io/singer-targer-template. gitproject_name [e. g. 'tap-facebook']: target-web_endpointpackage_name [target_web_endpoint]:target_web_endpointConfiguration file for the Target: There is a template you can use at&nbsp;config. json. example, just copy it to&nbsp;config. json&nbsp;in the repo root and update the following values: 12345678910111213141516171819202122232425{   method  :  POST ,   url :  https://api. some_web_site. com/lisitngs/ ,   username :  my_username ,   password :  my_password ,   post_headers  : {     Content-Type :  application/x-www-form-urlencoded   },   property_mapping : {     field1 : {  target_field_name :  target_field_1 },     field2 : {  target_field_name :  target_field_2 },     field3 : {  target_field_name :  target_field_3 },     field4 : {  target_field_name :  field_4 },     field5 : {  target_field :  field_5 }  },   additional_properties : {     system_id : 12,     special_key :  0cf18148-1687-11ee-be56-0242ac120002   },   filter_rules : {     field1 : {  type :  equals ,  value : true },     field2 : {  type :  not_equals ,  value :  123  },     field3 : {  type :  is_empty ,  value : false }  }} VariableDescriptionmethodmethod for calling url (GET or POST), default is GETurlendpoint url&nbsp;REQUIREDusernameuser name for basic auth (only for POST)post_headersdict of headers to pass (only for POST)property_mappingdefine the properties received from tap to be sent to the endpoint. You can update the target property names)additional_propertiesdefine additional properties with hard coded values that will be sent to the endpointfilter_rulesconfigure rules to only include records when matching all criteria. &nbsp; Notes about filter_rules: 1. Records will be sent to the endpoint only when they are valid for all the configured rules. 2. You can only identify one rule for each field. 3. There are 5 supported types of rules: Rule TypeDescriptionequalsthe field's value must equal the configured valuenot_equalsthe field's value must not equal the configured valuecontainsthe field's value must contain the configured valuenot_containsthe field's value must not contain the configured valueis_emptyif true, the field's value must not be empty. if false, the field's value must be emptySetting up to run the Target: Let's create a virtual environment to run our tap within: 12345678cd target-web_postpython3 -m venv ~/. virtualenvs/target-web_endpointsource ~/. virtualenvs/target-web_endpoint/bin/activategit clone git@github. com:jaygrossman/target-web_endpoint. gitcd target-web_endpointpip install requestspip install -e . deactivate We can pipe the output of a tap to our target with the following command (after the | symbol): 1run_your_tap | ~/. virtualenvs/target-web_endpoint/bin/target-web_endpointEXAMPLE: Running Tap-Csv + Target-web_endpoint&nbsp;: I created a sample_data folder in the project's github repo that includes: sample_data. csv file contains a github keyword searchtap-csv. config. json file contains config for the tap-csvtarget-web_endpoint. config. json file contains config for the target-web_endpointCalling this thread will try to search github (https://github. com/search) via a HTTP GET request with the keywords supplied in sample_data. csv. Install tap-csv: 1234python3 -m venv ~/. virtualenvs/tap-csvsource ~/. virtualenvs/tap-csv/bin/activatepip install git+https://github. com/MeltanoLabs/tap-csv. gitdeactivateWe can run tap-csv piped to our target-web_endpoint with the following command: 1~/. virtualenvs/tap-csv/bin/tap-csv --config sample_data/tap-csv. config. json | ~/. virtualenvs/target-web_endpoint/bin/target-web_endpoint --config sample_data/target-web_endpoint. config. jsonThe command outputs the following: 2023-06-29 23:17:10,957 | INFO&nbsp; &nbsp; &nbsp;| tap-csv&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; | Beginning full_table sync of 'seaches'. . . 2023-06-29 23:17:10,957 | INFO&nbsp; &nbsp; &nbsp;| tap-csv&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; | Tap has custom mapper. Using 1 provided map(s). 2023-06-29 23:17:10,957 | INFO&nbsp; &nbsp; &nbsp;| singer_sdk. metrics&nbsp; &nbsp;| METRIC: { type :  timer ,  metric :  sync_duration ,  value : 0. 000225067138671875,  tags : { stream :  searches ,  context : {},  status :  succeeded }}2023-06-29 23:17:10,957 | INFO&nbsp; &nbsp; &nbsp;| singer_sdk. metrics&nbsp; &nbsp;| METRIC: { type :  counter ,  metric :  record_count ,  value : 1,  tags : { stream :  searches ,  context : {}}}url: https://github. com/search?q=tap-ebaycompleted, response: &lt;Response [200]&gt;{ bookmarks : { searches : {}}} &nbsp; "
    }, {
    "id": 15,
    "url": "http://localhost:4000/creating-singer-tap-to-capture-ebay-completed-items/",
    "title": "Creating Singer Tap to Capture Ebay Completed Items",
    "body": "2023/06/19 - What is Singer: The Singer specification bills itself as  the open-source standard for writing scripts that move data . PipelineWise and Meltano are popular open source platforms that use the Singer specification to accommodate ingest and replication of data from various sources to various destinations. Singer describes how data extraction scripts&mdash;called  taps &nbsp;&mdash;and data loading scripts&mdash;called  targets &mdash; should communicate, allowing them to be used in any combination to move data from any source to any destination. Send data between databases, web APIs, files, queues, and just about anything else you can think of. ETL Pipeline in Singer, image credit: panoply Completed Items on eBay: For over 20 years, eBay has allowed users to search by keyword for auctions and Buy It Now listings that have recently ended. This is very helpful to provide buyer and sellers with a directional idea of how much comparable items sell for. On the left side of any search page (near the bottom of the search results), users have the ability to filter by Completed Items as shown below: The screen below shows a search for the search term  iphone 14 : URL:&nbsp;https://www. ebay. com/sch/i. html?LH_Complete=1&amp;_nkw=iphone+14 The Challenge: For a long time I've wanted a standard method to be able to capture data about items that have sold on eBay and have an easy + repeatable way to save the data (to files, a database, etc. ). I have pieced together various scripts in different languages (python, powershell, C#, php) at different times to accomplish this, but I wanted it to be based off a more standard and extensible framework. eBay offers&nbsp;developer APIs&nbsp;to query their data which I have used. While I could use the API's&nbsp;findCompletedItems&nbsp;end point, it is a far more straight forward implementation and faster learning opportunity for me to use some common python libraries to get the data from eBay's public web site. &nbsp;tap-ebaycompleted: Code for this Singer Tap is posted on github (click on image below): Project Scope: So the goal is to create a Singer tap that will allow us to generate out properly formatted JSON data with the details of the listing, that can be consumed by a Singer target (such as writing to a . csv file, an API or PostGres database). Below is a visual illustration: Helpful links to get background on Developing Singer Taps: Singer provides&nbsp;getting started docs&nbsp;on creating taps. &nbsp;Meltano provides code samples for developing taps&nbsp;There are 2 functions in the singer python library&nbsp;for my simple tap that we must care about:singer. write_schema - responsible for defining the JSON schema that data will be outputtedsinger. write_records - responsible for outputting each record (data row)&nbsp;: Setting up development for Singer Tap: In order to develop a tap, we need to install the Singer library: 1pip install singer-python&nbsp;Next we'll install cookiecutter and download the tap template to give us a starting point: 1234pip install cookiecuttercookiecutter https://github. com/singer-io/singer-tap-template. gitproject_name [e. g. 'tap-facebook']: tap-ebaycompletedpackage_name [tap_ebaycompleted]:Defining the schema: The JSON schema defines the structure that the data will be outputted by the tap. Below I am writing a small inline python function to do so:&nbsp; 1234567891011121314151617def get_schema():schema = {   properties : {     search_term : { type :  string },     title : { type :  string },     price : { type :  string },     bids : { type :  string },     buy_it_now : { type :  boolean },     condition : { type :  string },     image : { type :  string },     link : { type :  string },     end_date : { type :  string }     has_sold : { type :  boolean }     id : { type :  string }    }  }return schemaWe later call this function to create the schema object: 12schema = get_schema()singer. write_schema( completed_item_schema , schema,  id )Getting data from eBay's Completed Items Search Results: In python, there are a few different libraries to parse the contents of a web page. In this tap, I use: requests&nbsp;for getting the HTML of the page&nbsp;BeautifulSoup 4&nbsp;for parsing the elements on the pageWhile I added some extra logic in the actual tap, below will provide the basic idea of how we can parse the page if you are new to these libraries: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152search_term = 'iphone 13'url =  https://www. ebay. com/sch/i. html?LH_Complete=1&amp;_sop=13&amp;_nkw={search_term} response = requests. get(url)html_content = str(response. content)soup = BeautifulSoup(html_content,  html. parser )# Each completed item is within a &lt;li&gt; tag with class=s-item s-item__pl-on-bottomlistings = soup. find_all( li , class_= s-item s-item__pl-on-bottom )# Iterate over completed listingsfor listing in listings:  title = listing. find( div , class_= s-item__title ). text  price = listing. find( span , class_= s-item__price ). text  condition = listing. find( span , class_= SECONDARY_INFO ). text  image= listing. find( img )['src']  link = listing. find( a , class_= s-item__link )['href']  id = link[0:link. index( ? )]. replace( https://www. ebay. com/itm/ ,   )  bids =     try:    bids = listing. find( span , class_= s-item__bids s-item__bidCount ). text  except:    bids =     buy_it_now = False  try:    if listing. find( span , class_= s-item__dynamic s-item__buyItNowOption ). text ==  Buy It Now :      buy_it_now = True  except:    buy_it_now = False  has_sold = False  try:    if listing. find( div , class_= s-item__title--tag ). find( span , class_= clipped ). text ==  Sold Item :      has_sold = True      end_date=listing. find( div , class_= s-item__title--tag ). find( span , class_= POSITIVE ). text    else:      end_date = listing. find( div , class_= s-item__title--tag ). find( span , class_= NEGATIVE ). text  except:    end_date=    record = {     search_term : search_term,     title : title,     price : price,     condition : condition,     image : image,     link : link,     id : id,     bids : bids,     buy_it_now : buy_it_now,     end_date : end_date,     has_sold : has_sold  } We can then pass the schema ( completed_item_schema ) that we defined earlier and the record to singer. write_records: 1singer. write_records( completed_item_schema , [record])Building and Configuring the Tap: The template created a directory named tap-ebaycompleted. Since this is a very simple tap, I removed the contents and created a single file called __init__. py. In the main function, I provided the logic specified in the sections above with a little bit of additional logic. Create a configuration file There is a template you can use at config. json. example, just copy it to config. json in the repo root and update the following values: 123456{   search_terms : [ iphone 13 ,  iphone 14 ],   page_size : 240,   min_wait : 2. 1,   max_wait : 4. 4} VariableDescriptionsearch_termslist of terms that the tap will search for REQUIREDpage_sizenumber of records to return (values can be 240,120,60), default is 120min_waitminimum amount of time between searches, default is 2 secondsmax_waitmaximum amount of time between searches, default is 5 seconds&nbsp; Running the Tap: Let's create a virtual environment to run our tap within: 123456789cd tap-ebaycompletedpython3 -m venv ~/. virtualenvs/tap-ebaycompletedsource ~/. virtualenvs/tap-ebaycompleted/bin/activategit clone git@github. com:jaygrossman/tap-ebaycompleted. gitcd tap-ebaycompletedpip3 install requestspip3 install BeautifulSoup4pip3 install . deactivate We can run our tap with the following command: 1~/. virtualenvs/tap-ebaycompleted/bin/tap-ebaycompleted -c config. jsonBelow is a sample record representing a completed item: 123456789101112131415161718{   type :  RECORD ,  stream :  completed_item_schema ,  record : {   search_term :  iphone 13 ,    title :  Apple iPhone 13 - 128GB - Midnight ,    price :  $411. 00 ,    condition :  Pre-Owned ,    image :  https://shorturl. at/bqrsK ,    link :  https://www. ebay. com/itm/314645218752 ,    id :  314645218752 ,    bids :   ,    buy_it_now : false   end_date :  Jun 21, 2023    has_sold : True  }}Running the Tap with a Target to Export the Data to a . csv File: Install target-csv: 1234python3 -m venv ~/. virtualenvs/target-csvsource ~/. virtualenvs/target-csv/bin/activatepip3 install target-csvdeactivateWe can run our tap piped to target-csv with the following command: 12~/. virtualenvs/tap-ebaycompleted/bin/tap-ebaycompleted -c config. json |~/. virtualenvs/target-csv/bin/target-csv Below is the first couple of lines of the . csv file the command generated: search_term,title,price,condition,image,link,id,bids,buy_it_now,end_date,has_sold iphone 13,Apple iPhone 13 - 128GB - Midnight (Verizon),$170. 00,Parts Only,https://i. ebayimg. com/thumbs/images/g/AKkAAOSw55lkhLVt/s-l300. jpg,https://www. ebay. com/itm/225623521919,225623521919,0 bids,False, Jun 21, 2023 ,True Conclusions and What's Next: You should not consider this a production quality tap. eBay often changes the format of the HTML on their search results page and this Tap will need to be updated when this occurs. &nbsp;&nbsp;The&nbsp;tap-ebay&nbsp;project (developed by Drew Banin of Fishtown/dbt) is well done. It uses the fulfillment API to allow a seller to query for their orders received via eBay's marketplace. &nbsp; I may refactor this tap using some of Drew's concepts. Singer is very robust and powerful. It has pretty slick support for Authentication to data source and State management that I did not need in this very simple idempotent example. This python code only returns the first page of results for the provided search terms. It would be beneficial to add in support to iterate through multiple pages of results. Update 2023-06-27: Added support to configure the maximum search result pages to capture records from. I'd like to extend the tap with the option of retrieving the list of search terms getting pulled from web url or an API call. Update 2023-06-28:&nbsp;Added support to configure a public url where search terms and corresponding SKUs can be fed in from. I will probably build a new Singer target to persist the data from this tap&nbsp;to my private API . And then I can orchestrate the pipeline on a cron with either Dagster or try out Pipelinewise. Update 2023-06-30:&nbsp;Created a configurable singer target with blog post at&nbsp;Creating Singer Target to Send Data to Web Endpoint&nbsp;&nbsp; "
    }, {
    "id": 16,
    "url": "http://localhost:4000/modern-data-stack-in-a-box/",
    "title": "Modern Data Stack in a box (all Open Source)",
    "body": "2023/06/14 - This week I participated in a hackathon for fun with a couple of friends. There were 3 rules for each of us: Upgrade something you are unhappy with in a side projectTry some new piece of techDocument what you didMy project: Over the past 10 or so years, I have had a lot of fun running&nbsp;CollectZ&nbsp;- Research and Arbitrage Platform for collectibles categories. &nbsp; TL;DR - The platform informs what undervalued collectibles I should consider buying and when + where + at what price I should sell my inventory. If you are trying to run arbitrage (exploiting inefficiencies between marketplaces) effectively with physical (non-liquid) assets, it is rather important to be able to have a pretty diverse data set and be able do analysis + reporting + run models across a variety of features. &nbsp; My problem: As I went along building out my platform, I had data sitting in different diverse repositories and in many formats. Some of the exploration and reporting tools I cobbled together over the years suffered because they were: fragile to keep running (things broke unexpectedly)hard to troubleshoot for data lineage issuesperforming slowly with more advanced queries (based off relational databases and file stores)labor intensive to extend&nbsp;lacking the general features I wanted&nbsp;The tech debt became more than just noticeable, as it was impacting the effectiveness of the platform and more importantly effectiveness of my trades. &nbsp; Requirements for something better: Before I go about building anything, it is a really good idea to write down some requirements. &nbsp; &nbsp; When thinking about my reporting and data exploration needs for CollectZ, I had some requirements: I want a fully open source (free) stack that I can configure to run locally on my laptop and could be run in a cloud provider (likely AWS). I want to use dbt to build my modelsI want relatively a fast centralized data store (preferably columnar based)I want a way to keep the reporting sync'd regularly from production / source systemsI want an off the shelf tool for self service data exploration and BIThe Data: For this exercise, I am using a subsection of recent transaction and valuation data for Sports Cards category. &nbsp; I have exported data from 7 tables into . csv files and posted them to S3. Below is a relationship diagram source data model we will be working with: TableDescriptionRecord CountItemReference information about a sports card in our catalog116,459&nbsp;ItemValueWeekly calculated values for each item in different conditions5,746,066CardSetThe sport, year and name for each set associated with the card691SportThe sport associated with the card set4ItemTransactionRecent sales for selected items3,175,362ActiveItemItems available for sale4,489,085SourceSources of data for ItemTransaction and ActiveItem12  Please Note:  I am not able to share actual Collectz data on my blog and how we get this (let's assume there are lots of fun data engineering pipelines at work here).   Solution Architecture: I have run data teams that build out data products for my day job, so I knew I could streamline and modernize on some best practice tooling. &nbsp; The components of my stack: DuckDB&nbsp;- our local data warehouseAirbyte&nbsp;- for populating data into DuckDBdbt&nbsp;- for building models for analysisMetabase&nbsp;- BI toolJupyter&nbsp;- for ad hoc analysis via NotebooksDuckDB: I have a good amount of experience with commercial cloud data warehouses (Snowflake, BigQuery, DataBricks). While they are great products, they are all pretty expensive and I don't have budget for a personal project like CollectZ. I wanted something free + open source that I could run locally for development and later in AWS for production. My first thought was to go with Postgres, but I wasn't overly excited to manage a relational database (index, keys, etc. ) and I like columnar databases for analytics. I also thought about Druid, but that would involve some overhead to set up and surprisingly doesn't yet have dbt integration. &nbsp; My friend Rob told me about how DuckDB offered much of the functionality I liked in Snowflake, but it was super easy to manage and would run locally on my laptop. DuckDB is an in-process SQL OLAP database management system. AND I really like the idea of their tagline: All the benefits of a database, none of the hassle. After playing around with it, I found the main points and features of DuckDB? Simple, clean install, with very little overhead. &nbsp;Feature-rich with SQL, plus CSV and Parquet support. &nbsp;In-memory option, high-speed, parallel processing. &nbsp;Open-source. &nbsp;DuckDB isn&rsquo;t meant to replace MySQL, Postgres, and the rest of those relational databases. In fact, they tell you DuckDB is very BAD at &ldquo;High-volume transactional use cases,&rdquo; and &ldquo;Writing to a single database from multiple concurrent processes. &rdquo; This 10 minute Youtube video from Ryan Boyd does a good job with more background on DuckDB:  Installation Instructions: Reference from https://duckdb. org/docs/installation/ brew install duckdbAirbyte: I have implemented&nbsp;FiveTran&nbsp;at 2 of my day jobs (ElysiumHealth and Luma), and it is a very easy to configure tool to set up pipelines to sync data from various sources into a data warehouse. Overall it works well if you do not have low latency requirements (it is great for hourly syncs), but it is volume based and can be quite expensive once you go past syncing 500,000 records in a month on their&nbsp;new free tier offering. For this project, I wanted to try the closest Open Source equivalent I could find - Airbyte. &nbsp;  Please Note:  The latest version or Airbyte 0. 443, only supports DuckDB 0. 6. 1 (version 39). All the other tools in this post I am configuring are using DuckDB 0. 8. 0 (which launched within the past month), so I would not be able to use Airbyte in this full stack until its DuckDB version support is aligns with Metabase and dbt. &nbsp;&nbsp;Airbyte is a valuable tool for syncing data (I also tried it syncing to Postgres) and I feel that is worthwhile to share my learnings of how I would/will use it.   Installation Instructions Reference from&nbsp;https://docs. airbyte. com/quickstart/deploy-airbyte/ Install Docker on your workstation (see instructions). Make sure you're on the latest version of docker-compose. Run the following commands in your terminal:git clone https://github. com/airbytehq/airbyte. gitcd airbyte. /run-ab-platform. shOnce you see an Airbyte banner, the UI is ready to go at http://localhost:8000By default, to login you can use: username=airbyte and password=passwordWhen I opened up Docker Desktop, you can see airbyte has 12 dockers it has running:  Configuring a Pipeline in Airbyte&nbsp; 1) The first step is to set up a Source. In this case I am setting up a  File  source that can read from sport. csv in my S3 bucket. I will need to provide my AWS creds as shown below: 2) Next we will need to set up a destination where we can write the data. This&nbsp;link&nbsp;explains how to update the . env file so that a local directory on my machine will be mounted within the airbyte-server docker image.  3) We will need to configure a connection that will schedule running syncs from our source to our destination: 4) We see the result of the connection syncing data to my destination:&nbsp; 5) I checked the DuckDB database and saw the following data synced over:&nbsp; The _airbyte_data column in the screenshot above contains json representing the data in the csv file. This would need to be transformed into a relational format for reporting via a BI tool (such as metabase). dbt support for DuckDB: dbt is a popular tool for allowing developers and analysts to create data models in SQL (and now Python), managing graph dependencies and supporting tests. I have been a user of dbt since 2017 when my team at Rent the Runway first starting use it to standardize large parts of our pipelines. &nbsp; Installation + Set Up Instructions: Reference from&nbsp;https://github. com/jwills/dbt-duckdb: 1) Install dbt and dbt-duckdb: 12pip3 install dbt-corepip3 install dbt-duckdb2) Configure the ~/. dbt/profile. yml file with the following: 1234567collectz:target: devoutputs:  dev:  type: duckdb  path: /path_to_file/collectz. db  threads: 43) Create a new directory&nbsp; and create new dbt project: 123mkdir dbtcd dbtdbt init4) I entered the name for your project&nbsp;collectz&nbsp;when prompted Building models in dbt&nbsp; This post assumes you have working knowledge of how dbt works. If you do not, their&nbsp;quick start&nbsp;is helpful. Since I could not use Airbyte due to version incompatibilities mentioned above, I made simple models that created VIEWS on top of local copies of our 7. csv files. Example is _raw_active_item. sql (it uses the read_csv_auto function to read in a csv) as shown below: 1FROM read_csv_auto('/path_to _data_file/Active_Item. csv')  Please Note:  I tried to use&nbsp;dbt seed&nbsp;which would make the larger datasets persisted as tables, but it was pretty inefficient to copy over millions of rows when DuckDB is designed to efficiently process data in files at request time. I found the read_csv_auto() function performed well on data sets with 8M+ rows.   Now that I have  raw  tables, I created SQL files that would be my BI reporting schema containing the appropriate transformations and aggregations. Metabase: I have used a bunch of different BI / Dashboard tools over the years. I feel like Looker is probably the best of the commercial tools, but set up is time consuming and it has become really expensive. Metabase is an open source alternative that I have found really easy to set up, easy to use, and has some of the advanced features I would want. &nbsp; I found Maz's 6 minute&nbsp;Demo video&nbsp;posted below walks you through the basics from an analyst perspective doing some data exploration and then incorporating it into dashboards:&nbsp;&nbsp; &nbsp; Installation Instructions: install java - I used this link to install Eclispe Temurin:https://www. metabase. com/docs/latest/installation-and-operation/running-the-metabase-jar-file#:~:text=of%20JRE%20from-,Eclipse%20Temurin,-with%20HotSpot%20JVM% java --version openjdk 17. 0. 7 2023-04-18 OpenJDK Runtime Environment Temurin-17. 0. 7+7 (build 17. 0. 7+7) OpenJDK 64-Bit Server VM Temurin-17. 0. 7+7 (build 17. 0. 7+7, mixed mode) Download the&nbsp;JAR file for Metabase OSSDownload the&nbsp;DuckDB driver for metabase&nbsp;run the following commands from command line to create the metabase directory and move the jar files:mkdir ~/metabasemv ~/Downloads/metabase. jar ~/metabasecd ~/metabasemv ~/Downloads/duckdb. metabase-driver. jar ~/metabase/plugins&nbsp;Run the metabase jar file with the following commands:cd ~/metabasejava -jar metabase. jargot to http://localhost:3000 in your browser  Please Note:  To productionize metabase, we will want to review&nbsp;this page&nbsp;with instructions how to:&nbsp; &nbsp; 1. run metabase with a postgres as the data store instead of using the default h2 file. &nbsp; &nbsp; 2. run the java process as a service using systemd&lt;   Data Configuration in Metabase: Once you log into Metabase's Admin section, you set up a connection to register your DuckDB database as shown below: Under the Data Model area, you can define elements of your data model from any registered databases shown below. Here you can set which tables are visible and how tables join together (similar to primary-foreign keys). &nbsp; &nbsp;&nbsp; The Deliverable: So I made some widgets and dashboards that make it easy enough for me to explore my data, quickly see some overviews of products with dashboards, and troubleshoot issues. &nbsp;&nbsp; Data Exploration Example - Finding undervalued items: Since DuckDb doesn't have a nice web based SQLRunner like Snowflake or BigQuery, Metabase made it really easy for me to write a SQL query joining across tables with minimal aggregation + filtering to visualize results. It took me about 2 minutes to write the query below that returns underpriced items: Item Search and Dashboard: I was able to make a search screen where I can search by the year, sport and wildcard on the card's Description. Below I am searching for cards of Nolan Ryan from 1981 (the links take me to card details report): The card details page below shows me: Details about the cardIf I have it in my inventory, where I am selling it, the condition and the priceTime Series of&nbsp;Values (for graded and ungraded categories)Time Series of Volumes of sales with pricingTime Series of Volume of the item available (buy it now format)Where the item is available and for how much Thoughts: For me, this hackathon was a great success. : Metabase had made it easy to more effectively operationalize parts of this business with enhanced reporting and alerting. I now have a much easier way to answer my ad hoc and research questions quickly. I had the opportunity to play with a great new technology - DuckDB. &nbsp; And I got to integrate it with other products I like (dbt, metabase, airbyte). This blog post does a decent job of documenting the problem, solution approach and some implementation details + learnings. Other Learnings / Observations:: It is so much easier to manage running BI reporting off a columnar database vs. a relational database. DuckDB is pretty amazing for side projects if you can deal with the constraints. &nbsp;I would consider adding in an orchestration layer with Dagster. I like the idea of having ingestion, transformation and stopping/starting services controlled by a consistent and testable process. For ingestion, I also considered using the Singer/Meltano&nbsp;target-duckdb&nbsp;project. I also played with Rill Data. It is very cool for analyzing the structure of data files (csv, parquet, etc), but it can not read from DuckDB. Inspiration from:: Use dbt and Duckdb instead of Spark in data pipelinesModern Data Stack in a Box with DuckDBBuild a poor man&rsquo;s data lake from scratch with DuckDB"
    }, {
    "id": 17,
    "url": "http://localhost:4000/four-questions-of-subscription-ecommerce/",
    "title": "The 4 Analytics Questions of Subscription Ecommerce",
    "body": "2023/06/07 - I have spent over 20 years building my own subscription based service (SportsCollectors. Net) and working for companies with subscription offerings (Dell, Jupiterimages, Weight Watchers, Rent the Runway,&nbsp;ElysiumHelath). While the business models, value propositions and customer segments of these companies may be very different, there are similarities I recognized as these companies (all with product market fit) looked to accelerate their growth. From this, the 4 questions above are what analytics teams should spend resources trying to answer. Please Note:The order of these questions is in the context that you have an existing subscription based offering (with existing data available to analyze) you want to optimize+grow. If you are starting a new subscription offering/company, the order would likely be the opposite. So let's dig into each one of these topics: 1) What do you know about your valuable customers?: You are likely in business to serve your customers' needs and/or deliver some value that they are comfortable (or even happy) to pay you for. It helps to understand who are those customers that are most happy and making you money. These folks tend to be your biggest ambassadors (promoters) and support your growth the most. &nbsp; Margin as a measure of value: I've seen different definitions of  Lifetime Value , many revolving around top line revenue. I personally like to start with looking at the amount and the details that make up the total margin a specific user represents to the business. So I start by building a ledger for each user showing the credits (increases to equity account) and debits (decreases to equity account). Let's look at a very simple ledger for a subscription for a content site: DateDebit (money out)&nbsp;&nbsp;Credit (money in)&nbsp;Margin12/03/2018Google AdWords CAC (campaign 892)$2. 00&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-$2. 0001/01/2019&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Subscription (order 1987)$9. 99&nbsp;&nbsp;$7. 9901/01/2019&nbsp;Subscription promo (order 1987)$9. 00&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-$1. 0101/03/2019&nbsp;Monthly operations cost$0. 14&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;-$1. 1501/03/2019&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Monthly affiliate revenue$0. 37&nbsp;-$0. 7802/01/2019&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Subscription Renewal - Term 2$9. 99&nbsp;&nbsp;$9. 2102/03/2019&nbsp;Monthly operations cost$0. 14&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$9. 0702/03/2019&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Monthly affiliate revenue$1. 28&nbsp;&nbsp;$10. 3503/01/2019&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Subscription Renewal - Term 3$9. 99&nbsp;&nbsp;$20. 3403/02/2019&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Pay Per View Event (order 2508)$3. 99&nbsp;&nbsp;$24. 3303/02/2019&nbsp;Royalty for Pay Per View Event$1. 00&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$23. 3303/03/2019&nbsp;Monthly operations cost$0. 15&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$23. 1803/03/2019&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Monthly affiliate revenue$2. 31&nbsp;&nbsp;$25. 4903/15/2019&nbsp;Mid-term cancel - partial refund$5. 00&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$20. 49Totals&nbsp;$17. 43&nbsp;&nbsp;&nbsp;$37. 92&nbsp;&nbsp;$20. 49There's some interesting questions looking at this kind of data can unearth: How does this user's journal entries compare to others?&nbsp;Was their desirable ROI on the $2. 00 AdWords attribution?Was offering a $9. 00 signup promo worthwhile (we won't see margin on user until second month)?&nbsp;Since this user's affiliate revenue is increasing and they bought a Pay Per View Event, should we incentivize renewal?What do your users tell you?: Many companies may ask their users to provide details about themselves with the hope of using this info to provide a better user experience. Here are some examples of the types of industry specific profile information:&nbsp;WeightWatchers has a goal of helping members get healthier - so it may ask about current weight, weight loss goals, lifestyle or dietary preferences, age, location. Rent the Runway has a goal of helping members enjoy fashion options - so it may ask about body type, height, birthdate, location, style preferences. EHarmony has the goal of matching users for relationships - so it has users spend 45 minutes to fill a&nbsp;lengthy questionnaire&nbsp;about their lifestyles and personal preferences. &nbsp;&nbsp;SportsCollectors. net has the goal of helping members enjoy collecting sports autographs - so it may ask about want lists, favorite players/teams, what items they have for trade/sale, their ebay username, year of birth, location. &nbsp;&nbsp;Do your users' actions tell you things?: In addition to the margin profile, I have also invested time in building a user journey (now being marketed as the&nbsp;Activity Schema) -&nbsp; which is a time series of all the actions and communications with this subscriber. A user journey can be represented with the following data elements: UserID&nbsp;Timestamp&nbsp;User_Event&nbsp;Payload&nbsp;12345&nbsp;01/01/2019 00:00:00&nbsp;PAGE_LOAD&nbsp;{ device_type :  desktop ,  marketing_channel :  google_sem ,  url :  http://www. hello. com/about-us/?utm=sem_plan1 }12345&nbsp;01/01/2019 00:01:00&nbsp;PAGE_LOAD&nbsp;{ device_type :  desktop ,  url :  http://www. hello. com/plans }12345&nbsp;01/01/2019 00:02:00&nbsp;PAGE_LOAD&nbsp;{ device_type :  desktop ,  url :  http://www. hello. com/register }12345&nbsp;01/01/2019 00:03:00&nbsp;PAGE_LOAD&nbsp;{ device_type :  desktop ,  url :  http://www. hello. com/login }12345&nbsp;01/01/2019 00:04:00&nbsp;PAGE_LOAD&nbsp;{ device_type :  desktop ,  url :  http://www. hello. com/subscribe }12345&nbsp;01/01/2019 00:04:00&nbsp;PLACE_ORDER&nbsp;{ order_id : 1987,  order_status :  payment_received ,  order_type :  SUBSCRIPTION ,  amount :  0. 99 }12345&nbsp;01/01/2019 00:07:00&nbsp;RECEIVE_EMAIL&nbsp;{ template :  201900101_new_years_cohort ,  list_name :  hello subscribers }12345&nbsp;01/01/2019 00:015:00&nbsp;OPEN_EMAIL&nbsp;{ template :  201900101_new_years_cohort ,  list_name :  hello subscribers }12345&nbsp;01/01/2019 00:16:00&nbsp;PAGE_LOAD&nbsp;{ device_type :  mobile ,  url :  http://www. hello. com/article/hot-investments }12345&nbsp;01/01/2019 00:21:00&nbsp;PAGE_LOAD&nbsp;{ device_type :  mobile ,  url :  http://www. hello. com/article/trending-for-2019 }This structure allows us to define multiple types of events across our system and the relevant details about each as&nbsp;JSON&nbsp;in the  Payload  field. In order to achieve this, we need to have the ability to track what different users are doing in our system. We have a service that we call to log our users' actions to get the PAGE_LOAD events (there are services like&nbsp;snowplow,&nbsp;segment&nbsp;or&nbsp;google analytics&nbsp;for this), we get the PLACE_ORDER events from the order table in our database, and we get EMAIL related information from our email vendor (mailchimp,&nbsp;sailthru,&nbsp;cheetahmail&nbsp;do this kind of thing). This very simple example shows that User 12345 has 10 events associated with them: He arrives to the site from a google paid search. We can use this to attribute our marketing acquisition cost. He views our plans page, then registers for an account, then logs in, and then places an order all on a desktop browser. Seeing the steps taken before an order is placed allows us understand our conversion funnel. Our system sends email (through an email vendor) and can track if it is opened or clicked on. He reads 2 article pages on their mobile device. A real user journey may have many thousands of actions spanning across many visits/sessions from different devices/channels. This can allow us to see how often they interact with our system and how their behavior changes over time. Margin + Profile Info + Actions == Potential for Analysis: When we combine the margin of a user, their profile information, and the actions in their user journey, we can: Find who are the high margin users and what makes them so. Develop understanding what actions and features drive higher margins. &nbsp;Find out what our members like and want. Develop understanding of their interests across segments of our membership. Understand how should we communicate with them - (effectiveness of email, chat, customer service, retail channels). Develop features/content (and possibly create experiments) to further satisfy the user. This can help support goals for longer retention or converting on up-sell opportunities. Find out how to describe and categorize our users. Does their profile and actions allow us to classify them into naturally forming groups?Try to make our offerings more attractive to future users. What are common paths for conversion and what types of folks convert quickly, slowly, not at all. Where do users get stuck and what actions have worked to de-risk these scenarios. Find out how our users at different margin levels find us. This can help optimize our marketing/awareness efforts. Understand what happens when we make changes to the offering - such as changing features, content, messaging, pricing, support options, etc. ?&nbsp;2) How can you keep customers coming back?: &nbsp;For subscription based business models, how much users continue to pay for the subscription (also known as retention) is a key concern because:&nbsp;Assuming that your offering can be profitable (subscription fees are higher than your costs), your revenues and margins scale linearly to the number of subscription terms that users pay for. So figuring out how to maximize retention will increase your earnings. &nbsp;It is almost always cheaper to retain current customers than to acquire new ones. &nbsp; Measuring retention?: Retention rate (as defined by&nbsp;Wikipedia)&nbsp;is  the ratio of the number of retained customers to the number at risk . As an example: if you have 1,000 subscribers in term 1 and 950 of those same users are still active in term 2, then your retention rate for that period is 950/1000=0. 95 or 95%. &nbsp;Churn rate (another popular metric) is the inverse of retention, is  the percentage of service subscribers who discontinue their subscriptions within a given time period . Back to our example: if you have 1,000 subscribers in term 1 and 950 of those same users are still active in term 2, then your churn rate for that period is (1000-950)/1000=0. 05 or 5%. &nbsp;Retention rate for a subscription offering is a proxy metric many folks use to understand its business health and investors use it to compare companies. Visualizing retention rates by cohorts: The most common way people seems to look at retention rate is to segment into related groups - known as cohorts. Each person in a cohort must share a related yet distinguishable trait that separates them from the other cohorts. In these examples, our cohorts will be based on the month that users starts their subscriptions. Aaron Chantiles has done a nice job creating these&nbsp;&nbsp;3 cohort model reports: 1) Survival Analysis - for each month term we can see the percentage of the original users in that cohort that are still active.  2) Average Revenue per User - for each month term we can see the average revenue of the active users in that cohort. (this could be margin instead)Note: Big variances from month to month likely results from introducing major changes to your subscription plan, adding some great new features, breaking something, or big impact from seasonality.  3) Total Revenue by Cohort - for each month term we can see the total cumulative revenue of the active users in that cohort. (this could be margin instead) Understanding and Predicting user retention: KEY QUESTION - If we can somehow experience or 'learn' how users have behaved in the past (those who churned and those who stayed), then can we can predict how your current users will behave in the future? We can try to leverage our user profile information, ledger, and user journey data sets to discover things about our users and how they behaved. We'll start by defining some variables (features) that we think may contribute to retention for a subscription content site: metrictypesourcedescriptionmedian_incomeintprofilewe can join user's location on census datapopulation_densityintprofilewe can join location on census datauser_ageintprofileage in full yearsfirst_month_priceintledgeramount of first month cost (to determine impact of promos)customer_acquisition_costintledgeramount of acquisition cost (to determine impact of paid marketing sources vs. organic)total_marginintledgertotal margin the user has contributed to the companysearches_countintjourneythe number of searches they performedvisit_countintjourneythe number of times they visitarticles_per_visitdecimaljourneythe number of articles they view per visitviews_per_articledecimaljourneyaverage number of times they visit the same articlefavorite_countintjourneynumber of times they favorite contentrecommendation_countintjourneynumber of times they recommend/share contentnon_subscription_revenuedecimalledgeramount of non-subscription revenueterm_numberintjourneynumber of termsis_churn_next_termbooleanjourneydid the user churn in the next term&nbsp;Once we build a nice data set, we can do some data exploration. We can run queries on the top 50-100 highest margin users and see visually view if there obvious patterns that can guide deeper exploration. &nbsp;If we see that these folks read lots of content, come in with low customer_acquisition_cost,&nbsp;and/or produce more non-subscription revenue - then we can dig on specifics of the behaviors related users achieving those actions/milestones. &nbsp;We can plot each of the metrics against number of terms and see which ones correlate well. &nbsp; In the past I have built machine learning models to try to identify what variables drive retention and show us the probability that a user is going to churn from their membership in upcoming terms (a. k. a . Churn model). As you can probably imagine, there is a substantial amount of data engineering work (data capture, feature definition, cleanup/transformation/standardization/regularization/labeling, validation) needed and lots of data exploration before we consider running models. I may devote a full future blog post to this, but for now I'll share some outside blog posts with helpful approaches along with python examples how to do this. &nbsp; My preference is try to calculate a Subscriber Fragility Score that indicates the likeliness that a subscriber with churn in the upcoming billing term. Many folks use&nbsp; regression and classification models to do this: https://towardsdatascience. com/predict-customer-churn-in-python-e8cd6d3aaa7https://365datascience. com/tutorials/python-tutorials/how-to-build-a-customer-churn-prediction-model-in-python/https://neptune. ai/blog/how-to-implement-customer-churn-prediction&nbsp;Another approach is to try to predict how long the subscriber will remain with the subscription via Survivor Analysis:&nbsp;https://square. github. io/pysurvival/tutorials/churn. htmlhttps://thedatascientist. com/customer-churn-machine-learning-data-science-survival-analysis/I also enjoyed this blog post  Top Ten Mistakes Data Scientists Make While Building Churn Models :&nbsp;https://medium. com/@swansburg. justin/top-ten-mistakes-data-scientists-make-while-building-churn-models-d773bb7deaa53) How can you get new visitors to buy things and become customers?: This area is a huge focus in many companies and I know quite a few Product Managers who have over a decade each dedicated to working on incrementally improving buyer experiences and getting better conversion. In all those cases, they heavily rely on data (and analysts) to help guide their activites. Let's start with some questions&nbsp;: 1. What is the value proposition of the subscription product? What is the problem that we think it solves and for what groups of people? There are many approaches and tools analysts can use to define a product's value proposition to users. Peter Thompson introduces a diagram view that&nbsp;he calls the Value Proposition Canvas. He defines as: A value proposition is the place where your company&rsquo;s product intersects with your customer&rsquo;s desires. It&rsquo;s the magic fit between&nbsp;what&nbsp;you make and&nbsp;why&nbsp;people buy it. Your value proposition is the crunch point between business strategy and brand strategy.  Below is an example illustration for an angel investing syndicate with a co-working space for the member investors and their portfolio of startups.  I'd want to try to quantify the key elements of the value proposition, specifically from the buyer's perspective (wants, needs, fears and substitutes). We can try to collect factual information, statistics, or research results from credible external sources that support the value proposition. This could include industry expert opinions, awards, or mentions, as well as documented improvements or reductions in attributes like revenue or costs. 2. How can a user purchase a subscription? More specifically what is the process users generally take to make purchases and what is the messaging about the product? A subscription funnel is an analytical method used to analyze the sequence of events&nbsp;in a subscription-based offering. &nbsp;&nbsp;Funnel analysis tracks user behavior throughout their user journey and calculates how many people make it through each step,&nbsp;allowing marketers / product managers to optimize their strategies and improve user experiences, hopefully leading to subscription growth. Typical steps in a subscription funnel for a website can be broken down into the following stages: Awareness: The user becomes aware of the website or service. This can include visiting the website, receiving an email, receiving a referral or seeing an advertisement. Interest: The user shows interest in the service by signing up for a newsletter, registering for a free trial, or subscribing to receive updates. Evaluation: The user evaluates the service by exploring features, reading reviews, or comparing it with competitors. Conversion: The user decides to subscribe or purchase the service. Checkout: The process and/or steps a user takes to subscribe to the service and make payment. &nbsp;Being able to leverage each action and signal in the&nbsp;user journey&nbsp;(time series of events) discussed earlier provides an opportunity to understand the funnel for each user and across user groups. &nbsp; Below is a basic example of funnel visualization on a companies's checkout process that shows the drop off from each step:&nbsp;&nbsp; We could next take it further in a BI tool or App (streamlit, Dash, Shiny, etc)&nbsp; we add support for filtering based on user attributes such as demographics, referral method (ads, organic), user behaviors, timing, etc. 3. What do you we know about the people who become subscribers? Are there consistent differences from the visitors to your site that do not become subscribers?&nbsp; When analyzing users who convert and users who don't for a subscription product, there are several data elements that can be useful for an analyst / data scientist: User demographics: Age, gender, location, and other demographic information can provide insights into the types of users who are more likely to convert. Analyzing these factors can help identify target segments for marketing and user acquisition efforts. &nbsp;User behavior: Tracking user interactions with the product (via our&nbsp;user journey), such as features used, frequency of use, and duration of sessions, can help identify patterns and trends among converting users. This information can be used to optimize the user experience and improve conversion rates. &nbsp;Conversion funnel analysis: Analyzing the steps users take before converting can reveal potential bottlenecks or barriers to conversion. Identifying these issues can help optimize the conversion process and increase overall conversion rates. User feedback: Collecting and analyzing user feedback, such as ratings, reviews, and survey responses, can provide valuable insights into user preferences, pain points, and areas for improvement. This information can be used to refine the product and marketing strategies to better cater to user needs. Marketing and promotional efforts: Analyzing the effectiveness of marketing campaigns and promotional efforts can help identify which channels, messages, and targeting strategies are most successful in driving conversions. This information can be used to optimize marketing budgets and improve overall conversion rates. Customer lifetime value (CLV): Assessing the long-term value of customers can help determine which user segments are most profitable and inform decisions regarding customer acquisition, retention, and marketing strategies. You can likely do some quick exploration to see how much each of these metrics/areas correlate to subscription over different timeframes or cohorts. &nbsp; What can you try to learn more?: Ask users what they want and why they aren't buying You can ask them (maybe even offer some kind of bribe) to understand specifically what their needs/wants are and why do not think the value proposition of your offering will satisfy them at an acceptable cost. &nbsp; I would want to find out if they think: Do they actually think they need your product?Are your prices too high?Is there a competitor with a better suited offering or better distribution?Are their friends using your product and giving positive feedback about it?Record their sessions and see what they see + do&nbsp;There are a number of&nbsp;SaaS services&nbsp;that offer the ability integration full session recording directly into your web site. I have used&nbsp;Full Story&nbsp;in the past and it was helpful in some scenarios. &nbsp;Look at the Churn Model. &nbsp; I have found that it is often the case that the reasons why subscribers churn is also the reasons why users are hesitant to subscribe to your product. You may find overlap from an asset you already have. Test Things and Run Experiments You can conduct experiments to test different versions of your website or app to identify the most effective design, content, messaging, or features that drive customer engagement and conversion. You can use statistical hypothesis testing to determine the impact of each variation on customer behavior. Some specific places that can be effective in terms of learning and potentially impacting conversion:Promotions: Impact&nbsp;from offering a trial or promotion (discount) for new subscriptions. Pricing: Impact&nbsp;from offering different pricing options or the impact of different display variations of the pricing options. Funnel / Checkout: Impact from changing the steps in your checkout process. Messaging: The way you message about your product's benefits (on site, partner sites, email, physical collateral, etc. ). Channels: Impact from the places your product is promoted and referred from. Customer Segmentation, Targeting and Personalization You can analyze customer data to identify different segments and their behavior patterns. This will help you understand which customers are more likely to make a purchase or subscribe to your services. You can use clustering algorithms like K-means or DBSCAN to group customers based on their characteristics, such as demographics, purchase history, and browsing behavior. 4) How can you build awareness/traffic to your offerings?: I do not claim to be a growth marketing expert, but I have worked with some very smart folks in this domain area and have learned some things along the way. &nbsp; Marketing Channels: Here are some of the tactics I have seen used to varying degrees of effectiveness to build awareness and traffic to your subscription offerings as a growth marketer: Leverage social media platforms Facebook, Instagram, YouTube, Twitter, TikTok, LinkedIn and others have HUGE highly active audiences. They can be fantastic avenues for attracting potential customers. Much of the volume is done as part of running brand-awareness (paid) ads that target specific audiences based on behaviors and preferences. This can help improve reach and recall, and even create 'lookalike' audiences similar to your existing followers. These platforms offer many different ad formats and creative display options that can appeal to your potential audience. In addition to paid options, many of the social networks provide opportunities to promote your offerings (just be careful not to be too spammy or obnoxious about promoting your business). Facebook and LinkedIN groups are often dedicated to specific niche topics and may be a good avenue to find users. Example - I am part of many Facebook groups related to sports autograph collecting, so I often answer questions letting newer collectors know they can find information they seek on&nbsp;SportsCollectors. Net. Organize contests and giveaways Host simple content or giveaways to grow your following and drive brand awareness. Example - Tech Influencer&nbsp;Alex Xu&nbsp;gives away his popular book  System Design Interview  in order to promote his excellent&nbsp;ByteByteGo&nbsp;paid newsletter. Give something away for free Offer a free trial or free samples of your subscription service or product to give potential customers a taste of what you offer. Example - I signed up for a free one month trial for&nbsp;Github's Copilot&nbsp;service that offers suggestions as I write code. Content marketing Publish blog posts, articles, or other forms of content to establish your expertise and thought leadership in your industry. This can help build brand awareness and drive traffic to your website. Example -&nbsp;Sports Card Investor&nbsp;hosts regular podcasts talking about the latest high level market trends and features in their MarketMovers subscription product for sports card sales data and analytics. Influencer marketing Collaborate with influencers and brand ambassadors to promote your subscription offerings and reach a wider audience. Example - This&nbsp;youtube video&nbsp;from Mr. Beast, which has 1M+ views &mdash; it&rsquo;s engaging and encourages the audience to give Honey a try. Companies such&nbsp;LTK&nbsp;can even help you find influencers that would work for your company. Email marketing Use email marketing to engage with your audience and promote your subscription offerings. Example - While I was at ElysiumHealth, their quarterly newsletters (the Abstract) were packed with deep scientific dives on longevity topics related that drove interest to their core products. &nbsp; Employee advocacy Encourage your employees to share your brand and subscription offerings on their social media platforms. Create a culture where employees proactively want to evangelize your organization. Example - I saw that many of the employees at&nbsp;Rent the Runway&nbsp;across business different functions in the company would publicly suggest the company's subscription offerings as well as specific rental products. &nbsp; Events Hosting Physical events to introduce your product to new audiences. Example - Amazon hosts events at their&nbsp;AWS Startup Lofts&nbsp;where experienced AWS architects can help you design solutions to solve your technical problems using AWS products. Search Engine Optimization (SEO) Marketing technique that is focused on bringing organic, non-paid traffic to your website by using high quality content to get to the top of a search engine results page. We want to analyze what works (and doesn't): We will want a way to understand the impacts of traffic and conversion/sales from each of the marketing channels your company employs. &nbsp; Common Problem - Disparate reporting does not reconcile I have found that many marketers (potentially in different marketing functions) generally use SaaS products for their specific channels that each allow you to set up tagging on your site to track activities within their application. &nbsp; Your email service provider (like&nbsp;MailChimp,&nbsp;Iterable,&nbsp;Klaviyo, etc) may attribute the full value of customer purchase if the user has opened an email sent from their platform. Facebook and Google may attribute the full value of that same customer purchase if users were sent to your site from paid ads you bought on their systems. &nbsp;Your influencer or referral partners (like&nbsp;Yopto,&nbsp;Talkable,&nbsp;Mention Me, etc) may also attribute the full value of that same customer purchase if users were sent to your site from referral links from their systems. &nbsp;So what generally happens is that the leads for email, paid, and influencer marketing download the performance reports from their respective SaaS system and present an overstated ROI for their area. You won't have a clear picture of how well each channel is contributing and how to prioritize spend between. This invariably leads to questions from your finance team when budgets are being planned out and the numbers don't all add up. &nbsp; How Rules Based Attribution Models Work Rules Based (non-model driven) marketing attribution models assign credit to specific marketing channels or touch points based on predetermined rules or assumptions, rather than analyzing data and customer behavior patterns to determine attribution. These models are popular because they are easy to understand, relatively straight forward to implement and can provide directional signal around ROI. They can be a good choice for businesses that are just starting out, have limited resources or have less a complex/diverse marketing channel mix. I have personally seen most companies implement these kinds of models. There are several types of rules based attribution models that businesses can use. These include: Our user journey&nbsp;data contains the full set of actions associated with each user that we are able to track from internal and partner systems. It is industry standard that we set up&nbsp;UTM parameters&nbsp;on incoming links that will inform us of the source, medium, and campaign associated with that user's visit. We will store those parameters as part of payload for each relevant event that we can later use for analysis. We will want to use user journey to be able to track individual customer interactions with our brand. The closest method we have to do this is with sessions, defined as discrete periods of activity by a user. The industry standard is to define a session as a series of activities followed by a 30 minutes window without activity. So we can write some code to categorize the actions in our user journey into sessions, with the first action representing channel information we will use for attribution of that session. We can use all of the sessions before a conversion as part of our attribution calculation. &nbsp; Based on the preferred attribution methodology shown in the diagram above, we can divide up the sessions for each transaction and attribute the revenue (and return on associated marketing spend) accordingly. Claire Carroll wrote a helpful&nbsp;blog post&nbsp;detailing an example of how to model out user journey data into sessions with only SQL using the dbt framework. It is very similar to the methodology I have taken in the past do this. My personal preference is to use either Time Decay or Position-Based models. I have found the most important thing is that you get consensus with the model that everyone will use up front, and then optimize around the related metrics. Data Driven Attribution Data-driven attribution gives credit for conversions based on how people engage with your various ads and decide to become your customers. Unlike the previous discussed rules based models, data-driven attribution gives you more accurate results by analyzing all of the relevant data about the marketing moments that led up to a conversion. Google uses this data-driven attribution in Google Ads&nbsp; as the models takes multiple signals into account, including the ad format and the time between an ad interaction and the conversion. They can drive better conversions because their systems can better predict the incremental impact a specific ad will have on driving a conversion, and adjust bids accordingly to maximize your ROI. &nbsp; There are two machine learning models that have become popular to use for data driven attribution: 1. Shapely Value The Shapley value is a way to fairly distribute credit for a shared outcome among team members by applying an algorithm based on a concept from cooperative game theory called the Shapley Value. In the case of data driven attribution, marketing touchpoints are the  team members , and the  output  of the team is conversions. The&nbsp; algorithm computes the counterfactual gains of each marketing touchpoint, which means it compares the conversion probability of similar users who were exposed to these touchpoints to the probability when one of the touchpoints does not occur in the path. The actual calculation of conversion credit for each touchpoint depends on comparing all of the different permutations of touchpoints and normalizing across them. James Kinney posted a nice explanation of how Game Theory and Shapely Value can be applied to&nbsp;Data Driven Marketing Attribution. He also provides a helpful Jupyter notebook with python code on his&nbsp;Github. 2. Markov Model A Markov model is a type of probabilistic model that describes a sequence of events where the probability of each event depends only on the state of the previous event. In the context of marketing attribution, a Markov model can help us model user journeys and how each channel factors into users moving from one channel to another to eventually make a purchase. For example, let's say a user first sees a Facebook ad, then clicks on a Google search result, and finally makes a purchase on your website. A Markov model would help us understand the probability of users moving from Facebook to Google and from Google to your website. One advantage of using a Markov model for marketing attribution is that it can account for the structure of your data, which may lead to more accurate results. However, it is more complex than other attribution models, and may require the help of a data scientist to implement at scale. To use a Markov model for marketing attribution, we need to estimate a transition matrix that describes the probability of moving from one channel to another. We can then compute the  removal effects  of each channel, which tells us the probability of conversion when a channel is removed from the user journey. This allows us to determine each channel's contribution to conversion and/or value. James Kinney posted Cloudera uses Markov models to solve multi-channel attribution in his post&nbsp;Marketing Attribution with Markov. &nbsp; Challenges with Attribution&nbsp; It requires set up and discipline to collect all of the necessary data in a central place (like a data warehouse). That means your team needs the data engineering skill sets to do the appropriate pipeline building, transformation and modeling. &nbsp;Your user journey may be missing some percentage of actions. Many have found it to be challenging to associate actions with anonymous (non signed in) users using mobile applications and mobile browsers. &nbsp;It is challenging to find a proxy for incorporating offline data, such as exposure to a TV, radio or print ad. Lack of visibility into external trends that might affect marketing efforts and conversions, such as seasonality, without incorporating aggregate information. Attribution models can be subject to correlation-based biases when analyzing the customer journey, causing it to look like one event cause another, when it may not have. &nbsp;Consumers who may have been in the market to buy the product and would have purchased it whether they had seen the ad or not. However, the ad gets the attribution for converting this user. Bias toward cheap Inventory gives an inaccurate view of how media is performing, making lower cost media appear to perform better due to the natural conversion rate for the targeted consumers, when the ads may not have played a role. Attribution models can often overlook the relationship between brand perception and consumer behavior, or will only look at them at a trend regression level. The quality of creative and messaging are just as important to consumers as the medium on which they see your ad. One common attribution mistake is evaluating creative in aggregate and determining that one message is ineffective, when in reality it would be effective for a smaller, more targeted audience. This emphasizes the importance of person-level analytics. &nbsp;"
    }, {
    "id": 18,
    "url": "http://localhost:4000/optimizing-snowfalke/",
    "title": "Optimizing spend in Snowflake",
    "body": "2023/04/25 - Since about 2016, I have introduced Snowflake at two of my day jobs (Rent the Runway and Luma Financial Technologies) and have witnessed the platform's considerable evolution. &nbsp; As with any hosted SaaS platform, it is very important to understand both how the solution is architected, how users interact with it, and how it is priced. &nbsp; A very common concern among dats folks is that their Snowflake bills grown considerably as their use cases for it expand. A few months ago I saw this post on LinkedIn: I think the suggested action in the post could potentially lead to a very small cost savings, but I suspect there are more impactful areas we can look at to keep our Snowflake costs down. I thought it would be helpful to detail out: How Snowflake's billing worksAreas where you can look to optimizeHow Snowflake's billing works: I found an area&nbsp;and a pdf where Snowflake lays out their costs, so I'll do my best to summarize with commentary below. One of the main selling points for Snowflake is that they do a really good job of separating storage of the data, the compute that runs actions on the data (queries, transformations, etc. ), and the cloud services that run the platform's engine. So their pricing is based on 3 components: the volume of data stored in Snowflakethe compute time usedthe cloud services that are used&nbsp;&nbsp;1. Data Storage Pricing&nbsp; Data storage pricing is based on the daily average data volume (in bytes) stored in Snowflake, including compressed or uncompressed files staged for bulk unloading or loading, historical data maintained for File-safe, and data kept in database tables. Snowflake automatically compresses and optimizes all table data, then calculates storage usage based on this compressed file size. The monthly charge for data storage in Snowflake is set at a flat rate per terabyte (TB). However, the precise amount per TB paid depends on the platform (Amazon Web Services (AWS), Azure, or Google Cloud Platform (GCP)). Typically, Snowflake charges a minimum of $25 and up to $40 per terabyte of data stored in its US system. &nbsp; For instance, if a Snowflake account is a capacity AWS in us-west-1 snowflake account and the price is $25 per Terabyte per month. It is worth noting that the cost of storage in Snowflake is based on the amount of data stored in the account. This includes data stored in database tables, files staged for bulk data loading, and clones of database tables that reference data deleted in the table that owns the clones 2. Compute Pricing and Credits Snowflake bills compute with credits - the unit used to measure how much billable compute (virtual warehouses) you consume. A Snowflake credit is used only when resources are active, such as when a virtual warehouse is currently running, when loading data with Snowpipe, or serverless features are in use. A virtual warehouse contains one or more computing clusters for performing queries, loading data, and other DML operations. Virtual warehouses use Snowflake credits as payment for the processing time they use. How many credits you use depends on the virtual warehouse&rsquo;s size, running duration, and how many they are. Warehouses are available in ten sizes now. Each size specifies the amount of computing power a warehouse can access per cluster. By expanding a warehouse to the next larger size, its computing power and credit usage doubles per full operational hour. Check this out: The credit pricing rate depends on the Snowflake edition you use: Standard, Enterprise, or Business-Critical. Each edition offers a different set of features. &nbsp; The table below shows the Snowflake Credit usage and estimate of the USD cost per hour assuming one credit costs $3. 00 (I have found you can get a better rate with a year long commitment):&nbsp; SizeCredits/HourCost/HourXSMALL1$3. 00SMALL2$6. 00MEDIUM4$12. 00LARGE8$24. 00X-LARGE16$48. 002X-LARGE32$96. 003X-LARGE64$192. 004X-LARGE128$384. 005X-LARGE256$768. 006X-LARGE512$1,536. 00Some details to note: Warehouse only uses credits while running &mdash; not when suspended or idle. &nbsp;Snowflake does not charge for idle compute. In fact, it offers a quick start/stop feature to suspend resource usage whenever you choose or automatically with user-defined rules, such as &ldquo;suspend after five minutes of inactivity&rdquo;. Credit usage per hour directly correlates to the number of servers in a warehouse cluster. Snowflake bills credits per second &mdash; with a minimum requirement of 60 seconds. Meaning, starting or resuming a suspended warehouse incurs a fee of one minute&rsquo;s worth or usage. Also, resizing a warehouse a size larger costs a full minute&rsquo;s worth of usage. For example, resizing from Medium (four credits per hour) to Large (eight credits per hour) has an additional charge for one minute&rsquo;s worth of four additional credits. But after a minute&rsquo;s usage, all subsequent usage resumes on a per-second billing as long as you run the virtual warehouses continuously. &nbsp;Stopping and restarting warehouses within the first minutes leads to multiple charges because the one-minute minimum charge applies each time you restart. Operations, such as suspending, resuming, increasing, and decreasing are nearly instantaneous, So, you can precisely match your Snowflake spending to your actual usage, never worrying about capacity planning or unexpected usage. 3. Cloud Services Pricing and Credits Snowflake cloud services comprise a set of tools that support the main data storage and compute functions. The services include metadata and infrastructure management, SQL API, access control and authentication, and query parsing. These services are, in turn, powered by compute resources, meaning cloud services consume Snowflake credits, just like virtual warehouses. However, Snowflake only bills cloud services that exceed 10% of your daily compute resources usage. Thus, the 10% adjustment automatically applies on each day, based on that day&rsquo;s credit price. For example, if you&rsquo;ve used 120 compute credits and 20 cloud services credits on a particular day, the adjustment will automatically subtract 10% off the compute credits (120 X 10% = 12) for that day. So, your billable credits will be: Total used cloud services credits for the day (20) &ndash; adjusted amount (12) = 8 credits billed. If your cloud services amount is less than 10% of your daily compute credits amount, Snowflake charges you based on that day&rsquo;s cloud services credits. For example, if you&rsquo;ve used 100 compute credits and 8 cloud services credits, you&rsquo;ll have used 8% of your compute credits in cloud services credits. In this case, Snowflake will automatically count 8 cloud services credits as the day&rsquo;s cloud services usage. Areas where to optimize&nbsp;: Snowflake is both robust and complex platform. With that, the good and bad news is that there are quite a few options/opportunities to consider that can affect your spend. 1. Look at your spend &amp; consumption Before I look to optimize, I want to understand my current consumption vs. my budget. You can see this in Snowflake's GUI using the ACCOUNTADMIN role: A resource I found for monitoring spend is a dbt package from SELECT:&nbsp;https://github. com/get-select/dbt-snowflake-monitoring 2. Don't do things you don't need to! Do I have processes running to create calculations, tables, or views&nbsp; that no longer add value? A lot of analytics and data science is doing feature exploration, but we don't always clean up after our experimentation and disproved hypotheses. This can consume a bunch of credits ongoing basis when they take a lot of resources to create, and morse so if they exist in multiple environments (for testing, CI/CD). Am I running processes more often than I need to? As an example - when the models are based off data that we get daily (such as closing stock values), it can become costly to run these many times a day. I have found using a dbt tags&nbsp;to limit the models are run in jobs that are limited to running daily. Can I decrease sync frequency of my data ingestion tool? There can be a pretty big a credit difference in sync'ing data ever 5 or 15 minutes vs. hourly in FiveTran, AirByte, etc. &nbsp; 3. Data Warehouse Considerations In general, the majority of Snowflake compute cost is the result of automated jobs. &nbsp; This means a huge proportion of cost is the result of transformation jobs and this should be the priority to optimize Snowflake warehouse cost. &nbsp;&nbsp; Segment your workloads by warehouses My preference is to divide workloads in different warehouses for loading data, transforming data and for end user consumption as I previously described in&nbsp;My Snowflake Set up with Terraform. This way I can both configure the permission grants and warehouse size for each of these workload types. You may need to further segment - maybe by business vertical or priority of usage. Choose the right size of your warehouses As mentioned, the size of your Snowflake warehouse has a direct impact on your monthly bill. &nbsp; Your use case will be key in deciding whether to run large or small warehouses. In general, running heavy queries on large warehouses and light queries on small warehouses is the most cost-effective way to go. Be aware queries should run twice as fast on a larger warehouse but stop increasing warehouse size when the elapsed time improvements drops below 50%. I generally prefer to go with smaller warehouses and to define MAX_CLUSTER_COUNT parameter that will bring up additional nodes as scale is needed. For warehouses designed to run lower priority batch jobs set the MAX_CLUSTER_COUNT = 3 and SCALING_POLICY = 'ECONOMY' to balance the need to maximize throughput with optimizing compute cost. For end-user warehouses where performance is a priority set the MAX_CLUSTER_COUNT = 3 and SCALING_POLICY = 'STANDARD'. &nbsp; This will automatically allocate additional clusters as the concurrent workload increases. &nbsp; However, set the MAX_CLUSTER_COUNT to the smallest number possible while controlling the time spent queuing. &nbsp; With a SCALING POLICY of STANDARD, avoid setting the MAX_CLUSTER_COUNT = 10 (or higher) unless maximizing performance is a much higher priority than controlling cost. Suspend warehouses that are sitting idle If you have virtual warehouses that are inactive, you can suspend them to make sure you're not getting charged for unused compute power. &nbsp; Here is an example of how you can create a warehouse (the auto_suspend parameter controls how long until the idle warehouse is suspended): -- Create a multi-cluster warehouse for batch processingcreate or replace warehouse transform_wh with&nbsp; &nbsp; warehouse_size&nbsp; &nbsp; &nbsp; &nbsp;= XSMALL&nbsp; &nbsp; min_cluster_count&nbsp; &nbsp;= 1&nbsp; &nbsp; max_cluster_count&nbsp; &nbsp;= 3&nbsp; &nbsp; scaling_policy&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; = economy&nbsp; &nbsp; auto_suspend&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; = 60&nbsp; &nbsp; auto_resume&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;= TRUE; Update the query timeout default value By default, a Snowflake statement runs for 48 hours (172800 seconds) before the system aborts it. &nbsp; This means that Snowflake will charge you for the time it took to compute a query that may have been initiated by mistake. Set the STATEMENT_TIMEOUT_IN_SECONDS parameter: alter warehouse transform_wh&nbsp;set&nbsp; &nbsp;statement_timeout_in_seconds = 3600;&nbsp; &nbsp;-- 3,600 = 1 hour Coordinating queries You pay some money for idle warehouses (as defined by the auto_suspend&nbsp; parameter). &nbsp; There can be advantages to submitting multiple SQL jobs in parallel in a different connection running on a shared batch transformation warehouse to maximize throughput. Warehouse Observability To make sure you're staying within your Snowflake budget, you can use a resource monitor to suspend a warehouse when it reaches its credit limit. &nbsp; A great trick is to set credit thresholds at different levels. For example, you could set an alert for when 70% credit consumption is reached and then another for when 90% of the credit consumption is reached. use role accountadmin; create or replace resource monitor transform_wh_monitor&nbsp;with&nbsp; &nbsp;credit_quota = 48&nbsp; &nbsp; &nbsp;triggers on 70 percent&nbsp; do notify&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; on 90 percent&nbsp; do notify&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; on 100 percent do suspend&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; on 110 percent do suspend_immediate;&nbsp; 4. Understand and Optimize Queries Since credits for running queries/transformations are usually the biggest component of my snowflake bill, I generally want to understand: 1. What queries represent the biggest portion of my consumption?2. What parts of those of those queries take the longest?3. Can I refactor those queries better (faster + less resource intensive)?&nbsp; What queries represent the biggest portion of my consumption? I found a helpful blog post that talks about the ACCOUNT_USAGE schema (that contains details about my Snowflake usage) and how to build a dashboard within Snowflake detailing my usage. Below are some screen shots:  Since I am a frequent dbt user, I generally will look at the standard output after a  dbt run  for models that are taking over 60-120 seconds (shown below). Unless your data is quite large, this is generally a good indicator for a potential refactor. &nbsp; What parts of those of those queries take the longest? My friend, the Snowflake Query Profiler, is very helpful. It's probably one of my favorite features in the product.  The profiler shows you in detail how it breaks down your query in order to build your requested data set. It shows the amount of time it spends on each step. This will let you know where there may be opportunities for refactor your query. This is an excellent blog post that details more about the profiler:&nbsp;https://teej. ghost. io/understanding-the-snowflake-query-optimizer/ Can I refactor those queries better (faster + less resource intensive)? Like with most databases and infra, POORLY WRITTEN CODE MAY BE VERY COSTLY. Some things I look for that could be inefficient code to refactor:- Where is query spending its time - on (network, processing, synchronization)?- Is the query scanning too much data? Could you filter more efficiently or set up cluster keys?- Is it spilling out of memory and reading from disk?- Is the query calling too many micro partitions?- is there an inefficient join? &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; - Could you rewrite it as a CTE or with UNION'ing data sets? &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; - Should you change the order if you are joining two tables ON multiple fields?- Are you calling a view (or multiple cascading views) where having a materialized table makes more sense?- Are your UDF's (functions) running efficiently? Can you do the logic in SQL (more efficient) instead of another supported language?- Are there places where we can do incremental additions/updates instead of fully rebuilding full tables? 5. Data Storage Considerations Use the right type of table To help manage the storage costs associated with Time Travel and Fail-safe, Snowflake provides two table types, temporary and transient. Temporary and transient tables do not incur the same fees as permanent tables. Transient and temporary tables contribute to the storage charges that Snowflake bills your account until explicitly dropped. Data stored in these table types contributes to the overall storage charges Snowflake bills your account while they exist. Temporary tables are typically used for non-permanent session specific transitory data such as ETL or other session-specific data. Temporary tables only exist for the lifetime or their associated session. On session end, temporary table data is purged and unrecoverable. Temporary tables are not accessible outside the specific session that created them. Transient tables exist until explicitly dropped and are available to all users with appropriate privileges For large, high-churn dimension tables that incur overly-excessive costs, Snowflake recommends creating these tables as transient with zero Time Travel retention and then copying these tables on a periodic basis into a permanent table. This effectively creates a full backup of these tables. Because each backup is protected by CDP, when a new backup is created, the old one can be deleted Understanding Snowflake Stages To support bulk loading of data into tables, Snowflake utilizes stages where the files containing the data to be loaded are stored. Snowflake supports both internal stages and external stages. Data files staged in Snowflake internal stages are not subject to the additional costs associated with Time Travel and Fail-safe, but they do incur standard data storage costs. As such, to help manage storage costs, Snowflake recommends monitoring these files and removing them from the stages once the data has been loaded and the files are no longer needed It is also possible to use Snowflake to access data in on-premises storage devices that expose a highly compliant S3 API. With External Stages and External Tables against on-premises storage, customers can make Snowflake their self-service platform for working with data without having to worry about concurrency issues or the effort of managing compute infrastructure. Data governors can apply consistent policies to tables and monitor usage regardless of where the data is physically stored. Analysts and data scientists have a full view of all relevant data, whether it's on premises or in the cloud, including first-party or even shared, third-party data sets Delete what you don't need Similar to the earlier advice to&nbsp; Don't do things you don't need to! , I'd suggest you don't keep data that you will never need and/or that you do not want to continuing keeping fresh. While storage is relatively cheap compared to compute, it still can add on to your bill. Split large files to minimize processing overhead To distribute the load across the compute resources in an active warehouse, export large files in smaller chunks using a split utility. &nbsp; This will allow Snowflake to divide the workload into parallel threads and load multiple files simultaneously, which will reduce the compute time of your virtual warehouse. Use zero-copy cloning This unique feature lets you create database, table and schema clones which use pointers to the live data and don't need additional storage. &nbsp; As a result, you can save on storage costs and the time it takes to configure the cloned environment. &nbsp; Note that by deleting the original table, storage fees transfer to the cloned table. Always delete both the original and cloned tables you're not using. Conclusions: Snowflake is a great platform for Data Warehouse as a Service. It alleviates many of the more traditional DBA tasks I have had to perform on self hosted OLTP databases (SQL Server, MySQL, PostgreSQL, etc. ). &nbsp; But like any software platform (especially data platforms), we still need to consider continuous improvement that require commitments to monitoring and then possibly selected optimizations. Hopefully some of the suggestions detailed above will be helpful if you are using Snowflake. "
    }, {
    "id": 19,
    "url": "http://localhost:4000/dagster-fun/",
    "title": "Dagster with Python, Singer, and Meltano",
    "body": "2023/04/04 - I have been a fan of Dagster for data orchestration for a little while and wanted to share some of the basics. There are a lot of cool things I like about it (compared to airflow and other schedulers): It is Declarative (via their Software-Defined Asset object). I personally like tools and frameworks that allow me to declare a desired end state (Terraform, dbt, Puppet, Ansible, etc. ) vs. frameworks that have me build a bunch of imperative tasks that get daisy chained together. &nbsp; &nbsp;It makes it very easy to define dependencies as function arguments. It reminds a lot of simplicity I see with ref() statements in dbt. It feels like it is a better fit for iterative engineering. It has easy support for writing unit tests and running the same code/functionality in different environments. It pretty easily has integration support with many data related steps I would want to apply as part of an asset building pipeline - including tools like dbt,&nbsp;airbye, meltano, etc. Goal for this blog post:: In this post, I am going to document different ways how I can build pretty simple common pipelines that take a csv and upload the contents to postgres via Dagster using: Python CodeSingerMeltanoSet Up Steps for this demo:: We need to install Dasgter + necessary python packages (I am installing it locally, but we could install it in a docker also):&nbsp; &nbsp; &nbsp; pip3 install dagster dagit pandas psycopg2We need to install postgres (also adding pgadmin for web based admin) in dockers:https://towardsdatascience. com/how-to-run-postgresql-and-pgadmin-using-docker-3a6a8ae918b5This creates us a local postgres instance with a database  demo_db . We need to create a table in the demo_db database:CREATE TABLE IF NOT EXISTS public. sales(&nbsp; &nbsp; TransactionID text&nbsp; &nbsp; , Seller text&nbsp; &nbsp; , Date text&nbsp; &nbsp; , Value text&nbsp; &nbsp; , Title text&nbsp; &nbsp; , Identifier text&nbsp; &nbsp; , Condition text&nbsp; &nbsp; , RetailValue text&nbsp; &nbsp; , ItemValue text);We need some sample data in csv format to upload, so I cerated some sales data for a 1973 Topps Rich Gossage baseball card (below contains a few records): TransactionID,Seller,Date,Value,Title,Identifier,Condition,RetailValue,ItemValue 8094231,comicards990,2023-04-12,14. 59,1973 Topps Rich  Goose  Gossage Rookie White Sox HOF #174,134520441986,Ungraded,6. 00,8. 73 8094232,916lukey31,2023-04-11,10. 95,1973 Topps #174 Rich Gossage RC HOF Vg-Ex *Free Shipping*,275699466365,Ungraded,6. 00,8. 73 8094233,jayjay5119,2023-04-11,6. 50,1973 Topps Baseball! Rich Gossage rookie card! Card174! Chicago White Sox!,195695255305,Ungraded,6. 00,8. 73Running Dagster with Python: 1) The first thing I did was to run the scaffolding command to create a new dagster project: dagster project scaffold --name dagster-project It created the following directory and files: File/DirectoryDescriptiondagster_project/A Python package that contains your new Dagster code. dagster_project_tests/A Python package that contains tests fordagster_project. README. mdA description and starter guide for your new Dagster project. pyproject. tomlA file that specifies package core metadata in a static, tool-agnostic way. This file includes a&nbsp;tool. dagster&nbsp;section which references the Python package with your Dagster definitions defined and discoverable at the top level. This allows you to use thedagster dev&nbsp;command to load your Dagster code without any parameters. Refer to the&nbsp;Code locations documentation&nbsp;to learn more. Note:&nbsp;pyproject. toml&nbsp;was introduced in&nbsp;PEP-518&nbsp;and meant to replace&nbsp;setup. py, but we may still include a&nbsp;setup. py&nbsp;for compatibility with tools that do not use this spec. setup. pyA build script with Python package dependencies for your new project as a package. setup. cfgAn ini file that contains option defaults for&nbsp;setup. py&nbsp;commands. 2) In that directory we save our sales csv file as:&nbsp; 1973_topps_gossage_sales. csv 3) In that directory, create a file called python_assets. py: 1234567891011121314151617181920212223242526272829303132333435363738394041424344from dagster import asset # import the `dagster` library# python libraries we need for these assetsimport numpy as npimport psycopg2import psycopg2. extras as extrasimport pandas as pd# get sales from a csv file@assetdef get_sales():  csv_file_path = '. /1973_topps_gossage_sales. csv'  df = pd. read_csv(csv_file_path)  return df# write sales to postgres table, calling the dataframe from get_sales@assetdef write_sales(get_sales):  conn = psycopg2. connect(    database= demo_db ,    user='root',    password='root',    host='localhost',    port='5432',    options= -c search_path=dbo,public   )  table = 'sales'  tuples = [tuple(x) for x in get_sales. to_numpy()]  cols = ','. join(list(get_sales. columns))  query =  INSERT INTO %s(%s) VALUES %%s  % (table, cols)  cursor = conn. cursor()  try:    extras. execute_values(cursor, query, tuples)    conn. commit()  except (Exception, psycopg2. DatabaseError) as error:    print( Error: %s  % error)    conn. rollback()    cursor. close()    return 1  print( the dataframe is inserted )  cursor. close() In the above code, we have functions preceded by the @asset decorator. This tells Dagster to identify them as software-defined assets that can be materialized. &nbsp; The get_sales asset will read the csv file and populate the dataframe. The&nbsp;write_sales asset writes the contents of dataframe to our Postgres table. Notice the&nbsp;write_sales(get_sales)&nbsp;signature, that is how Dagster can recognize that&nbsp;get_sales&nbsp;is a dependency for&nbsp;write_sales. PLEASE NOTE: We would never want to hard code database credentials in our python code, below is how you can use environment variables to make it more secure:&nbsp;&nbsp;https://docs. dagster. io/guides/dagster/using-environment-variables-and-secrets&nbsp; 4) We can now run the asset pipeline in the Dagster dashboard. You can type the following on the command line to launch the dagit dashboard: dagster dev -f python_assets. py The you can visit the following address in your browser to view the dashboard: http://127. 0. 0. 1:3000/assets 4) Select the checkboxes in front of both assets and click the  Materialize selected  button. This will execute the assets in the correct order. &nbsp; The screen below show the successful materialization of the assets:&nbsp; 5) We can log into Postgres (via pgadmin) and see our records written into the public. sales table: Running Dagster with Singer: Singer is an open-source ETL tool from Stitch that lets you write scripts to move data from your sources to their destinations. Singer has two types of scripts&mdash;taps and targets. A tap is a script, or a piece of code, that connects to your data sources and outputs the data in JSON format. A target script pipes these data streams from input sources and store them in your data destinations. &nbsp;1) In order to migrate our dagster flow to use singer, we will need a tap to read our csv and a target to write to postgres. We can install them into python virtual environments with the below commands: 1234567891011python -m venv tap-csv-venvsource tap-csv-venv/bin/activatepip3 install git+https://github. com/MeltanoLabs/tap-csv. gitalias tap-csv= tap-csv-venv/bin/tap-csv deactivatepython -m venv target-postgres-venvsource target-postgres-venv/bin/activatepip3 install git+https://github. com/datamill-co/target-postgres. gitalias target-postgres= target-postgres-venv/bin/target-postgres deactivate2) Configure the tap-csv (docs at https://github. com/MeltanoLabs/tap-csv): - Create a file singer/config. json with this content: 123{   csv_files_definition :  . /singer/files_def. json }- Create a file singer/files_def. json with this content: 123456[  {   entity  :  sales ,     path  :  1973_topps_gossage_sales. csv ,     keys  : [ TransactionID ]  }]You can test that the tap will read in your file and convert it to json: tap-csv --config singer/config. json 2023-04-03 05:23:13,706 | INFO&nbsp; &nbsp; &nbsp;| tap-csv&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; | Beginning full_table sync of 'sales'. . . 2023-04-03 05:23:13,706 | INFO&nbsp; &nbsp; &nbsp;| tap-csv&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; | Tap has custom mapper. Using 1 provided map(s). { type :  SCHEMA ,  stream :  sales ,  schema : { properties : { TransactionID : { type : [ string ,  null ]},  Seller : { type : [ string ,  null ]},  Date : { type : [ string ,  null ]},  Value : { type : [ string ,  null ]},  Title : { type : [ string ,  null ]},  Identifier : { type : [ string ,  null ]},  Condition : { type : [ string ,  null ]},  RetailValue : { type : [ string ,  null ]},  ItemValue : { type : [ string ,  null ]}},  type :  object },  key_properties : [ TransactionID ]}{ type :  RECORD ,  stream :  sales ,  record : { TransactionID :  8094231 ,  Seller :  comicards990 ,  Date :  2023-04-12 ,  Value :  14. 59 ,  Title :  1973 Topps Rich \ Goose\  Gossage Rookie White Sox HOF #174 ,  Identifier :  134520441986 ,  Condition :  Ungraded ,  RetailValue :  6. 00 ,  ItemValue :  8. 73 },  time_extracted :  2023-04-03T09:23:13. 706647+00:00 }{ type :  STATE ,  value : { bookmarks : { sales : { starting_replication_value : null}}}}{ type :  RECORD ,  stream :  sales ,  record : { TransactionID :  8094232 ,  Seller :  916lukey31 ,  Date :  2023-04-11 ,  Value :  10. 95 ,  Title :  1973 Topps #174 Rich Gossage RC HOF Vg-Ex *Free Shipping* ,  Identifier :  275699466365 ,  Condition :  Ungraded ,  RetailValue :  6. 00 ,  ItemValue :  8. 73 },  time_extracted :  2023-04-03T09:23:13. 706848+00:00 }{ type :  RECORD ,  stream :  sales ,  record : { TransactionID :  8094233 ,  Seller :  jayjay5119 ,  Date :  2023-04-11 ,  Value :  6. 50 ,  Title :  1973 Topps Baseball! Rich Gossage rookie card! Card174! Chicago White Sox! ,  Identifier :  195695255305 ,  Condition :  Ungraded ,  RetailValue :  6. 00 ,  ItemValue :  8. 73 },  time_extracted :  2023-04-03T09:23:13. 707571+00:00 }2023-04-03 05:23:13,707 | INFO&nbsp; &nbsp; &nbsp;| singer_sdk. metrics&nbsp; &nbsp;| INFO METRIC: { metric_type :  timer ,  metric :  sync_duration ,  value : 0. 0012621879577636719,  tags : { stream :  sales ,  context : {},  status :  succeeded }}2023-04-03 05:23:13,708 | INFO&nbsp; &nbsp; &nbsp;| singer_sdk. metrics&nbsp; &nbsp;| INFO METRIC: { metric_type :  counter ,  metric :  record_count ,  value : 3,  tags : { stream :  sales ,  context : {}}}{ type :  STATE ,  value : { bookmarks : { sales : {}}}}{ type :  STATE ,  value : { bookmarks : { sales : {}}}} 2) Configure the taraget-postgres (docs at&nbsp;https://github. com/datamill-co/target-postgres): - Create a file singer/target_postgres_config. json with this content: 12345678{   postgres_host :  localhost ,   postgres_port : 5432,   postgres_database :  demo_db ,   postgres_username :  root ,   postgres_password :  root ,   postgres_schema :  public }You can test that the tap and target will read in your file and upload it to postgres: tap-csv --config singer/config. json | target-postgres --config singer/target_postgres_config. json&nbsp; INFO PostgresTarget created with established connection: `user=root password=xxx dbname=demo_db host=localhost port=5432 application_name=target-postgres`, PostgreSQL schema: `public` INFO Sending version information to singer. io. To disable sending anonymous usage data, set the config parameter  disable_collection  to true 2023-04-03 13:09:44,466 | INFO&nbsp; &nbsp; &nbsp;| tap-csv&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; | Beginning full_table sync of 'sales'. . . 2023-04-03 13:09:44,467 | INFO&nbsp; &nbsp; &nbsp;| tap-csv&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; | Tap has custom mapper. Using 1 provided map(s). 2023-04-03 13:09:44,467 | INFO&nbsp; &nbsp; &nbsp;| singer_sdk. metrics&nbsp; &nbsp;| INFO METRIC: { metric_type :  timer ,  metric :  sync_duration ,  value : 0. 000537872314453125,  tags : { stream :  sales ,  context : {},  status :  succeeded }}2023-04-03 13:09:44,467 | INFO&nbsp; &nbsp; &nbsp;| singer_sdk. metrics&nbsp; &nbsp;| INFO METRIC: { metric_type :  counter ,  metric :  record_count ,  value : 3,  tags : { stream :  sales ,  context : {}}}INFO Mapping: test to NoneINFO Mapping: sales to ['sales']INFO Mapping: tp_sales_transactionid__sdc_sequence_idx to NoneINFO Stream sales (sales) with max_version None targetting NoneINFO Root table name salesINFO Writing batch with 3 records for `sales` with `key_properties`: `['TransactionID']`INFO Writing table batch schema for `('sales',)`. . . INFO METRIC: { type :  timer ,  metric :  job_duration ,  value : 0. 048573970794677734,  tags : { job_type :  upsert_table_schema ,  path : [ sales ],  database :  demo_db ,  schema :  public ,  table :  sales ,  status :  succeeded }}INFO Writing table batch with 3 rows for `('sales',)`. . . INFO METRIC: { type :  counter ,  metric :  record_count ,  value : 3,  tags : { count_type :  table_rows_persisted ,  path : [ sales ],  database :  demo_db ,  schema :  public ,  table :  sales }}INFO METRIC: { type :  timer ,  metric :  job_duration ,  value : 0. 10874700546264648,  tags : { job_type :  table ,  path : [ sales ],  database :  demo_db ,  schema :  public ,  table :  sales ,  status :  succeeded }}INFO METRIC: { type :  counter ,  metric :  record_count ,  value : 3,  tags : { count_type :  batch_rows_persisted ,  path : [ sales ],  database :  demo_db ,  schema :  public }}INFO METRIC: { type :  timer ,  metric :  job_duration ,  value : 0. 10944700241088867,  tags : { job_type :  batch ,  path : [ sales ],  database :  demo_db ,  schema :  public ,  status :  succeeded }} Some things to take note of: - We defined the entity =  sales  in the tap. &nbsp; So that dictates the table_name for the target. - We see in the output  job_type :  upsert_table_schema , this means that write to postgres will do an upsert of the record based on the keys we defined in tap ( keys  : [ TransactionID ]). 3)&nbsp;In that directory, create a file called python_assets. py: 123456789101112# import the `dagster` libraryfrom dagster import asset# python libraries we need for these assetsimport subprocess# get sales from a csv file and write to postgres via singer tap and target@assetdef get_and_write_sales_with_singer():  ps = subprocess. Popen(['~/dagster/dagster-project/tap-csv-venv/bin/tap-csv', '--config', 'singer/config. json'],  stdout=subprocess. PIPE) output = subprocess. run(['~/dagster/dagster-project/target-postgres-venv/bin/target-postgres', '--config', 'singer/target_postgres_config. json'], stdin=ps. stdout)  ps. wait() print(output. stdout)4) We can now run the asset pipeline in the Dagster dashboard. You can type the following on the command line to launch the dagit dashboard: 1dagster dev -f singer_assets. pyThe you can visit the following address in your browser to view the dashboard: http://127. 0. 0. 1:3000/assets 5) Select the checkboxes in front of both assets and click the  Materialize selected  button. This will execute the assets in the correct order. &nbsp; The screen below show the successful materialization of the assets:&nbsp; &nbsp;6) We can log into Postgres (via pgadmin) and see our records written into the&nbsp;public. sales&nbsp;table: Running Dagster with Meltano: Meltano is an open source tool which can be used to extract from data sources and load it to destinations like your data warehouse. It uses extractors and loaders written in the Singer open source standard. 1) In order to migrate our dagster flow to use meltano, we will need to install meltano: 1pip3 install meltano2) Configure a new meltano project and switch into the directory:&nbsp; 12meltano init meltanocd meltano3) We need to install the tap and target: 12meltano add extractor tap-csvmeltano add loader target-postgres --variant&amp;nbsp;datamill-co4) Adding a job that include the tap and target: 1meltano job add demo_job --tasks  tap-csv target-postgres 4) Configure the following in meltano. yml (it should look very similar to the singer configuration from the Singer section): 123456789101112131415161718192021222324252627282930313233version: 1default_environment: devproject_id: fb9afb3c-5560-406e-b977-d86eef949779environments:- name: dev- name: staging- name: prodplugins:extractors:- name: tap-csv  variant: meltanolabs  pip_url: git+https://github. com/MeltanoLabs/tap-csv. git  config:  files:    - entity: sales    file: . . /1973_topps_gossage_sales. csv    keys:      - TransactionIDloaders:- name: target-postgres  variant: datamill-co  pip_url: git+https://github. com/datamill-co/target-postgres. git  config:  host: localhost  port: 5432  user: root  password: root  dbname: demo_db  default_target_schema: publicjobs:- name: demo_jobtasks:- tap-csv target-postgres5) Run meltano on the command line: 1meltano run demo_job2023-04-03T18:06:17. 336263Z [info&nbsp; &nbsp; &nbsp;] Environment 'dev' is active2023-04-03T18:06:19. 212684Z [info&nbsp; &nbsp; &nbsp;] INFO Starting sync&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;cmd_type=elb consumer=False name=tap-csv producer=True stdio=stderr string_id=tap-csv2023-04-03T18:06:19. 212939Z [info&nbsp; &nbsp; &nbsp;] INFO Syncing entity 'sales' from file: '. . /1973_topps_gossage_sales. csv' cmd_type=elb consumer=False name=tap-csv producer=True stdio=stderr string_id=tap-csv2023-04-03T18:06:19. 213097Z [info&nbsp; &nbsp; &nbsp;] INFO Sync completed&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; cmd_type=elb consumer=False name=tap-csv producer=True stdio=stderr string_id=tap-csv2023-04-03T18:06:19. 577999Z [info&nbsp; &nbsp; &nbsp;] time=2023-04-03 14:06:19 name=target_postgres level=INFO message=Table ' sales ' exists cmd_type=elb consumer=True name=target-postgres producer=False stdio=stderr string_id=target-postgres2023-04-03T18:06:19. 711365Z [info&nbsp; &nbsp; &nbsp;] time=2023-04-03 14:06:19 name=target_postgres level=INFO message=Loading 3 rows into 'public.  sales ' cmd_type=elb consumer=True name=target-postgres producer=False stdio=stderr string_id=target-postgres2023-04-03T18:06:19. 840375Z [info&nbsp; &nbsp; &nbsp;] time=2023-04-03 14:06:19 name=target_postgres level=INFO message=Loading into public.  sales : { inserts : 0,  updates : 3,  size_bytes : 452} cmd_type=elb consumer=True name=target-postgres producer=False stdio=stderr string_id=target-postgres2023-04-03T18:06:19. 862895Z [info&nbsp; &nbsp; &nbsp;] Incremental state has been updated at 2023-04-03 18:06:19. 862840. 2023-04-03T18:06:19. 871775Z [info&nbsp; &nbsp; &nbsp;] Block run completed. &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;block_type=ExtractLoadBlocks err=None set_number=0 success=True 6) Install the Dagster-Meltano library 12cd . . /pip3 install dagster-meltano7) In that directory, create a file called meltano_assets. py: 1234567891011from dagster import Definitions, jobfrom dagster_meltano import meltano_resource, meltano_run_op @job(resource_defs={ meltano : meltano_resource})def run_job():tap_done = meltano_run_op( demo_job )() # alternatively we could run this# tap_done = meltano_run_op( tap-csv target-postgres )()defs = Definitions(jobs=[run_job])8) We can now run the asset pipeline in the Dagster dashboard. You can type the following on the command line to launch the dagit dashboard: 1dagster dev -f meltano_assets. py"
    }, {
    "id": 20,
    "url": "http://localhost:4000/snowflake-setup-with-terraform/",
    "title": "My Snowflake Set up with Terraform",
    "body": "2023/01/28 - I have been working with Snowflake since 2016 when I proposed and chose to bring it into Rent the Runway to replace our very painful on premise Vertica implementation (yes we had it in a data center in NJ). Since then, Snowflake has grown considerably and is now one of the leading Data Warehouse offerings. After implementing data warehouses at several companies and having lots of conversations with some very smart folks, I've learned some things along the way. I've found it is a really good idea to think about up front how I want to segment responsibilities and permissions for databases. I want to decide what types of data to store in different places and to build roles + permission grants to enforce those decisions. &nbsp; My Best Practices Assumptions for segmenting data:: Best practice for running a cloud data warehouse is to build separate repositories for: raw data from source systems and providersschemas where data is regularly transformed/generated and documented to be consumed for reporting and analysisarea to data exploration, modeling, and experimentationTo ensure that these logical areas are used for their defined purposes, we can create specific roles and permissions for each. &nbsp; Logical set up of Databases with Role Permissions: &nbsp; databasefunctional descriptionprivilegesRawData is loaded from source systems and providers in its original (non transformed) formatNo users can directly query this dataload_role - full access&nbsp;transform_role - readAnalyticsData is transformed or generated (on a regular cadence) and documented to support reporting and analysis needs. Users can directly query this data, but can not write/updateMetabase (and other reporting tools) would read from this databasetransform_role - full accessreport_role - readWorkServes as an area to data exploration, modeling, and experimentationUsers can access, create/load, and change data in this areaanalyst_role - full access&nbsp;&nbsp; Other key assumptions with this set up: Each Snowflake Role has its own dedicated compute (Snowflake Warehouse). For instance the LOAD_ROLE can only run on a warehouse named LOAD_WH. The Raw database should contain a schema for each source system. &nbsp;For example, if we are syncing data from our ecommerce site with a Mongo backend, we would name the schema as MONGO_ECOMMERCE. &nbsp;In the Analytics database, we define 5 schemas:- src:&nbsp; holding dbt models of data that has been lightly transformed from the original source data. Here we may flatten nested variant data into a relational model. - trans:&nbsp;schema for intermediate dbt models - rpt: holding dbt dimensional models that drive our BI reporting- export: holding tables/views that are for data to be exported to systems outside of Snowflake -&nbsp;db_stats: metadata about our dbt models like total runtime, last runtime, number of rows added incrementally, etcThe specific tools in the diagram above are meant to be examples for certain roles:- Fivetran to sync raw data from source systems into RAW (could be meltano, airbyte)- dbt to create models in ANALYTICS (could be ML scripts via dagster/airflow)- Metabase to read from ANALYTICS for BI (could be Looker, Tableau, Superset)You need to consider the processes for Development and Testing of Pipelines + Models. This diagram does not describe how I would approach these topics. Snowflake configuration management with Terraform: Terraform (created by Hashicorp) is an open-source infrastructure as code software tool that enables you to safely and predictably create, change, and improve infrastructure. Chan Zuckerberg terraform provider for Snowflake: This is a terraform provider plugin for managing Snowflake accounts: GitHub -&nbsp;https://github. com/Snowflake-Labs/terraform-provider-snowflake Documentation available here: Terraform Registry We can use this framework to manage objects in snowflake such as: databasesrolesschemasuser accountspermission grantsGithub repo with my terraform configuration for this set up: https://github. com/jaygrossman/snowflake_terraform_setup&nbsp;&nbsp; PLEASE NOTE: This scope for this repo does not include the terrafom configuration to set up snowflake stages, functions, and file formatters. These objects often have more dependencies and require more advanced configuration, so I may plan to dedicate future blog posts to explaining the details. &nbsp; Folks who this would not have been possible without:: Collin MeyersRob SokolowskiTim Ricablanca"
    }, {
    "id": 21,
    "url": "http://localhost:4000/fun-first-project-with-my-3d-printer/",
    "title": "Fun First Project with my 3D Printer",
    "body": "2020/12/26 - I have long been curious about 3D printers and my daughter has been playing with CAD programs for her middle school STEM Club. &nbsp; So I decided to do some research and buy the Creality Ender 3 3D Printer Fully Open Source with Resume Printing DIY FDM Printer 220x220x250mm off Amazon for $165. I also bought 2 1KG spools of PLA Filament so I could print things. I am completely new to 3D printers, but i have put together my share of projects and I do engineering for my day job. I decided that 10pm Christmas Eve would be a good time to start assembling the printer, probably not the best decision. I open the box and there were a lot of pieces+tools and a single sheet with directions with only pictures. Two and a half hours later I had the printer assembled (I am pretty sure Santa's elves or someone experienced would have done it an hour) First Project - Toothpaste Squeezer: You ever get near the end of a tube of toothpaste and struggle squeezing out the last few uses? This is an annoyance for members of my family, so I figured this could be a good first project. So here's what I did: I went on Thingiverse. com and they have all sorts of amazing things you can 3D print (including the CAD files). They had several projects to choose from when I searched for toothpaste, and I chose the Simple Toothpaste Squeezer. Since it was my first project, I wanted something that was solid and with only 1 part to print. I download the . slt file from the project. I needed to convert the . slt file to a . gcode to use with my printer. I downloaded a great piece of free software that it seems like many people use - Ultimaker Cura. In Cura, I was able to choose my printer (the Ember 3) to automatically set up the file with correct settings and then  Slice  the file to convert it to . gcode. It also shows you how much filament will be used and how long the print will take. I saved the converted . gcode file to the provided SD card, inserted the card into the printer, and started my print. &nbsp;The printer begins drawing the outline of the project. It went around the edges and the holes many times. &nbsp;After the base layer was done, the printer began creating a honeycomb pattern as the Squeezer is not fully solid inside. I wasn't expecting this (being new to 3D printing) and thought it was worth sharing. The Finished Product:: The printing took about 45 minutes. When it was done, I popped the squeezer off the bed and the plastic felt pretty solid. &nbsp; So here is a picture of me trying it out (there's no hand modeling in my future): &nbsp; My Very Early Learnings:: I REALLY wish I had found this video before starting this adventure. It would have saved me so much time and answered so many of my questions. If you buy an Ember 3, watch this:There is a setting for the Ember 3 for voltage settings. I was a little disturbed when the machine would not power up until I switched it from 230V to 115V. Make sure everything is clean between prints. Make sure the extruder (where the filament comes out of)&nbsp;is cleaned out, especially if you are changing colors. &nbsp;Although it is a tedious pain, aligning the bed to the proper height before every print is really important (the video shows how to do this). I botched some subsequent print attempts when I didn't get the extruder close enough to the bed and the plastic moved all over the place. I love Thingiverse!!! The designs, comments, and remixes are just awesome. There is such nerdy awesomeness because people 3D print accessories to pimp out their 3D printers. I am going to add some things this week. My daughter made a few cool smaller designs with TinkerCAD. &nbsp;She saved her designs as . slt files, we converted them with Cura, and they were great when we printed them out. "
    }, {
    "id": 22,
    "url": "http://localhost:4000/image-processing-python/",
    "title": "Automating Image Processing with Python",
    "body": "2020/07/28 - If you have been reading this blog, you'll know I collect sports cards. It's fun to share what I have with other collectors (by posting scans on sportscollectors. net, facebook, etc. ). A few years ago I bought a Brother MFC-9130CW all in one printer/scanner that I use to do my scanning. I usually set it to scan documents in legal format so I can fit 9 cards on a scan (3 rows by 3 columns). And since I want to do this most efficiently, I generally save them as a 200dpi pdf file with multiple pages. The requirements: We will have a multipage pdf, with each page containing a 3x3 grid of equal sized card images. I will use this sample file for this exercise:&nbsp;sample_cards_file. pdf (3. 02 mb)We will need each image cropped to show the full card (we can have some extra space around it) and saved to it's own jpeg file. An example is show below: &nbsp;I will supply a directory path containing the outputted images. Since I will put them in my inventory database, I'll need to supply the starting number for the image names and have each subsequent image increment by one. The process:&nbsp; I like using Python for automating things, as it seems to have libraries for most things I want to do. &nbsp; So I figured it would be a good candidate for this project. The first thing we will want to be able to define some global variables#pathssource_file = '/tmp/sample_cards_file. pdf'out_dir = '/tmp/process/'# page setuprows = 3cols = 3# defines where the last run of this left off on (first item will be 1. jpg if 0)starting_count = 0#spacing offsetstop_offest = 0left_offset = 260bottom_offset = 0right_offset = 0spacer = 40vert_spacer=0We will need to do is convert the each page of the pdf to a jpeg file. Python has a library called pdf2image to do this:pip install pdf2imagepip install poppler# I used this syntax instead do of pip when running this in a jupyter notebook# conda install -c conda-forge pdf2image# conda install -c conda-forge popplerfrom pdf2image import convert_from_path# function that converts multipage pdf to individual# jpeg images. Function returns list of image paths. def convert_pdf_to_jpegs(pdf_path, out_dir):  file_paths=[]  pages = convert_from_path(pdf_path, 500)  page_count = 1  for page in pages:    image_path= {}temp_page_{}. jpg . format(out_dir, str(page_count))    #add to the list    file_paths. append(image_path)    #save converted file    page. save(image_path, 'JPEG')    page_count += 1  return file_pathsWe will have a function that breaks up an image with our 9 cards and save each individually to a specified directory. We will need the ability to seed the first image name. The function can use the spacing offset variablesfrom PIL import Imagedef split_images_from_page(image_path, out_dir, row_count, col_count, start_number):  # opens the image file   Im = Image. open(image_path)  # calculates height and width of image  full_width = Im. width  full_height = Im. height  image_height = int((full_height - vert_spacer - top_offest - bottom_offset) / rows)  image_width = int((full_width - spacer - left_offset - right_offset) / cols)    image_count = 0  row_current=1  #iterates through rows and columns  while row_current &lt;= row_count:    col_current = 1    while col_current &lt;= col_count:      # calculates the coordinates on the image to crop            croppedIm = Im. crop((left_offset + ((col_current - 1) * image_width) + spacer, top_offest + ((row_current - 1) * image_height), min(left_offset + (col_current * image_width) + ((col_current - 1) * spacer), Im. width), top_offest + (row_current * image_height) + vert_spacer))      # if you wanted to resize the image to 300 width and 420 height       # croppedIm = croppedIm. resize((300, 420))            # saves the image to specified directory      croppedIm. save( {}/{}. jpg . format(out_dir, start_number+image_count))          col_current += 1      image_count += 1        row_current+=1Calling the functions:import ospage_count = 1# convert pdf to jpeg file for each pagefile_paths = convert_pdf_to_jpegs(source_file, out_dir)# split each page into imagesfor file_path in file_paths:  number_start = starting_count + (page_count-1) * (rows * cols) + 1   split_images_from_page(file_path, out_dir, rows, cols, number_start)  page_count += 1# clean up delete jpeg files for each pagefor file_path in file_paths:  os. remove(file_path)&nbsp; Here is my ipython notebook with the code detailed above:&nbsp; pdf_scan_process. ipynb "
    }, {
    "id": 23,
    "url": "http://localhost:4000/open-source-data-tools-i-like/",
    "title": "Open Source Data Tools I like",
    "body": "2020/05/17 - A few months ago I participated on a panel talking about something data related at a Snowflake user group. &nbsp;In the picture below, I am in the white button down shirt and blue jeans looking less excited than the guy in plaid shirt on the left. Toward the end of the Q&amp;A session, the moderator asked the panelists what open source data tools do they use. I thought it would be nice to share my answers here as some of them were new to folks in the audience. These are all tools / frameworks my teams and i have experience with implementing: Site:&nbsp;https://www. getdbt. comTagline: dbt applies the principles of software engineering to analytics code, an approach that dramatically increases your leverage as a data analyst. Tech:&nbsp;Written in python So what problem does it solve? It is common for data folks to need to take raw data from a myriad of sources, load into a data warehouse, and then need to:- build new tables with aggregated data- take snapshots of data for time series analysis. &nbsp; As your data needs grow and you have more folks working on these types of tasks, you can start to see jobs with many steps daisy chaining one new aggregated table called by another. Understanding, testing, and debugging can get incredibly challenging (my teams faced this on a daily basis supporting hundreds of legacy aggregated tables/views). And it gets exponentially worse as subject matter experts leave your team/company. This was such a pain point that my team was in the process of creating our framework to tackle these types of problems. What is it? dbt (data build tool) is a command line tool that enables data analysts and engineers to transform data in their warehouses more effectively. &nbsp;&nbsp; Below is a picture of a modern data stack+pipeline - doing Extract, Load, Transform (known as ELT). &nbsp; dbt is handles the Transform step as shown below: The dbt framework essentially takes sql templates (jinja files) where define tables as . So your code may&nbsp; Once you run  dbt run  from the command line,&nbsp; the framework can figure out the order to build the aggregated tables (meaning build the tables that other downstream tables depend on). It even generates out a nice visual graphic of that dependency tree as part of the run sequence (shown below). This alone was a huge thing for us. &nbsp; dbt also allows us to define  tests  - SQL statements defined to validate some cases that you care about in the data of tables in the data warehouse. &nbsp; This is great to run on both source data (coming from upstream systems) as well as the dbt created aggregate data. This allowed us to find+monitor for all sorts of problems/inconsistencies in data from source systems that cost us so many hours of complex troubleshooting. I found this this&nbsp;excellent blog post&nbsp;in late 2017 and we a working proof concept within a week. It proved out many of our risk areas and it was quickly adopted by a variety of teams (you really need to know SQL to use it). Fast forward 2+ years, dbt has added all sorts of excellent features (including their own IDE and cloud offering) and has grown significantly in adoption.  Site:&nbsp;https://greatexpectations. ioTagline: Great Expectations (GE) helps data teams eliminate pipeline debt, through data testing, documentation, and profiling. Tech:&nbsp;Written in python While tests in dbt are really convenient+helpful, GE takes it to a different level. Here are some places I have used it: You can point GE at your existing database and it will generate out some nice documentation for you. We can define validation rules (called  expectations ) that can be run against flat files or database tables. Below are some basic examples of these rules: &nbsp;I have run these validation process for these expectations from jupyter, before file uploads, as part of ETL/ELT pipelines. You can even have the GE process generate out docs like this: Site:&nbsp;https://superset. incubator. apache. orgTagline: Apache Superset (incubating) is a modern, enterprise-ready business intelligence web application. Tech:&nbsp;Written in python So what problem does it solve? You have lots of data, GREAT! But you will probably want a way to build reports and visualizations so you can understand and share out what that data means to your business. There are great commercial offerings out there (like Tableau and Looker), but they are usually pretty expensive and have more overhead to administer. What is it? I am not going to go into detail of this here, since I wrote a blog post on my experience test driving SuperSet last year: http://jaygrossman. com/apache-superset-test-drive/ Site:&nbsp;https://debezium. ioTagline: Debezium is an open source distributed platform for change data capture. Start it up, point it at your databases, and your apps can start responding to all of the inserts, updates, and deletes that other apps commit to your databases. Tech:&nbsp;Written in Java I saved the most ambitious one for last. &nbsp; So what problem does it solve? It is common need for companies to want to be able to:- Sync all data changes between source systems (like production databases) and their reporting systems (data warehouse/data lake). - Build a real time series (without taking snapshots of tables) of all changes on key tables / data sets&nbsp; What is it? Debezium is a set of distributed services that capture row-level changes in your databases so that your applications can see and respond to those changes. Debezium records in a transaction log all row-level changes committed to each database table. Each application simply reads the transaction logs they&rsquo;re interested in, and they see all of the events in the same order in which they occurred. &nbsp; Change Data Capture, or CDC, is an older term for a system that monitors and captures the changes in data so that other software can respond to those changes. Data warehouses often had built-in CDC support, since data warehouses need to stay up-to-date as the data changed in the upstream OLTP databases. Debezium is essentially a modern, distributed open source change data capture platform that will eventually support monitoring a variety of database systems. The diagram below illustrates how debezium can be used as part of a CDC solution: &nbsp;Adrian Kreuziger has written an excellent blog post that provides a high level walk you through how he set this up for Convoy. It walks through considerations for PostGres (we used it with mySQL), Kafka, Kafka Connect, and Snowflake. In 2015, my team wrote something custom in Java to do some of what debezium does (capture change records from mySQL and write them to Kafka) to support requirements for real time time series. It was my first experience setting up Kafka and running it in production, so I learned a ton (both good and bad). Debezium (and KafkaConnect in general) were attractive because the creators seemed to have good options, it had configuration options out of the box, and we could get some community support. Please Note: There are some great commercial offerings in this space (FiveTran, Stitch Data, etc. ) that make it super easy to set up regular data sync'ing data between data stores. You should definitely consider those offerings before building your own version of this - managing kafka yourself is non trivial. We found that most vendor SLA's may not meet the needs of organizations with real time and near real time requirements (such as running a logistics operation with hundreds of workers on a warehouse floor). "
    }, {
    "id": 24,
    "url": "http://localhost:4000/your-dreasm-must-be-biogger-than-your-fears/",
    "title": "Your Dreams must be bigger than your fear",
    "body": "2019/11/03 - Last year I was eating lunch at a local Asian restaurant, and I pulled out an awesome message in my fortune cookie. It said, &ldquo;Your dream must be bigger that your fear. &rdquo;&nbsp; This message hits home for me, and this fortune is still attached to my monitor as a reminder. What is a dream and what is fear? A dream&nbsp;can be defined as a cherished aspiration, ambition, or ideal. It's something big that you really aspire or hope to accomplish at some point in the future. A good example of a dream is when Boston Red Sox pitcher Chris Sale fulfilled his dream by pitching in and winning the 2018 World Series. (Being a Sox fan, I was very happy to see this happen!) Fear&nbsp;can be defined as an unpleasant emotion caused by the belief that someone or something is dangerous, likely to cause pain, or a threat. An example could be that I have fear that many folks will not like my blog and troll me tirelessly, causing much public shame. So why don't people go after their dreams? I have met lots of folks who have big ambitions or aspirations, but are content to follow a path that is more comfortable. Jordan Jones of the Huffington Post wrote an article with her&nbsp;9 Reasons You Won&rsquo;t Pursue Your Dreams:&nbsp; Because there is an easier, safer path. Because we&rsquo;re waiting for the right time. Because we don&rsquo;t have enough time. Because we didn&rsquo;t succeed at first. Because we are too young. Because that&rsquo;s not what I&rsquo;m supposed to do. Because there&rsquo;s already something else out there. Because all you have is an idea. Because you trusted a lot of crappy advice over your own instincts. So how to counter your fear? Steve Harvey is an interesting guy who has had a pretty amazing career. He was homeless (living out of his car) in the 1980's as he would figure out how to make a run of his dream headlining stand up comedy. He'd go on to host  Showtime at the Apollo  and I am sure he saw lots of folks try to go after their dreams. &nbsp; I LOVE this video from when Steve being interviewed by Oprah! They give insightful advice to an audience member: The way to counter fear with the size of your dream. You have to want something so big that the size of your dream overcomes your fear. Start where you are and you have to know your gift (and capabilities). You have to be clear about your vision. if you want to go after our dream, you need to surround yourself with other dreamers (people that go after their dreams). He says,  Stop telling your big dreams to small minded people . Rock Pamper ScissorsVoucher code websitePowered by Embed YouTube Video&nbsp; I recently read a recent post by Ian Callum (Director of Design at Jaguar Cars) called&nbsp;Lessons learnt from 40 years as a designer. I liked this sentiment:  Success is not something that will land in your lap. You have to fight for it, break past your comfort zone and put yourself in situations that can be scary.   My fear story&nbsp;I have generally been more curious than cautious over the years, especially when it comes to opportunities. Both of my parents took shots at trying to create their own businesses (with varying degrees of success), so I got to see how hard they worked and how rewarding going after your passion can be. Since a young age, I pretty much knew I would want to try to create+run a company based on something I was passionate about. &nbsp;Way back in 1995, I was a junior in college and I found the internet for the first time. It was awesome! Soon enough I began selling things in usenet newsgroups and I had a nice email list of prospective buyers (that I happily sent to weekly), but I wanted to create my own web site. I had no background in this and no idea where to start. I asked around and none of my friends knew either. The whole thing was so foreign to me that it was scary. &nbsp;Thinking back almost 25 years ago, it feels like the stone age compared to today. Netscape 1. 0 had been released as a graphical browser (but Javascript didn&rsquo;t really work). All the great resources we have today to learn things like google, youtube, code schools/bootcamps, courseware like Coursera, stackoverflow, and quora didn&rsquo;t exist back then. &nbsp;I had some obstacles I had to overcome (but I found my way):&nbsp;obstacleremedyI didn&rsquo;t have a computer of my ownI used the Rutgers campus computer lab whenever I could get free timeI had no idea how hosting worked and I didn&rsquo;t have much money to pay for it&nbsp;I was lucky a student worker in the lab referred me to a free service - angelfire. comI had no idea how to code anythingI drove to a local book store and read an intro to HTML book (taking lots of notes on how all the tags worked, but not actually buying the book)I didn&rsquo;t have a digital camera or scanner to have pictures of my itemsI bribed a student worker at the computer center to show me how to use the one he had access toSo what happened If something is your DREAM, then you are likely motivated to go after it. I defined a first goal that was S-M-A-R-T (simple, measurable, attainable, realistic, timely). &nbsp;I needed to identify the obstacles that made me scared or apprehensive. I came up with ways to de-risk those obstacles. My fear was lessened and I took action!Once I got something (super ugly) working and people could contact me to buy things I had listed for sale, I realized the beginnings of my dream were possible and my fears subsided. I stayed on this path of teaching myself tech stuff, and by 2000 I had to skills to build my dream web site (sportscollectors. net). This first experience led me to go on to a fulfilling career building software projects and managing engineering teams build all sorts of cool things. &nbsp; Meet my friend Connie (a. k. a. the Corgi Queen) My day job employer (Rent the Runway) launched a mentorship program in 2015, where managers would get paired with contributors they usually don't work with. &nbsp;I was paired with a very nice + talented User Experience designer named&nbsp;Connie Cheng. She LOVES corgis (and has one named Lucy) to the point of mild obsession. Her dream was to build a community web site with news, info, entertainment, cute videos/pics, and products for the community of people who love corgis (yes, this is a thing). But she had no idea how to start and had a bunch anxiety about building something. I wanted to help since building my own projects have been so fulfilling. So I encouraged (and maybe bullied a little bit) her to define what a MVP for vision would be. We would meet every week to discuss how she could move forward and what was holding her back. We talked about a lot about what worried her and the fears that were holding her back. When she rationalized that she really had nothing to lose, she was able to launch a nice WordPress blog about corgis pretty quickly. &nbsp; Today she runs a successful (and profitable + bootstrapped) business revolving around CorgiThings. com. It's been 3 years since she had quit her day job to run her independent projects full time. She has a built a compelling brand and has designed lots of products corgi owners seem to enjoy. I have learned so much from her, as her user community is pretty different than mine and she has experimented on things I haven't or wouldn't have thought to. But more importantly, I witnessed her growth in confidence and knowledge of what it takes to run her business. Paying it forward - Building a Passion Project class Connie and I created a class called  Building a Passion Project  in 2017, that we run together as part of RTR EDU (Rent the Runway's internal education curriculum). Here is the marketing spiel: Has something ever bothered you that you wish existed and no one has solved it? Do you wish that something worked or worked differently? How about sharing/interacting with folks with your obsessions? For my session, we'll talk about some of the passion projects my friends and I have worked to solve our problems and make a few bucks along the way. We have run 5 sessions that have had probably a total of 150 or so students. The sessions detail: some of our experiences around following our dreamshow to overcome your fears and get startedworkshopping attendees through building something to following their passionsWe have an active slack channel open to folks who take the class (and some of our ambitious/curious friends). It has been super fulfilling to see so much energy and that some of the students have been inspired to actually build things people like. &nbsp; "
    }, {
    "id": 25,
    "url": "http://localhost:4000/fantasy-football-retirement-letter/",
    "title": "Fantasy Football Retirement Letter",
    "body": "2019/09/01 - Below is the ridiculous retirement email to members of my day job's fantasy league. Hope y'all enjoy. To my friends and fellow competitors, This is a tough thing for me to write, but I feel it is the right time. It is with some emotions that I am following in the footsteps of (the greatest tight end in NFL history) Rob Gronkowski, and announcing my retirement from fantasy football after 20+ years. Much like Gronk, winning another title last season was a physical + emotional struggle for me and I am looking forward to pursuing other interests. &nbsp; I feel it is only fitting to offer a silly side by side comparison of Gronk and Grossman over the past 5 years: "
    }, {
    "id": 26,
    "url": "http://localhost:4000/apache-superset-test-drive/",
    "title": "Apache Superset Test Drive",
    "body": "2019/03/02 - I have lately been playing with some commercial BI &amp; Dashboard tools. There is a certainly quite broad range when it comes to features, price, scalability, administration capabilities, how they can access data, and set up complexity. For a good sized enterprise (with 100+ users), some of these solutions can run you several hundred thousand dollars per year. As part of my diligence, I felt like I need to look at the best of breed Open Source offering - Apache Superset created by AirBNB. So I went to&nbsp;https://superset. incubator. apache. org/&nbsp;to check out the features and documentation. I was impressed, so I gave it a whirl. Installation They had some pretty simple instructions to get it running on Docker: 12345git clone https://github. com/apache/incubator-superset/cd incubator-superset/contrib/dockerdocker-compose run --rm superset . /docker-init. sh# you can run this command everytime you need to start superset now:docker-compose upThis created 3 docker images (superset, postgres to hold the configuration, redis to hold cached data): 123456&gt; docker psCONTAINER ID    IMAGE       COMMAND         CREATED       STATUS        PORTS          NAMESdca6c22fa844 superset_superset  /entrypoint. sh  6 days ago Up 4 days (healthy) 0. 0. 0. 0:8088-&gt;8088/tcp superset_superset_1 f22e0ca50545 postgres:10  docker-entrypoint. s…  6 days ago Up 4 days 0. 0. 0. 0:5432-&gt;5432/tcp superset_postgres_1 5ec0565adb58 redis:3. 2  docker-entrypoint. s…  6 days ago Up 4 days 0. 0. 0. 0:6379-&gt;6379/tcp superset_redis_1 In a browser, I went to http://localhost:8088 and saw this after logging in: The Use Case - Marc Lore's  Big 5  start up metrics I personally get much more out of learning or demo'ing some new tech when I can use it to find a solution to one of my (or my friend's or employer's) real life problems. So I wanted to build a dashboard in Superset that would help one of my personal projects. Marc Lore has founded some B2C companies that have had successful exits (thepit. com, diapers. com, jet. com) and is now running commerce for Walmart. I follow him on LinkedIn and saw his post this week:&nbsp; So I decided I would use Superset to show these metrics for my friend's company that I will not disclose the name (over 2 years old, profitable, growing, completely bootstrapped). In this blog entry, I will walk you through the ease of use of Superset's functionality by connecting to a datasource and building a visualization for the first item on Marc's list - tracking NPS. Setting up the data The first thing you need to do is set up connections to our data sources. Superset uses the SQLAlchemy&nbsp;python library for interfacing with database. If you are using a database other than sqlite, you may need to install the correct python library so that SQLAlchemy&nbsp;can reach your database as shown at https://superset. incubator. apache. org/installation. html#database-dependencies. &nbsp; I ran the following to install the mysql library on the docker running superset (calling the container_id from the docker ps command earlier: 12docker exec -it dca6c22fa844 bash pip install pymssql --userIn the top menu I went to SOURCES &gt; DATABASES. I set up a Database entry called  dw  that connected to mysql database: Superset allows us to then set up Tables that point at either a single table or view in whatever Database entries you create. You navigate to this by choosing the SOURCES &gt; TABLES from the top menu. &nbsp; Then I set up a table using the  dw  database pointing at a view in mysql called user_nps: Superset will look in the database to define the columns - default names, types, and if we treat them as dimensions can be overridden. The user_nps table in our mysql database contained 3 fields: FieldTypeDescriptionsurvey_date&nbsp; &nbsp;&nbsp;timestamp&nbsp; &nbsp;&nbsp;the date time of the surveyscoreintthe score the user provided between 1 to 10uidvarcharthe user's unique identifierI needed to understand categorize each user score as either a promoter, detractor, or passive in order to calculate an NPS score, as defined in&nbsp;https://customergauge. com/blog/how-to-calculate-the-net-promoter-score/. I could just show the calculated score per month, but I would prefer to show the breakdowns to get a better sense of the distributions of what my users think of this service. A Table in Superset allows me to create calculated fields (much like you could do in a sql view). In the table below, I have created 3 additional calculated fields: Here is the logic I used for the 3 calculated fields: survey_year = YEAR(survey_date)survey_month = MONTH(survey_date)nps_type = CASE  WHEN score&lt;7 THEN 'detractor'  WHEN score&gt;8 THEN 'promoter'  ELSE 'passive' END Building Visualizations In Superset, you can make Charts and you can insert those charts into Dashboards. Once I have a Table created, making a chart is somewhat straight forward. I did the following: Clicked on the NEW button in top menu option and chose the  Chart  option. I chose the table I was interested in using - workspace. user_nps. I was taken to the screen shown below and I filled out the options on the left menu:- Superset provides many types of visualization options, so I choose Bar Chart. - I provided the date range I wanted my chart to consider. - I chose the metrics, filter criteria, X-axis (Series&nbsp;+Breakdowns)&nbsp;and Y-axis (Metrics)I hit the  Run Query  button to visualize it and the  Save  button to save the chart for future use. As part of the command to save the Chart (show below), it provided the option to create a new Dashboard I called  Startup_Metrics : From the DASHBOARD top menu item, I can view any of the dashboards I have created. Below is a screenshot of the Startup_Metrics dashboard: There are many features for Dashboards. I can: Control the layout of the dashboard. I can add columns / rows / tabs, resize charts, and can even add markdown. Limit who may access or edit it (based on their roles). Set a cache limit to determine how often the data is to be refreshed from the source database. Create a permalink to send or email it to people. &nbsp;Explore any chart (click on the the 3 stacked dots on the upper right corner), allowing folks with permissions to view how it was created and make their own version. Limitations Since it leverages SQLAlchemy, there is a limitation that it does not provide the capability to join data across tables. This means that you must create a view with all the data elements needed for the Charts you want to build. Superset supports pretty simple aggregation, the complex and mulit-level groupings and calculations are probably better down as a view in a source systemIf you try to return more than a few thousand results to the browser, the performance becomes pretty slow. Superset does not have the ability to subscribe to a Chart or Dashboard so it would be emailed to a user on a specified interval, an important feature in most of the commercial dashboard solutions. Conclusions This doesn't have all the bells and whistles of Looker or Tableau. If you have a big enterprise that you want to launch self service reporting for hundreds of non-technical folks, this is probably not the best fit. The limitations above are things that would cause pain when a company scales to many simultaneous users. However, there are pros I love about it: Superset is free! It can provide a nice start for smaller companies and personal projects. &nbsp;The pre-canned docker-compose git project makes set up super easy. I was surprised how well it ran on my laptop. The learning curve was low, I hooked up to my database and built my first dashboard with 5 charts in under 2 hours. This thing has had different names, but it is over 3 years old and has went through lots of nice iteration. It's written in Python, which a large audience of data folks enjoy using. &nbsp; Since it is open source, we can fork and extend it as we please. "
    }, {
    "id": 27,
    "url": "http://localhost:4000/user-manual-for-jay-at-work/",
    "title": "User Manual for Jay at Work",
    "body": "2018/05/23 - I thought it would be a fun exercise to write a short document about what I index on for my day job and folks I work with. What matters most to me in a role:  Adding value to the world Building a sustainable business (legit P&amp;L) Learning personally Growing others Working with great peopleHow I think manager relationship works best (from the manger perspective): You are responsible for your career progression!I want to know your goals (if you can't express them succinctly, let's work on that together)We are going to put in a plan to accomplish themWe are going to continually visit this plan to make sure:To understand the progress toward the goalWhere to make adjustments as necessary (maybe the goal changes)What if my goals don&rsquo;t perfectly align with our team&rsquo;s goals? That &rsquo;s OK and somewhat expected. This will be a consideration of the plan we come up with together. We&rsquo;ll look into overlap and see if the team can offer opportunities for growth with current or future initiatives. &nbsp;If is it not a good fit, we can look for mentoring/networking opportunities from others inside or outside the org. Or even transition to another team. My Style (Some things about me): When possible, I gravitate to starting simple, seeing what works, and then layering on complexity. &nbsp;I am blunt and direct! I will ask questions when I don&rsquo;t understand or agree with things. I may still ask questions when I do agree with things. I will tell you my opinion of ideas. My ambition is to validate/support my &ldquo;5 priorities for the role &nbsp;Consistent alignment of goals and clear success criteria for projects are important, and must be defined before we do production implementation. I am drawn to Metrics Driven Development. While I will have opinions, I am open to how we get results. &nbsp;I am completely OK with someone proving my ideas/opinions are sub-optimal (in a timely manner, in the appropriate setting). Well thought out or super creative ideas can come from anywhere, not just exec/manager levels or subject matter experts. This is a learning opportunity for all of us!Action biased! I am more concerned with getting the right results than sticking to process and being consumed with tracking JIRA tickets. &nbsp;As technologists, it is our first responsibility to build the things that make the business successful. I vehemently agree with this statement I read in a blog: We believe that a predictable, reliable engineering team that builds the wrong thing is not a success. The engineering leader (and the&nbsp;engineers themselves) have to feel ownership and responsibility around what they are building, not just how they are building it.  Assumptions: We will not love working on every project we need to get done. Yes, on-call can suck (until we change things making it suck). There are some tradeoffs we will need to consider to help my goals of  Adding value to the world&rdquo; and  Building a sustainable business&rdquo;! I believe these are learning opportunities for us all and certainly worthwhile. I prioritize the high impact, reasonable effort activities over the fringe use cases. Project scope shouldn&rsquo;t change magically without discussion and agreement (especially requiring agreement by the team doing the work). I work a bit on the weekends and at night. This is my choice. &nbsp;I do not expect that you are going to work off hours. &nbsp;I might Slack you things, but unless the thing says URGENT, it can always wait until work begins for you on Monday. "
    }, {
    "id": 28,
    "url": "http://localhost:4000/snowflake-testimonial/",
    "title": "Snowflake Testimonial - Rent The Runway: Reinventing Retail with Data Driven Insights",
    "body": "2018/03/18 - In August, I spoke at the Snowflake analytics tour on the topic  Rent The Runway: Reinventing Retail with Data Driven Insights . I was also asked to provide an account of our experiences migrating data warehouse vendors - going from Vertica to Snowflake. Below is the video:  "
    }, {
    "id": 29,
    "url": "http://localhost:4000/number-one-espn-story_from_sportscollectors.net/",
    "title": "#1 news story on espn.com details something that happened on SportsCollectors.Net",
    "body": "2017/09/21 - In my 20's, I taught myself to code and I created what has become a fairly popular community for sports autograph collectors - SportsCollectors. Net. I have mentioned it a few times before on this blog. There is a really passionate collector who also happens to be a Major League baseball player named Pat Neshek. Pat is a really effective middle reliever and made his second All Star game this season. He is also known as one of the most generous players of giving his time and autograph to those who ask, and has even helped organize his teammates to answer their fan mail. &nbsp; I was pretty amazed when Pat joined SportsCollectors. Net in 2014 and introduced himself to the community. I know he has traded autographed items with some of the site's members. Here is an article of his collecting interests from a few years ago. So Pat is trying to get all the cards signed from a particular car set for his 3 year old son Hoyt (some are shown below). One of the cards he wanted to get signed featured Diamondbacks pitcher Zach Greinke, who would be his teammate at the All Star game. It is very common for players to sign items for each other, as former Yankee Mark Teixeira talks about Jeter signing hundreds of items for other players/coaches. So Zach promised he would sign the items for Pat the next time they saw each other, and then decided not to follow through. Pat posted his account of the incident on the site's message board yesterday. I get a message from a friend that SportsCollectors. Net was mentioned in a few news articles (it was a relatively slow sports news day I guess). &nbsp;A couple of hours later I log into espn. com and see the following as the first news story (and it remained in the top 3 all day): http://www. espn. com/mlb/story/_/id/20768268/pat-neshek-colorado-rockies-rips-zack-greinke-arizona-diamondbacks-alleged-autograph-snub&nbsp; Once on ESPN, the story got coverage on so many outlets - Yahoo, DeadSpin, CBS Sports,&nbsp;NY Post, BleacherReport, USA Today, Sporting News. And was a topic of many sports talk TV and radio shows. &nbsp; It is amazing to hear this story being discussed on Mike and Mike as I type this, but it also shows how much people are looking to promote sensational things over real sports stories. Mike Golic is on completely Pat's side, players should just sign things for other players. &nbsp; I am hoping another collector (maybe a SportsCollectors. Net member) is able to help Pat get his cards signed and there is a happy ending to the story. "
    }, {
    "id": 30,
    "url": "http://localhost:4000/simulating-vertica-conditional_change_event/",
    "title": "Simulating Vertica's conditional_change_event",
    "body": "2017/09/14 - Lately my team has spent a bunch of time migrating our data warehouse from Vertica to Snowflake. While Snowflake has excellent support for analytic functions, Vertica has some functions that no other columnar database supports. &nbsp; The conditional change event&nbsp;function  assigns an event window number to each row, starting from 0, and increments by 1 when the result of evaluating the argument expression on the current row differs from that on the previous row . CONDITIONAL_CHANGE_EVENT ( expression ) OVER ( . . . [ window_partition_clause ] . . . window_order_clause )&nbsp; An example using the conditional_change_event function So let's say we have the following table CCE_demo:&nbsp; This table contains chronological visits by users using a single browser. There were 2 visits from uid=10137196, then 2 visits by uid=15479000 and then 2 more visits by uid=10137196. &nbsp; We are interested in identifying each of these 3 groups of visits. A simple  GROUP BY uid  would result in 2 groups. 12345678SELECT  browser_id  , browser_visit_start  , uid  , (conditional_change_event(uid)     over (partition by browser_id order by browser_visit_start asc) + 1)     as cluster_idFROM CCE_DemoThe above SQL in Vertica would result in the following:&nbsp; How we can do this without conditional_change_event function TLDR; we will use a series of window functions. 1) we can define the first element in each group of visits with cluster_start=1 (using the LAG&nbsp;analytic function): 123456789101112131415-- identify the first record in each groupSELECT  browser_id  , browser_visit_start  , uid  , CASE    WHEN      LAG(uid) over (partition by browser_id         order by browser_visit_start asc) IS NULL THEN 1    WHEN      LAG(uid) over (partition by browser_id         order by browser_visit_start asc) != uid THEN 1  ELSE 0  END as cluster_startFROM CCE_Demo 2) Identify the unique groups: 12345678910111213141516171819202122232425262728-- identify the first record in each groupWITH a as (  SELECT    browser_id    , browser_visit_start    , uid    , CASE      WHEN        LAG(uid) over (partition by browser_id           order by browser_visit_start asc) IS NULL THEN 1      WHEN        LAG(uid) over (partition by browser_id           order by browser_visit_start asc) != uid THEN 1    ELSE 0    END as cluster_start  FROM CCE_DEMO)-- get the unique groupsSELECT    browser_id  , browser_visit_start  , uid    , cluster_start  , ROW_NUMBER() over (partition by browser_id     order by browser_visit_start) as cluster_idFROM aWHERE cluster_start=1ORDER BY browser_id, browser_visit_start, uid 3) We can create date windows for each cluster since we know when each one starts: 1234567891011121314151617181920212223242526272829303132333435363738394041-- identify the first record in each groupWITH a as (  SELECT     browser_id    , browser_visit_start    , uid     , CASE       WHEN         LAG(uid) over (partition by browser_id           order by browser_visit_start asc) IS NULL THEN 1       WHEN        LAG(uid) over (partition by browser_id           order by browser_visit_start asc) != uid THEN 1     ELSE 0     END as cluster_start  FROM CCE_Demo),-- get the unique groupsb as (  SELECT       browser_id    , browser_visit_start    , uid       , cluster_start    , ROW_NUMBER() over (partition by browser_id       order by browser_visit_start) as cluster_id  FROM a  WHERE cluster_start=1  ORDER BY browser_id, browser_visit_start, uid )-- assign end dates to the groups, set last group's end date to 2100-01-01 SELECT    browser_id  , browser_visit_start  , uid  , cluster_id  , COALESCE(      LEAD(browser_visit_start) over      (partition by browser_id order by browser_visit_start asc),      '2100-01-01') as end_date    FROM b 4) Use the date ranges (between browser_visit_start and end_date) to designate the group: 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758-- identify the first record in each groupWITH a as (  SELECT     browser_id    , browser_visit_start    , uid     , CASE       WHEN         LAG(uid) over (partition by browser_id           order by browser_visit_start asc) IS NULL THEN 1       WHEN         LAG(uid) over (partition by browser_id           order by browser_visit_start asc) != uid THEN 1     ELSE 0     END as cluster_start  FROM CCE_Demo),-- get the unique groupsb as (  SELECT       browser_id    , browser_visit_start    , uid       , cluster_start    , ROW_NUMBER() over (partition by browser_id       order by browser_visit_start) as cluster_id  FROM a  WHERE cluster_start=1  ORDER BY browser_id, browser_visit_start, uid ),-- assign end dates to the groups, set last group's end date to 2100-01-01 c as(SELECT     browser_id  , browser_visit_start  , uid  , cluster_id  , COALESCE(      LEAD(browser_visit_start) over       (partition by browser_id order by browser_visit_start asc),      '2100-01-01') as end_date     FROM b)-- use the window between browser_visit_start and end_date to assign the correct group to each recordSELECT  d. browser_id  , d. browser_visit_start  , d. uid  , cluster_idFROM CCE_Demo dLEFT OUTER JOIN c  ON d. browser_id=c. browser_id  AND d. uid=c. uid  AND d. browser_visit_start&gt;=c. start_date  AND d. browser_visit_start&lt;c. end_dateORDER BY   d. browser_id, d. browser_visit_start &nbsp;&nbsp; Other things we could have done The real data we have in our database that this example was based off of a table with 250 million rows. So taking advantage of scalable query processing in an MPP database was desirable. 1) Build a custom user defined function (using javascript and a FOR loop). 2) Export it to a language like Python or Java and use a FOR loop. &nbsp; "
    }, {
    "id": 31,
    "url": "http://localhost:4000/lateral-flatten-in-snowflake/",
    "title": "Snowflake's lateral flatten function on variant data type",
    "body": "2017/08/25 - Snowflake is a really interesting new data warehouse built on top of AWS. I like their architecture because they had the interesting idea to separate data storage (backed by small files on S3) and compute to run queries (EC2 instances running their API). I inherited a project where we would store complex JSON in a string in a field as varchar(64000). Then we would use regex patterns to get the values we wanted from them. Sometimes these regexes would get really involved, yuck. The Variant data type Snowflake offers mechanism to store semi structured data in field that is easy to parse - the Variant data type. &nbsp; In snowflake I have a table variant_demo with a single Variant field named json_data as shown below: If we were to click on the field, we could see the JSON elements as shown below: If I wanted to get the value of the  membership  field, I could write the following SQL: 12SELECT json_data:membership::string as membership FROM variant_demo; Now what if I wanted to pull out the list associated with  products  and join them to our products table? Lateral Flatten Function Snowflake has this really cool function that allow us to normalize a row from a list from JSON attribute from variant field. The SQL below uses lateral flatten to take the items in the list from json_data:products make them their own dataset: 12345678910WITH p as (SELECT  json_dataFROM  variant_demo)SELECT  b. value::string as product_styleFROM p,  lateral flatten(input=&gt; p. json_data:products) b  Once you have a nice clean record set, we can join on data with other tables: 1234567891011121314WITH p as (  SELECT     json_data  FROM    variant_demo) SELECT   b. value::string as product_style  , designer  , list_priceFROM p,  lateral flatten(input=&gt; p. json_data:products) b   INNER JOIN products prod     ON v. value::string=prod. style "
    }, {
    "id": 32,
    "url": "http://localhost:4000/creating-a-custom-jenkins-plugin-with-jruby/",
    "title": "Creating a Custom Jenkins Plugin with JRuby",
    "body": "2015/01/25 - I have seen scenarios where I would like to not have Jenkins not execute builds under certain circumstances. One common example is with the&nbsp;Maven Project Plugin, it will often update the . pom file and check it back into source control. When you have Jenkins jobs that poll source code repos for new commits, your builds will enter an endless cycle of triggering more builds. &nbsp;It would be nice to have the ability to stop builds from executing when the commit contained a specified phrase. I searched for such a plugin with no luck, so I have created my own plugin (that I use in production):&nbsp;https://github. com/jaygrossman/jenkins-ignore-commit-plugin&nbsp;&nbsp;Since there wasn't much detail for creating custom plugins in Ruby that I could find, this blog post will walk through the process. &nbsp;Options for making Jenkins Plugins1) Maven (default)2) JRuby&nbsp;&nbsp;&nbsp;Since&nbsp;setting up . pom files&nbsp;always seems painful to me, I wanted to try the Ruby option. &nbsp;I found this very light post from 2013 that showed a few examples and the jpi gem with not much explanation:https://wiki. jenkins-ci. org/display/JENKINS/Jenkins+plugin+development+in+Ruby&nbsp;&nbsp;Setting up a JRuby Plugin Development Environment&nbsp;I like to do all my development in reproducible environments when possible, so I set up a vagrant environment for building and testing Jenkins Plugin development &amp; testing. &nbsp;My&nbsp;Vagrantfile&nbsp;installs the following dependencies on Centos 6. 5:Java 1. 6Maven&nbsp;Jenkinsrbenv &amp; jruby 1. 7. 9jpi gemSetting up a JRuby Plugin Development Project&nbsp;1) Create a directory with the name of your project (jenkins-ignore-commit-plugin)&nbsp;2) Create a pluginspec file (jenkins-ignore-commit-plugin. pluginspec):&nbsp;12345678910111213141516171819Jenkins::Plugin::Specification. newdo |plugin|  plugin. name =  jenkins-ignore-commit-plugin   plugin. display_name =  Ignore Commit Plugin   plugin. version = '0. 0. 1'  plugin. description = 'Commits that contain a supplied phrase within the   commit messages will be skipped. '    # You should create a wiki-page for your plugin when you publish it, see  # https://wiki. jenkins-ci. org/display/JENKINS/Hosting+Plugins#HostingPlugins-AddingaWikipage  # This line makes sure it's listed in your POM.   plugin. url = 'https://wiki. jenkins-ci. org/display/JENKINS/Ignore+Commit+Plugin'    # The first argument is your user name for jenkins-ci. org.   plugin. developed_by  jaygrossman ,  Jay Grossman &lt;jay. grossman@org&gt;     plugin. uses_repository :github =&gt;  jaygrossman/jenkins-ignore-commit-plugin   # This is a required dependency for every ruby plugin.   plugin. depends_on 'ruby-runtime', '0. 12'end 3) You'll need these gems: jenkins-plugin-runtime, jpi, jruby-openssl. Here is my Gemfile: 12345678910gem  jenkins-plugin-runtime ,  ~&gt; 0. 2. 3 group :development do  gem  jpi ,  ~&gt; 0. 3. 8   gem  jruby-openssl ,  ~&gt; 0. 8. 8   gem  rake ,  ~&gt; 10. 0. 4   gem  pry   gem 'coveralls', require: false  gem 'rubyzip',  ~&gt; 0. 9. 9 end4) If your plugin requires user interface elements (such as checkbox, textbox, textarea, password) such as the items shown below, you'll want to create a View.  To set up the View: Create views sub-directory. Create a plugin sub-directory with views (I called my directory views/ignore_commit). &nbsp;Create a file named config. erb in that directory. The entry() function can define the form element, title, field id, and description as shown below:1234567&lt;% f = taglib( /lib/form )f. entry(:title =&gt; 'phrase', :field =&gt; 'ignore_commit_phrase', :description =&gt;  Commits containing this phrase will be considered NOT_BUILT ) dof. textboxend%&gt;5) Next you'll want to create a model to do the actual work for the plugin. a) create a models sub-directoryb) create a file plugin. rb file (mine is called ignore_commit. rb):12345678910111213141516171819202122232425262728293031323334353637383940414243class IgnoreCommit &lt; Jenkins::Tasks::BuildWrapperdisplay_name  Ignore Commits with Phrase attr_accessor :ignore_commit_phrasedef initialize(attrs)  @ignore_commit_phrase = attrs['ignore_commit_phrase']end# Here we test if any of the changes warrant a builddef setup(build, launcher, listener)  begin  changeset = build. native. getChangeSet()  # XXX: Can there be files in the changeset if it's manually triggered?  # If so, how do we check for manual trigger?  if changeset. isEmptySet()    listener. info  Empty changeset, running build.      return  end  logs = changeset. getLogs()  latest_commit = logs. get(logs. size - 1)  comment = latest_commit. getComment()  if comment. include? ignore_commit_phrase    listener. info  Build is skipped through commit message.      listener. info  Commit: #{latest_commit. getCommitId()}     listener. info  Message: #{comment}         build. native. setResult(Java. hudson. model. Result::NOT_BUILT)    build. halt( Build is skipped by Ignore Commit Plugin.  )  end  rescue    listener. error  Encountered exception when scanning for filtered paths: #{$!}     listener. error  Allowing build by default.      return  end  listener. error  Encountered exception when looking commit message: #{$!}   listener. error  Allowing build by default.    endendBuilding the code and generating the Jenkins Plugin From within the project directory, run: 1jpi buildThe JPI gem executes a build via maven and will compile a . hpi file (plugin binary file that can be uploaded into Jenkins) in a pkg sub-directory: You can then manually install the plugin from the Plugin Manager upload page in Jenkins. If you have a local&nbsp;&nbsp;local Jenkins environment (like in our vagrant set up), you can run the following commands to upload the plugin to it: 12bundle update rakejpi serverRunning the Build and Testing Vagrant image 1) Download this Vagrantfile and put it in the root of your plugin project directory: https://github. com/jaygrossman/jenkins-ignore-commit-plugin/blob/master/Vagrantfile 2) Run this command to build the plugin and load it into a local Jenkins instance: 1vagrant up 3) Once the vagrant up execution is complete, paste the following link into a web browser on the host machine to view the Jenkins instance running in the vagrant: 1http://localhost:58080"
    }, {
    "id": 33,
    "url": "http://localhost:4000/measuring-understanding-predicting-subscriber-value/",
    "title": "Measuring, Understanding, and Predicting Subscriber Value",
    "body": "2014/06/16 - The&nbsp;subscription business model&nbsp;is a business model where a customer must pay a subscription price to have access to the product/service. Below are some examples of some popular commercial subscription offerings: Salesforce. com has built a popular brand offering CRM functions. Amazon, Microsoft, etc. offer hosting infrastructure. &nbsp;WeightWatchers and eDiets offer access to their diet information, tools, and community. Getty offers download of stock photography. Netflix and Hulu offer access to view streamed movies. Odeo and Enigma. io offers access to reference data. &nbsp;It is becoming more common that people would rather subscribe to products and services than buy them outright. I am such a big fan of the model for web/software based applications that I implemented it successfully in 2002 on SportsCollectors. Net. The obvious differentiator for a subscription business vs. selling a sku&rsquo;d product is the potential for recurring revenue. Most subscription businesses are inherently engineered making it easy for subscribers to continually want to pay for access to a valuable product. This eliminates the need for additional costs of acquisition or sales cycles. Measuring Subscription Models As with any business, I am usually interested in understanding its profitability and growth. In the case of subscription models, I look at the profits and growth at both a company/product level and at a user level. I monitor the following metrics to help evaluate performance: Recur Revenue (RR) for time periodHow much subscription based revenue does the business generate. &nbsp;RetentionLength of time that users are willing to pay for the subscription fees. &nbsp;Total Lifetime Value (TLV)All Revenue opportunities and estimated value associated with the user&rsquo;s subscription. I break this down into 4 categories:&bull; &nbsp;Subscription Income&bull; &nbsp;Product/Affiliate Sales&nbsp;&bull; &nbsp;Residual Income (advertising, data/content licensing, etc. )&bull; &nbsp;Brand Enhancement (subscribers can become product evangelists)&nbsp;Customer Acquisition Cost (CAC)Costs (marketing, sales, etc. ) associated with onboarding the user. &nbsp;Operational Costs&nbsp;Non-growth spend, the costs of operating the business. This includes hosting infrastructure, developers/designers, license fees, salaries, cost of goods sold, general/administrative, R&amp;D, etc. Measuring a Subscription Company/Product As an example to illustrate, let&rsquo;s say our company offers a subscription product for $10/month. We currently have 500 active subscribers. Each month we average 55 new subscribers and lose 40, with subscription length averaging 5 months long. We calculate an average of $5 of lifetime value for each user related to non-subscription income. The CAC is $8 per user and our operations costs $1000 monthly. Profitability is the most obvious thing we can look at. Most people would agree with the definition of profit = revenues &ndash; cost. &nbsp; Month&rsquo;s subscription income = (500 subscribers * $10 subscription price) = $5000Month&rsquo;s non -subscription income = ($5 non-subscription income / 5 months average length of subscription * 500 subscribers) = $500Month&rsquo;s acquisition costs = ($8 CAC * 55 new subscribers) = $440Operations cost = $1000 Monthly Profit Margin = Month&rsquo;s subscription + Month&rsquo;s non -subscription income - Month&rsquo;s acquisition costs - Operations costMonthly Profit Margin = $5000 + $500 &ndash;$440 - $1000 = $4060 Then I like to look at growth measures. How many people are we bringing and how many are staying can tell us a lot. &nbsp; New Subscriber Ratio = New Subscribers / All SubscribersNew Subscriber Ratio = 55 / 500 = 11%&nbsp; Monthly Recur Revenue Growth = (Current Month&rsquo;s Recur Revenue &ndash; Previous Month&rsquo;s Recur Revenue) / Previous Month&rsquo;s Recur RevenueMonthly Recur Revenue Growth = ($5000 &ndash; $4850) / $4850 = 0. 31% So adding more active subscribers than you lose is almost always a good thing, but I am interested in whether it is cost effective. Growth Efficiency measures how much it costs you to acquire $1 of Acquired Customer Value: Growth Efficiency = TLV for average user / (CAC for average user&ndash; Operational Costs for average user)Growth Efficiency = ((5 month subscription * $10 subscription fee) + $5 non subscription income) / ($8 CAC + ($1000 operations cost/500 subscribers * 5 month subscription))Growth Efficiency = $55 / ($8 + $10) = 3. 06 In businesses that scale well, we&rsquo;ll see a linear representation showing as the Operations Costs per user decline as they get spread out as more subscribers are added. Hence growth in subscribers will result in greater profitability and growth efficiency. &nbsp; Measuring a Subscriber If we assume SubscriberX is fairly typical example of the population, and subscribes to the service for 6 months and we invested $7 in CAC to bring him onboard. The revenues for the SubscriberX are represented by the TLV : (6 month subscription * $10 subscription fee) + $5 non subscription income = $65 The estimated operations cost per user is calculated by taking the overall operations cost, dividing by the number of subscribers, and multiplying it by the number of months the user subscribes to the service. Combining operations and customer acquisition cost give us: &nbsp;$7 CAC + ($1000 operations cost/500 subscribers * 6 month subscription) = $19. &nbsp; SubscriberX represents $46 ($65 TLV - $19 cost) of lifetime profitability. Growth Efficiency = TLV for SubscriberX / (CAC for SubscriberX &ndash; Operational Costs for SubscriberX)Growth Efficiency = ((6 month subscription * $10 subscription fee) + $5 non subscription income) / ($7 CAC + ($1000 operations cost/500 subscribers * 6 month subscription))Growth Efficiency $65 / ($7 + $12) = 3. 42 Understanding User Value Just because we can measure the LTV of a subscriber, does not necessarily mean we understand it. It&rsquo;s important to understand why some subscribers have higher LTV than others. This will allow us to identify actions we can take to attempt to increase an individual subscriber&rsquo;s LTV. Although each subscription business is unique and may have different variables describing users&rsquo; options and actions, we can take machine learning approaches to gain insight. We should hopefully be able to define the key variables that go into solving for LTV as the target feature. I usually start by getting a high level understanding of the distribution of the population&rsquo;s LTV. By getting the summary and generating histograms for LTV and log(LTV), I&rsquo;ll get the structure of distribution (if it is normalized) and size of the deviation. I then do the same for the components that comprise LTV (Subscription Income, Product/Affiliate Sales, Residual Income, Brand Enhancement). Then I can use the identified variables and create an optimized decision tree solving for LTV. The partitions of the data identify the most important features and the decision points. I'm not just interested in solving for LTV, but I want a way to estimate the relationships among variables. So I'll use Regression analysis with backwards elimination to find the set of variables that best explain the variance (highest r-squared) and lowest error. &nbsp; For example, on one site I noticed that members that log into the site 5+ days a week and average 2+ logins per day are most likely to highest LTV. The guidance provided was to take steps to improve user engagement and incent users to return regularly. Enhanced alerting features were added, leading to increases in subscriber visitation and LTV. Predicting User Value Making predictions is another common machine learning task. Since the structure of our data is defined (we know the target feature and the key variables), there are quite a few supervised learning options that can be used to make predictions (linear regression, CART, support vector machine, K-Nearest Neighbors, etc. ). Predicting LTV I have found it to be valuable to predict the LTV at different time intervals. For instance, I want to predict the LTV of a subscriber based on the first month&rsquo;s activity. Then the second month, third month, sixth month, etc. It can open up interesting possibilities such as: Better understanding behavior as it relates to changes in LTVCategorizing members to make recommendations/introductionsIdentify candidates where early intervention methods may increase LTV or retentionI set up templates for a number of predictive algorithms and feed each the subscriber data set. To avoid overfitting, cross validation support is implemented as part of the algorithms. I will most often use the one returning the lowest RSME or a blend of the most accurate ones. Predicting Retention Subscription sites generally want the highest retention possible, as it translates to greater LTV and profits. Another metric I look to gather is the probability that the member will continue to pay for the service during the upcoming billing period. &nbsp; This becomes a binary classification problem, predicting whether a subscriber will be retained for the following month. The model considers variables related to each subscriber&rsquo;s lifetime activities and current month&rsquo;s activities. My modeling uses a combination of logistic regression, k-Nearest Neighbors, and CART, also looking for the optimal RMSE. Conclusions The framework described in this post has allowed me to gain better understanding of my subscription business and guided my activities in the following areas: Product DevelopmentMarketingCustomer ServiceCreating a representation for the Brand Enhancement component of LTV is usually not a straight forward exercise. I have attributed value for: subscribers (or that have web sites) that refer trafficmoderators and adminssubscribers that host activities/events"
    }, {
    "id": 34,
    "url": "http://localhost:4000/web-enable-r-models-with-shiny/",
    "title": "Web Enable R models with Shiny",
    "body": "2014/03/24 - Last September I was preparing for my fantasy football league and I found this really cool&nbsp;web app&nbsp;that suggested the optimal draft recommendations based on the rules of your league. I then found the associated article on r-bloggers that explained how it was done: http://www. r-bloggers. com/win-your-fantasy-football-snake-draft-with-this-shiny-app-in-r/ Shiny is the secret sauce to make this possible: The Shiny package makes it super simple for R users like you to turn analyses into interactive web applications that anyone can use. Let your users choose input parameters using friendly controls like sliders, drop-downs, and text fields. Easily incorporate any number of outputs like plots, tables, and summaries. No HTML or JavaScript knowledge is necessary. If you have some experience with R, you&rsquo;re just minutes away from combining the statistical power of R with the simplicity of a web page. So Shiny takes advantage of a familiar Client-Server pattern. The package will generate HTML layout and javascript code (I see includes for bootstrap. js), that calls wraps calls to our R logic CRAN has the full reference for the shiny package. &nbsp;Download pdf here Nice Intro to Shiny This week I got an email from&nbsp;Vivian Zhang&nbsp;that she was going to introduce Shiny as part of her weekly NYC Open Data meetup. Since it was walking distance from my office, I decided to attend. She posted a blog entry that does a pretty good job detailing the session (along with a video of the entire 2 hour session): http://www. nycopendata. com/2014/03/25/building-interactive-web-app-with-shiny/ The project for the class was to create a Shiny project to run linear regression and return the summary and plot of the summary. I was able to solve this and Vivian included&nbsp;my code. So here are the key takeaways: You need R version 3. 0+. &nbsp;Download hereYou need R-Stuidio. &nbsp;Download hereThe Shiny tutorials and examples are available on the&nbsp;rstudio site, and they are really helpfulYou'll need to create ui. R script. This is where you define the form controls (dropdown lists, numeric inputs, text boxes, check boxes, radio buttons, file upload) and the output panels to display the results of your models (text, tables, plots). &nbsp;You'll need to create server. R script. This script takes in the form inputs, processes logic, and returns the content to output to the user. &nbsp;Fun toy I built with Shiny&nbsp;Last June I did some work using R to build models to&nbsp;predict Ebay Sales. I thought this would be a really great opportunity to use Shiny to web enable those models. On Thursday (3/27), I was able to create the Shiny app show below on my round trip train commute in one day (roughly 2 hours):&nbsp; The user is prompted to: select a playerselect the categoryenter the seller's ebay idindicate whether item is authenticatedenter the minimum bidThe application returns:&nbsp;historical sales reference info for the sku (player+category)the predicted probability that the listing will result in a salethe predicted sale price and predicted price bucket (category)I have shared my code and a very limited sample data set on github at:&nbsp;https://github. com/jaygrossman/EbaySalesPredictionShiny&nbsp;&nbsp;This is little app is going to make it so much easier for me to determine deals on eBay, as it's much easier than munging some data and firing up R on the command line,&nbsp;"
    }, {
    "id": 35,
    "url": "http://localhost:4000/auction-framework-mvp/",
    "title": "My Auction Framework MVP",
    "body": "2014/03/04 - I have been a huge fan of eBay since the moment I first found it (back in 1997 or so). It truly was the world changing marketplace because: It quickly gained critical mass with worldwide populations of buyers and sellers. The auction format is easy to understand and allowed the market to determine the prices. &nbsp;It was an early web 1. 0 company that thrived after the dotcom bust of 2001. Over the years, I have seen both start ups and established companies launch would be competitors to eBay. Some try to stand up some cheap package and others build a custom enterprise solution, but just about every one of them dies off with a lack of interest or sustainable business model. I have to admit, I have always thought it would be a lot of fun to stand up auction functionality (either as part of an existing site or as a stand alone project). I have built out 2 pretty feature rich versions customizing some canned scripts (one in ASP and another in PHP), but I did not feel they offered enough value to bring to production. I wasn&rsquo;t really happy with the quality of the user experience of either solution and didn&rsquo;t really want to commit myself to needing to extend either of those technologies. The Project I am a big fan of quickly launching usable features and then iterating on them based on the learning from users&rsquo; interactions (lean/agile development). Since I run a community full of prospective auction buyers and sellers (SportsCollectors. Net), I thought it would be a good place to test this type of functionality. &nbsp; The first thing I needed to do was to determine what use cases my&nbsp;Minimum Viable Product&nbsp;(MVP) would need to accommodate in order to provide value. My vision was to create some of the core user experience of eBay circa 1999 (well before the recommendations and the magento shopping cart integration). &nbsp; Since SportsCollectors. Net already had a design, an active user base, user/login functions, messaging, and user rating/feedback, I was able to leverage these elements and not even have to spend time on them. So I could focus on these core auction related functions: Buyers need ability to browse/search for active auctionsBuyers need the ability to view the auction detailsBuyers need the ability to place bids (with proxy logic)Bidders need to see a list of active auctions they have bid on. Winning bidders need to be sent payment instructions once the auction has ended. This included associating the auction with the Trade Manager framework so feedback could be given. Sellers need the ability to create standard auctions (no support for reverse, dutch, or fixed price formats). This must include support for image uploads, payment methods, and categories. &nbsp;The next thing I did was to create a very basic entity model that defined the key objects in the system, the porperties to describe them, and the actions they can perform. The diagram below shows the corresponding data model (which has since been refactored a bit): &nbsp; I spent about 3 days of my train commute (2+ hours per day) coding the user functionality on the site. Below are screens of the browse/search and auction details pages:  The proxy logic for bidding took the most time of any function. There are a few involved scenarios to accommodate, like when the bid is not high enough or when high bidder raises the maximum bid amount. &nbsp;It was certainly helpful to have built out robust unit tests (validating all the fringe scenarios) before writing any implementation code. I was really happy to not have to worry about creating a look and feel or generic user management functionality. The look and feel can especially be a huge time suck. The Test I decided I would run a test of 50 auctions of vintage sports cards of star players (mostly Hall of Fame caliber). All auctions would run for 14 days and have a starting bid of $0. 25 (the median retail value of the lots was around $8). I figured the best time to have them end was on a Sunday night between 8-10PM EST. I spent another 2 more hours scanning the cards and entering the information for the auctions. To expedite the process, I wrote a small console application to trigger a scan and then divide a scanned image into individual images based on coordinates &ndash; much like a photoshop macro. To let folks know about the new functionality, I posted a simple announcement the site&rsquo;s&nbsp;message board&nbsp;and the site&rsquo;s&nbsp;Facebook page: &nbsp; Other than these general announcements, I provided no additional information or instructions to how everything would work. My hypothesis was that this member base was familiar enough using auctions to figure out how it would work. In the event there would be questions, they could reply to the message board post. &nbsp; The Results I&rsquo;d like to think the auction was pretty easy to use, as people used it to get some good deals. It had 43 different bidders made a total of 163 bids. &nbsp;47 of the 50 auctions resulted in sales (85%), for a total of $65. 19 ($1. 38 average). &nbsp; On the final day, the auctions saw 24 bids, 10 of which were leading bidders raising their proxy bid amounts. I had figured there would be some items were people would bid in the last few minutes, but it did not happen. Of course I didn&rsquo;t take into account that the Oscars and a new Walking Dead episode may have distracted people during that window. There was a theme on the message board (which saw 30 replies to the announcement) and via personal messages/emails, that members liked the auction feature. Many had expressed interest in using it to offer their items for sale. I am interested to see if everyone pays for their winnings, and how timely the payments are. We should know over the next few weeks. I am also interested to see the preferred payment methods for buyers (Paypal, check, money order, or cash). Conclusions This was fun and I am happy I decided to 1) build this MVP during my commute and 2) run a set of auctions. I think the feature set that was part of this first test offered the right functionality. &nbsp; I intentionally made starting bid very attractive to promote interest, as well as picking items (vintage star cards) that I knew would garner interest at these low price points. I likely spent more on the cards than what they sold for, but it was well worth it to get some tangible feedback on the idea. I have not yet decided if an auction service is viable to run &ndash; either to be added permanently to the site (only open to members) or living as a different site. &nbsp; I have also considered adding the following enhancements and offering the framework to others (maybe open source): Buy it now and fixed price listingsAllowing other sellers can listOptions to charge listing and/or final value feesWatchlists for biddersNotification when outbidNotification when your auction is sold (for seller)"
    }, {
    "id": 36,
    "url": "http://localhost:4000/between-the-numbers/",
    "title": "Between the Numbers",
    "body": "2013/11/21 - I am a huge baseball fan, and I am especially excited that my Boston Red Sox won the 2013 World Series. Part of my love for the game and math came from following the statistical performance of different players and comparing them. I can remember the endless debates with my friends as a kid about who was better, Wade Boggs or Don Mattingly. My dad and his friends debated their generation&rsquo;s stars - Ted Williams vs. Joe Dimaggio vs. Mickey Mantle. Until the past 10-15 years, there was a standard set of stats that people used to measure batter performance. Things like batting average, home runs, runs batted in (RBI), hits, runs, etc. The announcers would always talk about these as the most important measurable aspects for position players (non-pitchers). I recently re-read a really good book that was re-released in 2005,&nbsp;Baseball Between the Numbers: Why Everything You Know About the Game Is Wrong. A team of data enthusiasts and stats folks (including Nate Silver of NY Times election prediction notoriety) came up with a new set of statistical measures to quantify a single player&rsquo;s value objectively. It&rsquo;s a really good read for the serious baseball fan and I picked up a copy for $10 delivered. The key element is the Wins Above Replacement (WAR) framework, that represents how much better a player is than what a team would available to replace them. The model takes into account hundreds of different quantifiable attributes of a player&rsquo;s performance (including hitting, defense/fielding, throwing, running) to find a methodology to compare all players and pitchers. The authors went to great pains to make sure the model would normalize differences in baseball team&rsquo;s home stadiums (it is much easier to hit home runs at Yankee Stadium than Safeco Field in Seattle) and across different eras. This enables an objective frame of reference to determine what player had the greatest single season contribution. Before reading the book, I&rsquo;d have guessed Babe Ruth was the greatest player by a huge margin. He not only changed the game as the first great home run hitter, but also was a star pitcher for some World Series winning Red Sox teams. While I knew Barry Bonds put up some amazing numbers from 2001-2004, I never really appreciated just how great he was until reading this book. They make a case that Bonds is the only player even close to Ruth. One of the most interesting points of the book for me was some of the strategy decisions. In 2004, Bonds was so feared that managers often would intentionally walk him with the bases empty to lead off an inning and rather face the other hitters in the lineup (this is after Jeff Kent had left as a free agent). That year he came up to the plate 617, and walked a record 232 times (with 120 intentional walks). To put that in perspective, Wade Boggs was known for have the ability to generate walks and had a career high of 125 walks in 1988 to lead the American league. Even though the chances for a run to score is quite a bit higher when a runner is on first than with the bases empty, the authors made a strong case that it was better to walk Bonds with the bases empty and face the other hitters. Applying this to businesses This got me thinking about how we take a different approach to objective measures to quantify business entities. I often hear executives during earnings calls talking gross sales, cutting expenses, customer acquisition, and customer retention/repeat sales as the most important things for their business. Maybe these are the items Wall Street analysts expect and they are catering to their audience, but those can&rsquo;t be the only important items to running a successful business. Can I take a similar approach to can quantify the complete value of an entity compared to the average alternative? Each type of entity would need to context specific variables that would empower a model for calculating the Value Above Replacement (VAR). By context specific, the measures for the variables may be very specific to a company, region, or cohort. Example: WAR for a Software Developer A real life example for using this type of model could be to quantify the value of mid-level software developer may have in a startup. I have seen too many managers get excited to focus on metrics that can be easily measured, but don&rsquo;t necessarily relate to business impact (like number of bugs closed, number of features delivered, percentage of code with test coverage, number of releases, number of hours working on a project). I have come up with the following list of features that I feel better relate to individual and organizational success: Metric NameDescriptionBaselineWeightCustomer SatisfactionWe build products for users. Their feedback and attitude may relate to improved positive outcomes. &nbsp;&nbsp;Outcomes deliveredWe build products to deliver outcomes (more revenue, better efficiency). We should measure how our projects contribute to the goals of the company. &nbsp;&nbsp;Cycle time / VelocityThe time it takes to deliver an initiative of certain size or complexity. &nbsp;&nbsp;Features/Products ProposedI am big fan of encouraging everyone to contribute to product development, and strong engineers can add value. The measure can be the items contributed that make it into a product backlog. &nbsp;&nbsp;Karma PointsRisk of introducing defects, overhead, technical debt. Facebook has a pretty&nbsp;good framework, but I'd rather start a new person with a low score vs. a perfect score. &nbsp;&nbsp;Domain knowledgeCome up with a quantifiable measure of business or technical domain knowledge. &nbsp;&nbsp;Institutional knowledgeCome up with a quantifiable measure of company specific domain knowledge and relationships. &nbsp;&nbsp;Skill growthThe skills acquired and used during a given period. &nbsp;&nbsp;CollaborationThere are many types of customer relationships for a developer / team members, sale cycle. &nbsp;&nbsp;CompensationCompensation vs. market price&nbsp;&nbsp;Assuming that this list is representative of the features needed to assess value for this position and none are mutually exclusive, we could come up with a baseline measure for each metric (based on the average or median level developer we&rsquo;d consider). If certain features represent higher value to the organization than others, then each other features can be weighted. I left the baseline and weight columns empty as they are organization specific. Then each employee can be evaluated objectively, comparing their contribution with baseline to calculate the VAR. We now have an objective mechanism to do some important HR related items: Justify compensation changesIdentify performance gaps/excellenceCompare employees (internally and externally)Evaluate new candidatesA high performance company would certainly strive to always hire folks above the baseline. You may find that your superstar is actually worth multiple times more than the baseline candidate. Other business use cases Some other places this methodology can be used: Product developmentMarketing initiativesSales cyclesProduction/Manufacturing or logisticsProduct sourcingVendor selectionInvestmentsCompensation analysis"
    }, {
    "id": 37,
    "url": "http://localhost:4000/adding-facebook-comments-to-blogengine.net/",
    "title": "Adding Facebook Comments to BlogEngine.Net",
    "body": "2013/11/15 - This blog runs using the open source&nbsp;BlogEngine. net&nbsp;platform using ASP. Net 4. 0. So far I have been pretty happy with the features provided overall, as it was pretty straight forward to set up and there are many themes/widgets. The issue I had with using the out of the box comments system was seeing about 15 spams comments posted a day. I've seen some of the bigger blogs use Facebook Comments and they had less of the spammy junk, like ESPN in the screenshot above. Since I couldn't find a pre-canned BlogEngine widget available using Facebook comments, I needed to use&nbsp;Facebook Comments plugin&nbsp;to add this functionality. &nbsp;I was able to insert the following code into PostView. ascx of my theme to implement Facebook comments: 1234567891011121314&lt;fb:comments-count href= &lt;%=Post. PermaLink %&gt; &gt;&lt;/fb:comments-count&gt; &lt;%=Resources. labels. comments %&gt;&lt;div id= fb-root &gt;&lt;/div&gt;&lt;script&gt;    (function (d, s, id) {    var js, fjs = d. getElementsByTagName(s)[0];    if (d. getElementById(id)) return;    js = d. createElement(s); js. id = id;    js. src =  //connect. facebook. net/en_GB/all. js#xfbml=1 ;    fjs. parentNode. insertBefore(js, fjs);  } (document, 'script', 'facebook-jssdk'));&lt;/script&gt;&lt;div class= fb-comments  data-href= &lt;%=Post. PermaLink %&gt;  data-width= 470  data-num-posts= 10 &gt;&lt;/div&gt;Since I wanted the comments box to appear only on the post page (and not on the homepage), I wrapped the Facebook markup in if statement below: UPDATE (in 2020): I wound up removing the functionality described in this post due the volume of spam posts in Facebook comments. "
    }, {
    "id": 38,
    "url": "http://localhost:4000/decorating-sites-with-browser-extensions/",
    "title": "Decorating sites with Browser Extensions",
    "body": "2013/11/09 - We&rsquo;re big fans of internet grocer FreshDirect in my household. When we lived in Brooklyn and now in North Jersey, we place a delivery order about once a week. While there is a lot I like about the service offering, there are a few features that could improve the user experience. One of the big ones for me would be to associate WeightWatchers points values for each food on the site. It would save me a bunch of time and frustration if my wife and I would be able to see them while we were filling our cart. &nbsp; &nbsp; I knew this was going to be a  fun  project since both FreshDirect and WeightWatchers do not offer public APIs. The two big challenges would be: How will I assign points values to foods?How can we effectively present the points information as part of the shopping experience without needing FreshDirect to make changes to their site?Getting foods and their points values The first requirement is that I need to associate the foods on their site with their respective points values. &nbsp;I wrote a process that spidered all of the category pages on FreshDirect. com and recorded Nutritional information for each product found. I then used that information to calculate the points values and saved it to a json file with the following format: 12345{  items : [  {  id : gro_sprite_diet_le_02  , points :  0  },   {  id : cat_hldy_chsplatter  , points :  3  },   {  id : mea_pid_3335013  , points :  4  } ] }Displaying values on FreshDirect. com I needed a way to show the points values on FreshDirect. com without requiring them to make any changes. The only way I think of injecting content into a site was triggering some javascript during the user&rsquo;s session. Lucky for me, that is exactly the purpose of browser extensions. The first thing I did was open up a category page on FreshDirect. com in my Google Chrome browser. I opened Chrome&rsquo;s javascript console (Ctrl+Shift+J) and wrote some code to parse the DOM looking for all the products on the page. I then inserted the points value under each product link. I took the code from the console and saved it to a file called contentscript. js. I needed to get access to my json file with all the points values (for this exercise it is hosted locally at http://localhost/points-foods. txt). When I tried to access the json file, Chrome threw an error message that reminded me that most browsers will block calls to other sites to prevent malicious activities. While I could have gotten this to work with&nbsp;JSONP, extensions are designed to provide support for solving this problem. To create a Chrome extension, you need a manifest. json file (a very basic example below). The &ldquo;matches&rdquo; attribute defines the domains you want the scripts in the extension to access. The &ldquo;js&rdquo; attribute defines the scripts that will execute when a user navigates to a page in one of those domains. 1234567891011{   name :  WeightWatchers Points Values ,   manifest_version : 2,   version :  1. 0. 0. 0 ,   content_scripts : [{       js : [ contentscript. js ],       matches : [ https://www. freshdirect. com/* ,       http://www. freshdirect. com/* ,       http://localhost/* ]  }]}I opened chrome://extensions/ and checked the &ldquo;Developer Mode&rdquo; checkbox. I loaded the unpacked extension by navigating to the folder that contained contentscript. js and manifest. json files. On the screen below, you&rsquo;ll see the WeightWatchers Points Values 1. 0. 0. 0 available:&nbsp; &nbsp; With the extension enabled, you&rsquo;ll see the WeightWatchers points value injected below each product on the screen below&nbsp;(with the blue points icons): &nbsp; Conclusions Seeing the points values changed our purchasing habits in some cases. We chose to buy a more expensive type of pork ribs because it was 5 points compared to the cheaper 9 point type we have ordered in the past. FreshDirect made more money and we were happy to make a healthier choice, that&rsquo;s a win-win. I love loosely coupled design patterns (like decorators), where the systems can function completely independent of one another. &nbsp;This function works really well and it was built without any interaction of either FreshDirect or WeightWatchers. This was my first browser extension. &nbsp;There is amazing potential with this as a mechanism for delivering recommendations and personalized content. You just need to convince folks of the value to install/use your extension. My original project idea was to programmatically log into my Fresh Direct account and assign points values to all the food we had ordered (100+ orders). I spent about 3 hours to get to this work with Powershell code. Then I got more ambitious and decided to spider for all the products, which consumed another 2-3 hours. While I know I could have done this with far more elegant code using JQuery (since Chrome has the awesome&nbsp;Content Scripts support), I decided to use XMLHttpRequest and old school DOM parsing. This way the same code could be applied to other browsers that don't support content scripts. &nbsp;I took about 4-5 hours to write all the javascript code and figure out how extensions work. The follow on project:Since I saw a lot of my friends were using seamless to order from local restaurants, so I wanted to put my extension concept to the test. &nbsp;First there was some ground work to do:- further built out my food catalog to over 500,000 foods + menu items at&nbsp;&lt;span &gt;restaurant chains&lt;/span&gt;&nbsp;that published nutritional values. - I cross referenced the menu items on the seamless ordering pages with my catalog (using the lucene search engine was a big help here). User features:- I displayed points values for food items on seamless menu pages (also on menupages. com and grubhub). - I also displayed indicators when I had previously tracked a menu item that I had eaten (within my weight watchers food journal)- If I clicked on the points icon, a modal appeared to track my food item in the food journal and/or share it on social media&nbsp;Consistent with my activities on FreshDirect. com, I noticed that my lunch order choices changed because I had this new information. &nbsp;&nbsp;Future applications for browser extensions I hope to explore:Having the ability to see opinions of my specific social groups for products/content. Maybe try some sentiment analysis with thumbs up/thumbs down indicators or ratings scores. I would love this for Netflix!Enhancing shopping experiences and deal finders. Would be awesome for something to tell me the popularity of an item and optimal bid amount while shopping on eBay. Then I could integrate the extension with my sniper with a single click. Building productivity/administrative tools on top of my every day web applications (Gmail, banking, my sites, etc. )&nbsp;"
    }, {
    "id": 39,
    "url": "http://localhost:4000/calculating-weightwatchers-points-from-picture/",
    "title": "Calculating WeightWatchers points from a picture of the Nutritional Information",
    "body": "2013/10/19 - When I have tried to track the food I am eating as part of a diet, entering the key nutrition elements was not always convenient. Since I haven't seen a WeightWatchers product for reading nutritional values from a picture, I thought it would be a cool thing to prototype. One of the guys in my office had a box of oatmeal packets on his desk, so he took a picture with his iPhone 5 - &nbsp;(oatmeal_photo. JPG (2. 99 mb)). Below is a lower resolution version: I had never previously done anything at all with&nbsp;OCR, so I knew I'd learn at the very least. I googled looking for open source OCR libraries and web sites that parsed text from images. I tried quite a few and got the best results from the tesseract project:https://code. google. com/p/tesseract-ocr/&nbsp; I banged out a quick powershell script to: Execute tesseract to output a text file. &nbsp;Parsed the output file for values for Servings, Protein, Carbohydrates, Fat, and FiberCalculated the WeightWatchers Points valuations based on these values. Although I am unable to share my code with everyone, here's my PowerGUI screen showing the output from the prototype:&nbsp; Conclusions: I am really psyched I was able to get this functionality working in under 3 hours. It's really important to do this kind of exercise from time to time. None of the OCR options I found were anywhere close to perfect at parsing all the text from this image. There's a bunch of assumptions and fuzzy logic transformation rules that a production quality version would require. &nbsp;I tried to run some of the libraries with other pictures. I realized quickly that picture quality (size, resolution, and clarity) and lighting glares make a huge difference on how accurately the text in the image gets recognized. This is not a trivial challenge in production!I was pleasantly surprised that&nbsp;tesseract&nbsp;was able to understand most of the text on the page, including the text written sideways on the right side of the picture. "
    }, {
    "id": 40,
    "url": "http://localhost:4000/sftp-powershell-module/",
    "title": "Powershell Module for transferring files via SFTP",
    "body": "2013/06/22 - I had the need for several automated jobs to be able to exchange files securely with partners via the Secure FTP (SFTP). &nbsp;I chose powershell because the jobs: get executed on windows nodes. It is less overhead than creating console apps in . NETrequired things like querying databases, transforming data, and archiving filesI spent a little bit of time looking into different options, and decided to utilize the latest version of the&nbsp;WinSCP&nbsp;library provided the required options - such as designating the transfer mode, deleting source files, and specifying file masks. I created a generic powershell module that exposes upload and download functions. The module is available on github: https://github. com/jaygrossman/SftpPowershellModule 1234567891011121314151617181920212223242526272829Function SFTPDownloadFiles { Param(    [string] $Username, # Required. Username of SFTP account    [string] $Password, # Required. Password of SFTP account    [string] $HostName, # Required. HostName of SFTP server    [string] $RemotePath, # Required. Starting path of SFTP server    [string] $LocalPath, # Required. Starting path of local server    [string] $SshHostKeyFingerprint, # Required. Host Key,     ssh-rsa 1024 xx:xx:xx:xx:xx:xx:xx:xx:xx:xx:xx:xx:xx:xx:xx:xx    [string] $FileMask, # Optional. Filter string to limit files to download    [bool]  $Remove=$false, # Optional. Whether to remove original file    [string] $TransferMode= binary # Optional. Transfer can be binary, ascii, or automatic ) Function SFTPUploadFiles {  Param(      [string] $Username, # Required. Username of SFTP account      [string] $Password, # Required. Password of SFTP account      [string] $HostName, # Required. HostName of SFTP server      [string] $RemotePath, # Required. Starting path of SFTP server      [string] $LocalPath, # Required. Starting path of local server      [string] $SshHostKeyFingerprint, # Required. Host Key,       ssh-rsa 1024 xx:xx:xx:xx:xx:xx:xx:xx:xx:xx:xx:xx:xx:xx:xx:xx      [string] $FileMask, # Optional. Filter string to limit files to upload      [bool]  $Remove=$false, # Optional. Whether to remove original file      [string] $TransferMode= binary # Optional. Transfer can be binary, ascii, or automatic  )} "
    }, {
    "id": 41,
    "url": "http://localhost:4000/predicting-ebay-auction-sales-with-machine-learning/",
    "title": "Predicting eBay Auction Sales with Machine Learning",
    "body": "2013/06/10 - Abstract:&nbsp; Online auctions are one the most popular methods to buy and sell items on the internet. &nbsp;With more than 100 million active users globally (as of Q4 2011),&nbsp;eBay&nbsp;is the world's largest online marketplace, where practically anyone can buy and sell practically anything. The total value of goods sold on eBay was $68. 6 billion, more than $2,100 every second. This kind of volume produces huge amounts of data that can be utilized to provide services to the buyers and sellers, market research, and product development. &nbsp; In this analysis, I collect historical auction data from eBay and use machine learning algorithms to predict sales results of auction items. I describe the features used and formulations used for making predictions. Using the sports autograph category on eBay, the algorithms used can be relatively accurate and can result in a useful set of services for buyers and sellers. 1)&nbsp;Introduction&nbsp;&nbsp; I run a subscription based community with 30,000+ collectors of sports autographs and memorabilia &ndash;&nbsp;http://www. sportscollectors. net. &nbsp; Figure 1. Homepage of SportsCollectors. net Since eBay is the world&rsquo;s largest marketplace for sports autographs, the vast majority of the site&rsquo;s membership uses it to buy and/or sell items via auction format. The ability to provide a method to estimate auction sale prices is desirable to this community. Members of most communities related to collectibles have reported they most often try to predict how much an auction would sell for by performing a search for item and manually calculating the average sales price shown in the completed listings page (shown in Figure 2 below).  Figure 2. Example of eBay&rsquo;s completed listings page for autographed baseballs by Jim Rice. In this example, there have been 43 sales via auction format with an average sale price of $32. 05 ending in the past 60 days. To best serve this audience, I am interested in 2 things: Determine whether an auction listing will result in a sale. Predict final sale prices for auctions. &nbsp; 2) Data Collection&nbsp;&nbsp; I run an automated process that collects fixed price and auction listings information available on eBay. The process queries for listings at product sku level, defined by the combination of: Player&rsquo;s reference data from SportsCollectors. Net - every player to have played pro baseball, football, basketball, and hockey since 1948eBay autograph category by sport (shown in Table 1):SportBaseballBasketballFootballHockeyCategories-Balls-Bats-Hats-Helmets-Index Cards-Jerseys-Lithographs, Posters, &amp; Prints-Magazines-Other Autographed Items-Photos-Plaques-Plates-Postcards-Programs-Ticket Stubs-Trading Cards-Balls-Floor, Floorboard-Index Cards-Jerseys-Lithographs, Posters, &amp; Prints-Magazines-Other Autographed Items-Photos-Trading Cards-Balls-Hats-Helmets-Index Cards-Jerseys-Lithographs, Posters, &amp; Prints-Magazines-Other Autographed Items-Photos-Plaques-Programs-Ticket Stubs-Trading Cards-Index Cards-Jerseys-Magazines-Other Autographed Items-Photos-Pucks-Sticks-Trading CardsTable 1. Breakdown of categories of autographed products by sport 3) Features 3. 1 Auction Features&nbsp;&nbsp; Table 2. Features extracted from the auction&rsquo;s meta data:&nbsp; FeatureDescriptionPriceFinal price the auction. If the listing does not result in a sale, the Price will be equal to the StartingBid. StartingBidMinimum bid for the auctionBidCountNumber of bids made for the auctionTitleAuction titleQuantitySoldThe number of items sold in the listing. Represented by a 0 or 1. SellerRatingSeller&rsquo;s eBay ratingSellerAboutMePageWhether the seller has an eBay About Me pageStartDateThe beginning date and time of the auctionEndDateThe ending date and time of the auctionPositiveFeedbackPercentThe percent of positive feedback (of all the feedback) received by the sellerHasPictureIndicates the seller included a picture with the listingRepresented by a 0 or 1. MemberSinceThe date the seller created their online marketplace user accountHasStoreIndicates the seller has an eBay StoreRepresented by a 0 or 1. SellerCountryThe country of the sellerBuyItNowPriceThe optional price to buy the item instantlyHighBidderFeedbackRatingHighest bidder&rsquo;s eBay ratingReturnsAcceptedWhether the seller accepts returns. Represented by a 0 or 1. HasFreeShippingWhether the seller provides free shipping. Represented by a 0 or 1. &nbsp; 3. 2 Derived Features&nbsp;&nbsp; Table 3. Features derived from the auction&rsquo;s meta data: FeatureDescriptionIsHOFWhether the player in their sport&rsquo;s Hall of Fame. &nbsp;Represented by a 0 or 1. IsAuthenticatedWhether the received third party authentication. &nbsp;Represented by a 0 or 1. &nbsp;Determined by inspecting the auction&rsquo;s title and description details for a whitelisted set of keywords and ruling out a blacklisted set of keywords. HasInscriptionWhether the item has an inscription. &nbsp;Represented by a 0 or 1. &nbsp;Determined by inspecting the auction&rsquo;s title and description details for a whitelisted set of keywords and ruling out a blacklisted set of keywords. AvgPriceThe average sale price by skuMedianPriceThe median sale price by skuAuctionCountThe number of auctions listed by skuSellerSaleToAveragePriceRatioThe ratio of the sale price realized by a specific seller divided by the average price of the same skusSellerAuctionSaleCountThe number of sales the seller has madeSellerItemSellPercentThe ratio of the number of sales divided by number of auctions listed by sellerStartDayOfWeekThe day of the week (number) that the auction StartedEndDayOfWeekThe day of the week (number) that the auction EndedAuctionDurationThe number of days the auction lastedStartingBidPercentThe ratio of the StartingBid divided by sku&rsquo;s AvgPriceSellerClosePercentThe ratio of the number of auctions resulting in sale for a seller divided by total number of auctions the seller listedItemAuctionSellPercentThe ratio of the number of auctions resulting in sale for a sku divided by total number of auctions the listed for the sku&nbsp; 4) Training and Test Data Data to determine whether an auction listing will result in a sale: &nbsp;Query CriteriaRecordsMean Sale PriceMedian SalePriceRange Sale PriceTraining SetAll Auctions ending in April 2013258,588$28. 96$9. 99$0. 01-$300. 00Test SetAll Auctions ending in first week of May 201337,460$24. 65$9. 99$0. 01-$300. 00&nbsp; Data to predict final sale prices for auctions:&nbsp; &nbsp;Query CriteriaRecordsMean Sale PriceMedian SalePriceRange Sale PriceTraining SubsetAuctions ending with a sale in April 201379,732$33. 04$14. 99$0. 01-$300. 00Test SubsetAuctions ending with a sale in first week of May 20139,392$29. 17$12. 55$0. 01-$297. 50Filter Criteria Only Standard auction format. Only items signed by a single player. 5) Analysis and Prediction Since this analysis is trying to answer two questions, this section details the methodologies for solving each problem individually. 5. 1 Determine whether an auction listing will result in a sale. This is a binary classification problem, as the goal is to optimally predict QuantitySold (containing values of 0 or 1) as the target feature. &nbsp; The model chosen to create the classification predictions is Logistic regression. Logistic regression uses a set of covariates to predict probabilities of class membership. I achieved an optimized prediction model based on a set of 5 derived features with standardized values. Prediction via Logistic RegressionBaseline: Prediction of sale when AvgPrice is less than SalePrice85. 97%42. 74%&nbsp; 5. 2 Predict final sale prices for auctions. &nbsp; Figure 3. Histogram of Price feature in Training Subset. Figure 3 shows a high concentration of sales under $20. 00. Since sports autographs on eBay are not a commoditized item (as compared to consumer electronics or books), I have seen some pretty interesting ranges in sale price for the same items. This leads to one of the challenges for this analysis, that my Training Subset was not Gaussian. Figure 4 below represents the Price feature from training data after a log transform. We can see that the graph is skewed with a very high Min (first quartile). The test also set has a very similar distribution. Figure 4. Histogram of log(Price) feature in Training Subset after log transformation There are 2 different approaches used to solving price prediction as a machine learning problem: 5. 2. 1 Price Prediction by Regression Classification and Regression Trees (CART)&nbsp; Classiﬁcation and regression trees&nbsp;are machine-learning methods for constructing prediction models from data. The models are obtained by recursively partitioning the data space and ﬁtting a simple prediction model within each partition. As a result, the partitioning can be represented graphically as a decision tree. Classiﬁcation trees are designed for dependent variables that take a ﬁnite number of unordered values, with prediction error measured in terms of misclassiﬁcation cost. Regression trees are for dependent variables that take continuous or ordered discrete values, with prediction error typically measured by the squared difference between the observed and predicted values. I created an optimized decision tree using the Training Subset, and then used it to create predictions against the Test Subset. The Root Mean Squared Error (RMSE) of the CART predictions is 4. 30. &nbsp;&nbsp; Figure 5. Plot of CART Predictions vs. Observed Sale Prices from the Test Subset. The green line represents Prediction=SalePrice and the blue line is the smoothed actual. Figure 5 shows that the model does a pretty good job for the majority of the listings and then becomes significantly less accurate. &nbsp; I created subsets of the Test Subset with different maximum predictions. I saw that $50 was the optimal balance in terms of RMSE and keeping a high portion of records of the original data set. &nbsp; Baseline I created a analysis baseline using the CART model. I created a decision tree based the log(Price) and AvgPrice features in the Training Subset. The tree is used along with the Test Subset to create predictions. Using this method, the predictions had an RMSE of 5. 30. 5. 2. 2 Multi-Class Classification Using the Training Subset, I divided the Sale Price (target variable) into $5 intervals and created discrete categories. Each auction is assigned to one category. This allows for a multiclass classification problem in which case the output is a $5 range instead of the specific price. Prediction using K-Nearest Neighbors (KNN)&nbsp; Wikipedia defines KNN&nbsp;as a non-parametric method for classifying objects based on closest training examples in the feature space. &nbsp; I was able to use the KNN to create a model based on the Training Subset, using the Price Interval as the factor and k=3. This model was combined with the Test Subset to generate predictions for what $5 interval each item would fall into. Below are the results: accurately predicted: 50. 92%&nbsp;predictions within one group: 76. 68%predictions within two groups: 86. 15%5. 2. 3 Filtered Prediction I combined the two methods to get optimal results: Use predictions from CART with only out predictions under $50. 00. Use KNN classification predictions to limit outliers. Filter out auctions where predicted $5 interval is greater than 2 from the predicted price. MethodRMSESale-PreditionStandard Deviation% of SubsetBaseline CART (using AvgPrice)5. 30+21%40100%CART4. 30-15%28100%CART for predictions under $50. 003. 52-14%1588%CART for predictions under $50. 00and within 2 price intervals0. 84-7%1265%&nbsp; 6) Conclusions&nbsp; How reliable is using the average price for predicting sales? I would not be comfortable using the average price to predict auction sales. It has a higher RMSE, higher difference between actualized prices, and a very high standard deviation. Can we determine whether an auction listing will result in a sale? Since logistic regression provided an almost 86% success rate for predicting if the auction would result in a sale, I would be comfortable considering the model&rsquo;s prediction. This would be quite useful from a seller&rsquo;s perspective, to help minimize: The listing fees a seller would accumulate due to unsold listings. The time invested in listing unsuccessful items. Can we predict final sale prices for auctions? The combination of the CART prediction using KNN for eliminating obvious outliers does a good job predicting final sale price when predictions are under $50 (about 65% of the observed cases). &nbsp; Since I have yet to find a service or commercial product related to predicting eBay auction results, this analysis could be valuable in offering services for the following applications: Buying recommendations/arbitrageListing OptimizationProduct Sourcing and Logistics&nbsp;Price Estimation for Complimentary Services (Shipping, Insurance providers)&nbsp; 7) Future Exploration There are some additional features I can consider to potentially enhance these models:&nbsp; Time of year, month, major events (Super Bowl, Spring Training)Bid Timing, clusters around bids near end of auctions. &nbsp;Chou et al 2007 have done analysis on predicting price based on bidding patterns A Simulation-Based Model for Final Price Prediction in Online Auctions (http://www. jem. org. tw/content/pdf/Vol. 3No. 1/01. pdf)Measure of Player&rsquo;s Popularity/Demand/Interest (on eBay, sportcollectors. net, twitter, espn, etc. )Semantic parsing auction description for most relevant keywords or phrasesOther areas to explore with this data: Sku representations for items signed by multiple playersGuidance around 3rd party authentication servicesProduct recommendations and predicting arbitrage scenarios. More Verticals:Other Collectibles (sports cards, coins, stamps, toys)Car parts&nbsp;Consumer ElectronicsR code used for this analysis:&nbsp;https://github. com/jaygrossman/eBaySalesPrediction &nbsp; "
    }, {
    "id": 42,
    "url": "http://localhost:4000/startup-money/",
    "title": "Show my startup the money…or not.",
    "body": "2013/04/26 - Living/working in the New York City area I get to meet quite a few people starting up their own ventures and I am intrigued by how the small, early companies start out. There are all sorts of interesting dynamics to consider: How many founders do they have? If more than one, why?Did they hire employees/interns/partners at the beginning? If so, why?Did they look for outside investment? If so, did they need it?What is their plan for growth/expansion/scale?Since every business and founder&rsquo;s perspective are different, there is not a single set of right or wrong answers to these questions. Since most businesses end up in failure, I find&nbsp; My informal poll I informally talked to 50 friends and colleagues about businesses either they have already started or plan/hope to start in the next year. 34 had started businesses and 16 were planning to. While I would not categorize the folks in this sample as representative of the average entrepreneur in America, it resulted in some interesting aggregates (some of them I will share here): -31 of the 50 had more than one founder. 13 had more than two founders. -47 of the 50 had no employees (other than the founders). Only one had more than 10 employees. -26 of the 50 was either looking, planned to look for, or had received outside investment. &nbsp; -Of the 34 actual businesses, 7 had invested the time writing an MBA level business plan. Of those 7, only one thought it was a worthwhile use of his/her time. -Of the 34 actual businesses, 23 had an active revenue model (the business had a possible way to begin earning money) at launch. There are bunch of other feature variables that would have likely been telling but not appropriate to ask all of these folks &ndash; such as the company&rsquo;s lifespan, revenues, profits, pivots, investments/assets, scope, etc. While all these variables can certainly have strategic or operational impact on a startup, I am drawn to the finding that slightly more than half of the group desired or perceived the need for outside investment in the very stages to make their business viable. Needing to find outside investment is a pretty serious critical path dependency, especially if need is real. &nbsp; Types of business models and their need for investment I&rsquo;ve come up with my own categories for business models, each having a varying reliance on capital investment from the beginning. Ambitious This type of scenario usually tries to solve big or ambitious problems. They tend for startups to be complex, have many large dependencies, and/or require expensive labor or materials. They simply require large amounts of capital day one to even consider undertaking - like opening a manufacturing facility, construction for a commercial building, or creating a gene research offering. &nbsp; Commodity concept requires initial cash This type of scenario represents trying to implement an established business model or pattern. Examples would be opening up a franchise (like a 7-11 or Dunkin Donuts), starting a restaurant, or opening retail store. Founders will have to have capital day one for things like franchise fees, employees, inventory, insurance, etc. The grow as you go approach This type of scenario typically has fewer dependencies and are somewhat less ambitious (at least from the start), so there is not the absolute demand for capital as the other categories. These types of businesses can start small (sometimes as even as a hobby) and be scaled up as sufficient. These may be businesses offering services, selling more flexible products, and/or being able to quickly adapt to markets. I think of examples such as software/web site creators, marketplaces, small vendors, import/export/dropshippers, accountants, doctors, &nbsp;lawyers, plumbers, and ad agencies. With the rise of Internet and lower barriers of entry, &ldquo;grow as you go&rdquo; scenarios have become popular. &nbsp;The remainder of this blog post is going to focus on this category. I estimate that at least 45 of the 50 companies I inquired about fall into this category. While they may not necessarily require large influxes capital day one, many of the entrepreneurs perceived they would need some relatively early on. Why you shouldn&rsquo;t take money David Heinemeier Hansson (creator of Ruby on Rails and 37signals partner) has really strong opinions on why startups should not take outside investment (especially venture capital). He shares some of them in the video below: He calls venture capital a time bomb, the most harmful thing you can do to business. &nbsp;He has not met a single founder who took money that would not have done it differently. Once you take money, you are an addict looking for another round. . . and you can't piss off your dealer. If you take money, you must either prove your business is viable or the VC owns your company when money runs out&nbsp;When spending other people's money, you care much less about business. When it is your own money, you want make more it faster. Worst thing for business is to have no constraints, because you don't sense of urgency. Having to spend your own money is most powerful driving force for entrepreneur. Take the Money Back in 2008, ShopSavvy released one of the first ten mobile applications in the world and had good success. They were proud to spurn investor offers and stay independent. &nbsp;They wrote regretted bootstrapping their business detailed in a post named&nbsp;Starving Your Startup. Here are the key takeaways: Taking money from prominent investors is great social proof. It can lead to attracting key Venture Capital firms and top employeesMore capital you can move faster and hire more/better employees. They discussed how a smaller competitor took money and was bought by eBay &ndash; a missed opportunity for them to get acquired and turning eBay into a competitor. They sum up the post with: The good news is that by bootstrapping your startup you will have a lot more control and own a lot more of the company at the end of the day. The bad news is that your startup, by definition, will likely be a lot smaller than it could have been with the right investment from the right investors. Even if you end up building that billion dollar business it will have taken you much longer than it might have had you taken outside investment. We spent the last five years turning ShopSavvy into a hundred million dollar company &ndash; it is likely we could have done that in a year with the right venture capital partner. I have been part of both completely bootstrapped and funded startups (via angel investment, not from venture capital). &nbsp; My Experience with Funding In the middle of 2003, my friend D and his friend N were enamored with Friendster, as it was exploding in popularity. D has some serious graphic design chops and N put up the money, so they approached me to build a competing social network that was named Hipstir. This is my first and only experience with a funded startup. ASP. NET 1. 1 was the hot technology and I decided that I was going to use the project to learn it. I spent every minute outside of my day job that Fall building out the site (and nosebleeding through a huge tech learning curve) so we were ready to launch in October. To give context, MySpace. com had just launched and only had 50,000 members. Friendster was in the millions of members, but then had site outages almost daily trying to support their network structure. We sent out emails to our friends and all the sudden people started to use the site. Every day we&rsquo;d see more people sign up people would spam their friends with emails from the site. All of the sudden the site hit a tipping point in the Philippines (of all places), and we were seeing 2,000-5,000 joining every day. I was going crazy every night for weeks trying to keep the site functionality during peak hours (roughly 3am EST). We had 200K-300K users, so I made the decision that we should have a revenue model since I had spent so much time building/running it but was not getting any monetary reward. The site was doing pretty well with Google Adsense for a few months until Google went public shortly after and drastically reduced its payments for clicks from non US markets. It was too bad, as we eventually got up to 600K users and quite a bit of regular traffic. I felt like I was pressured into putting in huge efforts to build and run the site. Some of the reason was that I was proud of what I built and was so excited to see all the interest in it. I also felt somewhat compelled to try to make it work longer than I probably should have since N had invested in it. If I had bootstrapped the project, I may not have pulled so many all-nighters and been content with slower (and probably more healthy) growth. My Experience Bootstrapping I have mentioned my favorite project a number of times on this blog, SportsCollectors. Net. It was my dream for so many years to build the ultimate community for sports autograph collectors. It drove me to learn about computers and programming, mostly so I could build the site without needing partners/other developers/outside money. So I taught myself what I needed to build it and bootstrapped the project. I very much considered the process of building the site as a hobby at first and it grew to be more with success. &nbsp; From 2000-2002, I paid the hosting expenses on shared hosted servers as the site began to grow in popularity. In 2002, I rebuilt the entire site in a different language and began offering subscriptions for premium features. Every year since then, the site sees steady growth and it continues to be a great resource for 1000&rsquo;s of collectors. &nbsp; Since I have no partners or investors, I am in the wonderful position to shape the community. I&rsquo;ve been able to build some really cool features (exclusive only to this site) that offer value to the site&rsquo;s members, but may not directly relate to increased revenues. I also am able to enforce policies and remedy issues quickly as I am the sole decision maker. Conclusions Personally, I&rsquo;d prefer to bootstrap a startup if I have the choice. I like the idea for a few reasons:&nbsp; Founders control all decisions related to the control of company&rsquo;s direction. I want to own decisions on setting priorities and how to spend money. Trying to raise capital can be a sizable distraction that bootstrapped founders will avoid. Pressure to perform is a good thing. Spending your own money makes the priorities more clear. I get to set my own goals for the company and be beholden to a terms sheet and negotiated timelines. &nbsp;Partners can offset some of the need for funding if their skills are complimentary. However, I can see some scenarios where taking some funding could make sense: To accelerate the delivery and execution cycles. TIME IS A REALLY KEY THING. You can always make money or meet people, but you can&rsquo;t get time or missed opportunity back. There are situations were getting outside help makes sense. If you are building a manufacturing facility, then you&rsquo;ll probably need millions of dollars that most people don&rsquo;t have readily available. "
    }, {
    "id": 43,
    "url": "http://localhost:4000/moneyball/",
    "title": "The Moneyball Effect",
    "body": "2013/04/26 - In 2004, Michael Lewis wrote this book called  Moneyball: The Art of Winning an Unfair Game  detailing the focus of the Oakland A's (baseball team) on analytical, evidence-based, sabermetric approach to assembling a competitive baseball team, despite a disadvantaged revenue situation. The central premise of Moneyball is that the collected wisdom of baseball insiders (including players, managers, coaches, scouts, and the front office) over the past century is subjective and often flawed. Statistics such as stolen bases, runs batted in, and batting average, typically used to gauge players, are relics of a 19th century view of the game and the statistics that were available at the time. The book argues that the Oakland A's' front office took advantage of more analytical gauges of player performance to field a team that could compete successfully against richer competitors in Major League Baseball. Rigorous statistical analysis had demonstrated that on-base percentage and slugging percentage are better indicators of offensive success, and the A's became convinced that these qualities were cheaper to obtain on the open market than more historically valued qualities such as speed and contact. These observations often flew in the face of conventional baseball wisdom and the beliefs of many baseball scouts and executives. The book was made into a movie in 2011, starring Brad Pitt as A's general manager Billy Beane. There is a specific scene in the movie that especially struck home with me, &nbsp;where a room full of the A's most senior scouts are discussing the question how the team is going to assemble the best team after losing high priced star players Jason Giambi, Johnny Damon, and Jason Isringhausen. As the room is full of scouts areone is discounting a a player because he has an ugly girlfriend, Pitt becomes frustrated how they are taking the totally wrong approach to his problem (21 seconds into the trailer below): If you have read any part of my blog, it is petty obvious that I am a big fan of data analyis and an even biger sports fan. So the premise that a sports team had successfully innovated with a data driven approach was really cool for me. Numbers based Decision Making Wikipedia tells us that&nbsp;decision making&nbsp;can be regarded as the mental processes (cognitive process) resulting in the selection of a course of action among several alternative scenarios. Every decision making process produces a final choice. The output can be an action or an opinion of choice. I see a great many decisions in Corporate America get made because of opinions or assumptions, rather than being supported by data. Here's s few supporting scenarios I have seen lead to inefficiency: The highest paid person's opinion (HiPPO) becomes the defacto decision adopted. Historical reasons may dictate decisions, even when the current circumstances completely invalidate those reasons. Some companies blindly follow their competitors lead, rather than understanding the problem. Companies are overly reactive to external factors (customers, markets, competitors) without understanding the impact. Could you enjoy even more success if the answers to most important questions that guide your actions were based some type of statistical measure - instead of opinions, assumptions, or a hunches? Do you think your actions would change and the outcomes would be different? With all the hype around big data and data science these days, I don't think I really need to go into any length about the importance of undertanding data in today's business world. &nbsp;I saw a really great quote from Yahoo CEO Marissa Mayer:&nbsp; If we have data, let&rsquo;s look at data. If all we have are opinions, let&rsquo;s go with mine. A company where I once worked is an interesting example (I will not cite their name). The key metric that the management was focused on was revenue from new client acquisition and new sales. &nbsp;Since they didn't utilize the facility to accurately track their full cost structure in their ERP system, a few of us assumed that supporting the expensive process of client acquisition could not have been profitable. &nbsp; This always seemed extremely wrong to me, being that they had a significant amount of revenue and likely had higher profits from repeat business. When I asked why the focus on new business, the answer I routinely heard was that new sales was the established measure the VP of Sales (the HiPPO) had used at other companies in this vertical. &nbsp; Were they right? I am not sure. But neither were they, and that is a scary proposition for their management and shareholders. The Art of Winning an Unfair Game As I watched the movie, it was an instant aha moment for me. &nbsp;I immediately focused on the subtitle - &ldquo;The Art of Winning an Unfair Game&rdquo;. Even if I don't have the best/most resources possible, how I can maximize my ability for achieving a successful outcome. So I like how Lewis uses the word  art  to describe the way the A's were successful. While many people may associate science with the statistical analysis as his edge, I would argue it was more an  art  the way Beane applied the process to building the team. There are plenty of situations where the playing field seems unfair or uneven. I certainly do not have the budget and staffs that some of my competitiors enjoy. I don't have the &nbsp;formal higher education (MBA, PHD) of some collegues. These things that may seem like some pretty big disadvantages, but they &nbsp;do not have to prevent me from being successful and competitive. &nbsp; Professor Shannon W. Anderson has been using the &ldquo;Moneyball&rdquo; case in her MBA classes for several years. In this post, she offers an overview of how she applies it to business today: Typically, we tend to think about budgets and constraints as the enemy of creativity. There is a sense that you need to think big thoughts, and not be constrained by an accountant looking over your shoulder. The Moneyball demonstrates that the hard budget constraints the A&rsquo;s faced were the cause of creativity&mdash;it forced Billy Beane and his managers to come up with new ways to get the most out of their dollar. &nbsp; Beane and his staff figured the only way to compete was to disrupt the market with through innovation. They looked at at the problem and were able to define a better way of approaching the solution. &nbsp; I am big believer that the right mindest, commitment, and process can help level the playing field in many scenarios. That is probably why I enjoy being an entrepreneur so much and enjoy shipping value to users. So here is the Moneyball Process I have adopted: Define the objectives that will make the endeavor successfulWhat are the factors that will help you achieve this objective?Identify the specific activiies that you and your team can do to help achieve the objedctiveCreate measures and evaluate themSome other good takeaways from the book: Adapt or die. &nbsp;Find hidden value. &nbsp;Money isn&rsquo;t everything. &nbsp;A good way to drive innovation is to be clear about what you want to accomplish and do not get stuck in the rut of doing things the way they&rsquo;ve always been done. Filter out the noise and stay focusedLook for finite, measurable business parameters that make great candidates to apply analytics and derive insights not readily apparent. Go ahead and take risks, but contain them. "
    }, {
    "id": 44,
    "url": "http://localhost:4000/roadtrip-hackathon/",
    "title": "Personal Roadtrip Hackathon",
    "body": "2013/02/17 - A hackathon is generally an event a small group collaborates together to build a product or prototype in a very short period. Some of the more popular sponsored examples (such as the ones during the&nbsp;TechCrunch Disrupt series) usually last for 24 hours. &nbsp; For a while, I have been intrigued by the concept and have wanted to see what I can accomplish. Unfortunately I have not had the opportunity to enter one with my always busy schedule and family commitments. &nbsp; This weekend I had a planned&nbsp;trip with a friend to Houston for the NBA All Star weekend. It was a unique opportunity since I would have some time to myself as I would not be traveling with my family or colleagues. Since the flight from JFK Airport in NY to Houston is over 4 hours, I could either rest or be productive. The Plan I needed to pick a project that I was interested in and spend whatever free time I would have over the weekend to work on it. I decided that my project would be to build a platform for determining market values of products in a new vertical. I knew I could leverage some the work I had invested in the&nbsp;valuation platform for SportsCardDatabase. com. The Work When the pilot announced we could start using electronics on the flight, it was time for the fun to begin. The following are the high level items I worked on: Defined the data model and KPI's for the new product types&nbsp;Set up product database tables to persist this informationPopulated product tables with sample informationWrote code module to use the product information and use it to build data gathering rules. Wrote code module to interpret the gathering rules and gather the data. Set up database tables to store the gathered data. Wrote code module to populate the gathered data into the tables. Wrote code module to update gathered data as required by business rules. Since I was able to leverage some pre-existing code, I was able to get all of these code modules with sufficient unit tests by the end of the flight. Since I did not have internet access aboard the flight, I was unable to fully integration test functionality for calling partner APIs. I spent about 1 hour or so the next day testing and debugging the code. I was able to get the system recording the transaction data required. I spent another 20-30 minutes enhancing a variety of log messages. The following day I spent another hour adding 2 new KPIs to each of the transaction records, meaning the gathering rules and supporting code modules needed to be upgraded. I was able to run an end to end test of the system (took another 20 minutes), and it met all of the requirements before my return flight home. I have invested roughly 7 hours and I have the most difficult part of my valuation platform for this new vertical. I wasn&rsquo;t overly confident I would be able to finish this functionality before the end of the trip (and I now have my return flight for other things). Next Steps When I arrive home, I will deploy the system on a proper server and let it run for the next 3-6 months to gather data. Once there is enough data, I can analyze it and calculate market values for the products. I will continue to add new items for this vertical to the product tables in the database. &nbsp; Once I have a strong set of regularly updated market values, I can provide a user interface and allow users access to it. Conclusions By defining a plan for a focused set of functionality, along with the opportunity for uninterrupted work, I was able to produce more and higher quality work than I usually do. If I needed to collaborate with others, there is no way we could have come to consensus and deliver as much in such tight timeframe &ndash; even if we were to divide the work. &nbsp; I realized that I was really tired after 4 hours of solid work on the flight and my productivity would have likely decreased had I continued without a break. I am not sure I could focus like this for 24 hours straight like the traditional hackathon. I don&rsquo;t usually do this kind of development without online resources (google, stack overflow, etc. ). I was really shocked the system only needed a relatively short amount of debugging without these resources for reference. Unit tests certainly helped! I kind of like the hackathon format. In 3-6 months, I can try 2 more hackathon events to finish the project focusing on: Performing analysis on the data. Building a user interface and reports. "
    }, {
    "id": 45,
    "url": "http://localhost:4000/2013-nba-all-star-weekend/",
    "title": "2013 NBA All Star Weekend",
    "body": "2013/02/16 - My buddy Mike Dolan runs a very cool magazine targeted to professional athletes called Athletes Quaterly. Each issue is loaded with content/interviews about current and former pro athletes. &nbsp;Mike and I are both very big NBA fans, especially from 70-90's eras. This weekend I had a very cool opportunity to join him in Houston, the host city of the 2013 All Star game. Mike has interviewed or knows various members of the NBA Retired Players Association, so we got to hang out with many of the legends at the Hotel Derek where they were staying. &nbsp;We left for Houston on Wednesday and former Syracuse and Knicks forward John Wallace was on our plane. It was a 4 hour flight to Houston, so I got to listen to the TV on JetBlue while I worked on my laptop. &nbsp;&nbsp;On Thursday morning, we joined a number of legends at a basketball clinic at a local high school with 300-400 kids. Pics are below: dancing  Kenny Battle, Ollie Taylor, Bob Elliot, Harvey Cathings, Caldwell Jones, Harold Keeling, Otis Birdsong, Steve Hayes, Johnny Egan, Johnny Newman, Sam Vincent, Lawrence Funderburke, Geoff Huston, and Greg Kite&nbsp;&nbsp;Caldwell Jones is listed at 6’11, and he is schooling a local kid who was at least 4 inches taller. (UPDATE in 2019 - I later found out this was 7’6 Tacko Fall. He had one move at that time, turn around and dunk with his right hand. ) Harvey Catchings and Johnny Egan running drills. After the clinic we came back to the hotel for lunch. We got to eat with Michael Ray Richardson, T. R. Dunn and another big guy we didn't know. There were lots of specials on Michael Jordan's career with his 50th birthday this weekend, and everyone was commenting on how great he was. &nbsp;Moses Malone walked up to the hotel and a woman working at the hotel entrance asked says,  excuse me who are you?  He says in a deep voice,  I am Moses and I am checking in  as he blows by her. She was frozen flabbergasted and a group of players just starting cracking up. &nbsp;That night there was a welcome reception for the players and partners and many of the legends were there. David Cooke was nice enough to take this picture of me with former Jazz center Mark Eaton (I am not sitting down, he's 7'4):&nbsp;&nbsp;Some of the cool stories and conversations I heard:Michael Ray was trash talking some player, and Artis yells out  don't let him get in your head. He talks so much stuff . And they both start cracking up. Rick Barry is talking to Mack Calvin. I hear the always opinionated Barry that there should be more ABA in the Hall of Fame. Then he says,  Why is so and so in the Hall of Fame. He was a good college player, but he didn't do S**T in the NBA. I just don't get it.  Mark Eaton puts his arm around this other 6'10 big guy I didn't know. He says,  You may not know this, but you are the reason I started working out. You beat me up so bad I had no choice.   They both laughed but Eaton was serious. Bob Elliott was telling me how he played in Piscataway, NJ during his days with the Nets. I went to Rutgers so I knew that arena very well, and he said it was the hardest court in the ABA or NBA. It was so brutal that David Thompson refused to play a game on that floor. We talked about his days with Maurice Lucas and Mike Newlin. &nbsp;Mike some shared some other cool stories from last year's reception:Steve Hayes told a story about his days with the Blazers. When Bill Walton signed with the San Diego Clippers, the Blazers were to get players as compensation (Kevin Kunnert, Randy Smith, Kermit Washington and Mike Gminski). Dr. Jack Ramsey told the team,  everyone is safe, except for you Steve  and Steve Hayes got waived as 3 big guys were coming in. Mike was talking to former NC State and Sonics center Tom Burleson. A guy comes up to Burleson and says,  I punched a guy in the face for you . Burlseon had no idea what who the guy was and didn't remember it. . . When Mike did the research, he found&nbsp;this article&nbsp; detailing that it was Archie Clark avenging a leg whip on Tom with a punch to the jaw. &nbsp;As we were having breakfast on Friday, we saw Bill Russell walking in to eat with Satch Sanders. &nbsp;Mike interviewing Bo Kimble. They talked about how Bo is using the Paul Westhead run and gun system for the high school and junior college teams he coaches. Mike interviewing one of my all time favorite super shooters, Dale Ellis. &nbsp;&nbsp; Karl Malone is pretty jacked these days. Tracy Murray and Magic Johnson chatting it up. Magic Johnson chasing after Scottie Pippen to say hello. &nbsp;Here is the full list of players we either talked to or saw in the hotel:scottie pippenmoses malonemagic johnsonkarl malonerick barryspencer haywoodharvey cacthngscaldwell jonesmajor jonesbob lovematt fishmark eatonvernon maxwelltracy murraytom  satch  sandersbill russellmichael ray richardsontr dunndavid cookeanthony masonjohn wallacedarrell armstrongchris gatligtate georgesam vincetwillie norwoodkenny battlesteve hayesbob elliottsean elliottdick barnettollie taylormack calvindale davisantoine walker&nbsp;ruben pattersondale ellisbo kimblealex englishartis gilmorebernard thompsonsteve coltereddie johnsonmichael williamskenny battlejohnny eganthurl baileyjohnny newmanmorlon wileyruss williamscliff levingstonwayne rollinsjames donaldsonsam vincentharold keelingtom hooveranthony aventcharles smithlawrence funderburkegeoff hustonronnie brewer sr. willie burtongreg kitejack marinotis birdsonglarue martinmarvin roberts"
    }, {
    "id": 46,
    "url": "http://localhost:4000/invest-in-people/",
    "title": "When do you start investing in people and stop investing in software?",
    "body": "2013/02/13 - Yesterday a friend IM&rsquo;d me this question out of the blue with a really interesting question: When do you start investing in people and stop investing in software? Before I get into my answer, let&rsquo;s explore more about investments and how they relate to software and people. What is an investment?&nbsp; A generic definition for investment is allocating money and /or resources into some initiative with the expectation of gain. Most investment scenarios carry an associated set of risks. So the most obvious motivator for people to make an investment (time, money, energy, etc. ) in anything is to get some positive return. Investment in software&nbsp; Software can be defined as a collection of computer programs and related data that provides the instructions for telling a computer what to do and how to do it. It usually consists of a set of programs, procedures, algorithms and its documentation concerned with the operation of a data processing system. For this discussion, I will also include bringing in professional services (outside subject matter experts) as part of making a software investment. I spend a good part of my days understanding common problems and trying to find automated/repeatable solutions. I&rsquo;ve always seen this is one of the biggest value drivers for the software development industry as a whole, and a big reason I wanted to teach myself to program. Why invest in software? Investments in software and technology platforms can be cost effective options that often allow companies to meet a great variety of business objectives, such as: Provide an industry standard solution. Avoid trying to work on problems that have adequately been solved for you. &nbsp;Achieve greater reliability and efficiency of businesses processes at scaleProvide capability to store and analyze data/knowledgeInteract/collaborate more efficiently with customers, partners, vendors, and other employees. Enforce governance/adherenceTie together complex sets of business rules and workflowsExample Scenario for Investing in Software&nbsp; A few years ago I created a software platform on SportsCardDatabase. com that automatically gathers marketplace data and calculates market values for over 1 million consumer products. ProjectSportsCardDatabase. com value engineInvestmentsPurchase of server hardware (money)Purchase of Web hosting (money)Development (time)Integration and partnerships (time)ReturnSubstantial learning opportunityRevenues from the siteBenefits gained by users of siteAlternatives to this InvestmentI was unable to find an alternative productized solution that met the requirements. The manual process performing the same steps for determining the values would be a full time job for 10 people. It likely would have had a higher rate of errors. &nbsp; Had I not made the investment to build and support this custom software, then there is no way I could have ever been able to offer the core functionality for this profitable business. It has empowered me to gain greater understanding of the market and build additional functions such as arbitrage deal finder (based off the market values). Investment in people Investing in people revolves around allocating resources with the expectation of higher productivity or value. &nbsp; How can a company invest in its people &nbsp;There are several ways organizations can invest in people: Training - The action of teaching a particular skill or type of behavior. There are many ways people can learn valuable skills and acquire knowledge, such as taking an instructor led class, peer led activities/examples, consuming or writing documentation, testing things, or doing the functions of your desired job. &nbsp;Making Process or Culture Changes &ndash; Providing an environment that will better position employees with the opportunity to succeed. &nbsp;Effective Hiring and Review Process &ndash; Employees may often find to opportunities for higher productivity in an environment with more effective and productive coworkers. In general, you are average of your peers. Why invest in people? Companies often decide to invest resources in their people with the anticipation of the following positive outcomes: In many cases, your people and your process are what set you apart from your competitors. &nbsp;It is the people who create future performance and results, in turn helping shape your organizational culture. Leverage talent &nbsp;to reach strategic objectivesHigher levels of employee commitmentCreate opportunities for learning as an organizationCost savings due to ability to self-staff projects and not having to pay premium rates for in demand skillsetsA recent&nbsp;study&nbsp;documented the positive outcome of people investment: &ldquo;The findings are quite remarkable. &nbsp;In general, we found a clear relationship between training expenditures per employee and financial performance in the following year. &nbsp;Almost all financial measures (stock performance, income per employee, gross profit &nbsp;margin, market value per employee) are significantly higher for those companies that &nbsp;spend an above-average amount per employee on training. &ldquo; &nbsp; Why Companies often Don&rsquo;t Invest in People? In general, there are many things within companies competing for resources. It is often difficult to justify committing precious resources to people often over other visible business objectives. &nbsp; The same study noted the inherent financial bias against people investment:&nbsp; &ldquo;Further, because training and education are treated on a firm&rsquo;s books as costs, not as investments, those firms that make such investments must do so in spite of the pressures of the market (to reduce costs) rather than because of them (as might be the case if the market had the information necessary to recognize such expenditures as worthy investments). &nbsp;This leads to a collective tendency to under-invest in human capital &ndash; more inefficiency that affects society as a whole. &nbsp;It&rsquo;s bad for stockholders and firms, and it&rsquo;s bad for the people who work in them, since research has found that workplace training is an important determinant of workers&rsquo; future earnings capacity. &ldquo; So training is considered an operational expense, but paying a considerable amount more for a professional services (software) solution may be considered a more desirable capital expense in some companies. &nbsp; Example Scenario for Investing in People&nbsp; I recently made the decision to stop working on new features (despite significant pressure to progress on feature development) and spend 2 full weeks writing detailed user documentation for new set of tools. &nbsp; ProjectDocumentation of ToolsetInvestmentsDevelopment of Documentation (time)Presentation of Documentation to various user groups (time)ReturnOnboarding materials allowed for greater productivity for team membersLower volume of support issues and time spentBetter collaboration and integration with other teamsBetter opportunity to avoid inconsistent implementations and technical debt in future projectsAlternatives to this InvestmentWithout the documentation, members of the team and outside teams had less clarity of the toolset, leading to significant inefficiencies. &nbsp; Had I not made the investment to create this documentation, the most valuable resources would have continued to be consumed with low value tasks (such as support). With the documentation, much of the support is eliminated and it improves the opportunity for all team members to contribute on higher value contributions. Should We Invest in People or Software? So back to my friend&rsquo;s question,&nbsp;When do you start investing in people and stop investing in software? After a little bit of back and forth, I found the reason for his question originated because his boss showed interest in automating a bunch of manual or inconsistent procedures to improve efficiency. &nbsp; My immediate answer to him was &ldquo;when it makes sense or it is more cost effective&rdquo;. We know that companies only have a limited set of resources to invest (time, money, energy). Each company needs to prioritize the business objectives it wishes to solve, and managers need to come up with the most effective strategy to support solutions to the top priorities. &nbsp; When it makes sense: So there is no one answer, each business objective will likely need its own analysis. In some cases, a software solution can look like a good candidate to make progress in solving the designated problem, such as implementing an accounting package/ERP system to help with tracking uncollected Accounts Receivables. In other cases, there may not be some software product or solution that is a good fit to solve the designated problem. An example could be the objective for better overall adherence to the company&rsquo;s organizational policies through better training materials and sessions for middle level managers. &nbsp; When it is cost effective: So I can think of many scenarios where cases could be made for an investment in both people and software. &nbsp; These days analysis of big data is really hot in enterprises, but it is challenging to find people with the Data Science background to fill this role. While I could certainly see an internal teams wanting to take on this type of project (and gain from the very valuable learning), it likely would be more cost effective to bring established software tools (Hadoop, Hive) and an experienced outside vendor. Time to market is often a huge priority for startups and new projects. Implementing a quality software solution may require most often considerable time (and likely budget), as well as a certain level of skill, and new ventures may not be able to wait. &nbsp;Instituting in a less efficient manual process up front may allow to move forward. You&rsquo;ll have the opportunity to actually use the process, get feedback, and learn. Then you can make iterative educated decisions based on what you have learned, deciding if/when you need to automate. &nbsp; Conclusions You can certainly invest in both software and people, they are not mutually exclusive. &nbsp;For some reason they don't always occur in tandem like I'd think they should. It just seems like a lot of companies are very comfortable with the investment related to buying or building a piece of software to fill some business need. They very often don't realize or don't properly scope out the corresponding investment in people that will need to be made to enable the software implementation to be successful. I lose sleep thinking about how my career has been littered with me having to pick up the pieces from these scenarios - Content Management Server, Interwoven, Documentum, Sharepoint, Vignette, Commerce Server, BizTalk, Control-M, Bladelogic, and a slew of homegrown frankenstiens. &nbsp;I&rsquo;d think the companies that prioritize becoming efficient in the execution toward their objectives will invest in their people. It is part of their culture and employees understand how the investment will benefit both them and the company. While investment does not mean the company will be successful in reaching the objectives, but it enforces the objectives&rsquo; priority. I read this exchange on a&nbsp;linkedin forum&nbsp;and it hits home:CFO asks his CEO, &ldquo;What happens if we invest in developing our people and then they leave the company?&rdquo; CEO answers, &lsquo;What happens if we don&rsquo;t, and they stay?&rdquo;&nbsp;A member provided this very insightful comment:&ldquo;Everybody leaves sooner or later, but if you're going to implement a strategy it's going to take skilled people. You only have two choices: buy the skills developed by somebody else at a cost premium that stays forever or build them internally more cheaply. An added bonus is you could become an 'academy' company like GE, IBM, and Xerox that talented job applicants compete to work for because of the development opportunities. &rdquo;&nbsp;I&rsquo;ve spent time at both companies that invest in their people and those that do not invest in their people. I realized greater productivity and higher satisfaction when I recognized a planned investment was being made in me and my teams. &nbsp;"
    }, {
    "id": 47,
    "url": "http://localhost:4000/powershell-email/",
    "title": "Sending email through Gmail SMTP server - C# and Powershell examples",
    "body": "2013/01/13 - I know I haven't posted much code on this blog, but this snippet I found valuable. While I can certainly host my SMTP server, it would be so much easier if I could just Google's (since I am using Google Apps for my site's email). Below is the code you can use to send email's via Google's SMTP server. enjoy. . . C# 12345678910string GmailUserName= myusername@gmail. com ;string GmailPassword= mypassword ;string SendTo= sendto@gmail. com ;string EmailSubject= Test Subject ;string EmailBody= Test Body ;System. Net. Mail. SmtpClient client = new System. Net. Mail. SmtpClient( smtp. gmail. com , 587);client. Credentials = new NetworkCredential(GmailUserName, GmailPassword);client. EnableSsl = true;client. Send(GmailUserName, SendTo, EmailSubject, EmailBody);Powershell 1234567891011121314151617$emailSmtpServer= smtp. gmail. com $emailSmtpServerPort= 587 $emailSmtpUser= myusername@gmail. com $emailSmtpPass= mypassword $emailFrom= myusername &lt;myusername@gmail. com&gt; $emailTo= sendto@gmail. com  $emailMessage=New-Object System. Net. Mail. MailMessage( $emailFrom , $emailTo )$emailMessage. Subject= Test Subject $emailMessage. IsBodyHtml=$true$emailMessage. Body=@ &lt;p&gt;Here is a message that is &lt;strong&gt;HTML formatted&lt;/strong&gt;. &lt;/p&gt;&lt;p&gt;From the SMTP script&lt;/p&gt; @$SMTPClient=New-Object System. Net. Mail. SmtpClient( $emailSmtpServer , $emailSmtpServerPort )$SMTPClient. EnableSsl=$true$SMTPClient. Credentials=New-Object System. Net. NetworkCredential( $emailSmtpUser , $emailSmtpPass );$SMTPClient. Send( $emailMessage )After this code sends the email, it will appear in the account's Gmail sent mail folder (just as if you had sent it via tthe gmail. com site). Since Gmail does not have much of a mail merge option unless you use Google Docs, I wound up using this code for personalizing messages within an opt-in email newsletter. "
    }, {
    "id": 48,
    "url": "http://localhost:4000/good-and-bad-places/",
    "title": "The Difference between a Good and Bad Place to Work.",
    "body": "2012/12/24 - I enjoy reading the insights I can find from&nbsp;Ben Horowitz (he was founder/CEO of Opsware and nowadays is founder/partner at&nbsp;Andreessen-Horowitz&nbsp;which manages near $3B). &nbsp;I recently read one of his&nbsp;blog posts&nbsp;where he provided a member of his team with his view of the difference between a good place to work and a bad place to work: &ldquo;In good organizations, people can focus on their work and have confidence that if they get their work done, good things will happen for both the company and them personally. It is a true pleasure to work in an organization such as this. Every person can wake up knowing that the work they do will be efficient, effective and make a difference both for the organization and themselves. These things make their jobs both motivating and fulfilling. In a poor organization, on the other hand, people spend much of their time fighting organizational boundaries, infighting and broken processes. They are not even clear on what their jobs are, so there is no way to know if they are getting the job done or not. In the miracle case that they work ridiculous hours and get the job done, they have no idea what it means for the company or their careers. To make it all much worse and rub salt in the wound, when they finally work up the courage to tell management how fucked up their situation is, management denies there is a problem, then defends the status quo, then ignores the problem. &rdquo; For the next few hours, I just kept thinking about how he nailed these definitions so well. It also got me thinking about how I would characterize my own employment history and those described to me by some of my friends. It certainly reinforced just how important culture is to an organization and to the employees. What is Culture? Wikipedia&nbsp;defines&nbsp;Organizational culture&nbsp;as the collective behavior of humans who are part of an organization and the meanings that the people attach to their actions. Culture includes the organization values, visions, norms, working language, systems, symbols, beliefs and habits. It is also the pattern of such collective behaviors and assumptions that are taught to new organizational members as a way of perceiving, and even thinking and feeling. Organizational culture affects the way people and groups interact with each other, with clients, and with stakeholders. A&nbsp;Strong culture&nbsp;is said to exist where staff respond consistently to stimulus because of their alignment to organizational values. In such environments, strong cultures help firms operate like well-oiled machines, engaging in outstanding execution with only minor adjustments to existing procedures as needed. Conversely, there is&nbsp;weak culture&nbsp;where there is little alignment with organizational values, and control must be exercised through extensive procedures and bureaucracy. Why is My Company&rsquo;s Culture Important? In&nbsp;another post, Horowitz touches writes about designing a way of working that will: Distinguish you from competitorsCorporate Culture is The Only Truly Sustainable Competitive Advantage. &nbsp;Given enough time and money, your competitors can duplicate almost everything you&rsquo;ve got working for you. They can hire away some of your best people. They can reverse engineer your processes. The only thing they can&rsquo;t duplicate is your culture. I love this series on how easy it is to poach tech talent. . . Guy bumps into a competitor&rsquo;s star engineer at a trade event:&ldquo;Would you come work for us if we gave you $1 million/year?&rdquo;&ldquo;I would. &rdquo;&ldquo;How about $50,000/year?&rdquo;&ldquo;What do you think I am?&ldquo;We&rsquo;ve already established that. Now we&rsquo;re negotiating. &rdquo;While not everyone is for sale, enough are to make you vulnerable. &nbsp;Ensure that critical operating values persist such as delighting customers or making beautiful productsEveryone needs to be working toward the same mission and goals. Clearly defined roles and responsibilities makes for more efficient pursuit of those goals. &nbsp;Help you identify employees that fit with your missionI read over and over that you should always only hire the best&hellip; and not accept less. But how do you define the best? Does that mean the most intelligent or potentially most productive individual? Are you looking for someone that matches the specific skills listed on a job description?My buddy Ed and I once had a discussion about how to build a team of developers. I asked if he was going to blow his budget and get someone senior level SME to move his projects along. His take was that team fit was his number one priority, and wasn&rsquo;t interested in having a rock star prima donna. He said he found smart motivated people can figure out the technical solutions, but the bad apples will cause more pain than the value they provide. This was detailed really well in the book&nbsp;The No Asshole Rule&nbsp;(unfortunately I lent my copy to an asshole who never returned it). Tony Hsieh&rsquo;s (founded and sold both LinkExchange and Zappos) advice on how to hire the right people to build a great company culture: From the beginning, company culture was always important. Is this someone I would choose to hang out with or grab a drink with, if we weren&rsquo;t in business? If the answer was no, we wouldn&rsquo;t hire them.  How does Culture relate to Strategy and Execution? I love this diagram that&nbsp;Michael Sahota from agilitrix&nbsp;came up with: It makes you think about how to view many of your important initiatives, especially why you want to do something and the relative importance of the outcome. If you are introducing a new activity, you can align it with one of the 3 aspects: Tactics &ndash; &ldquo;How do we work?&rdquo; is about day to day practices and process elements. These are things that a team or organization can adopt. Strategy &ndash; &ldquo;What do we want to achieve&rdquo; is about aligning the company around key goals and initiatives. Culture &ndash; &ldquo;Who do we want to be?&rdquo; is about clarifying the organizations reason for existing as well as it&rsquo;s values and vision. Culture is the foundation for Strategy and Tactics, and can lead to better efficiency in delivering activities in those groups. If you are part of an organization where it is part of the culture to regularly set widely understood goals with measurable objectives, then there likely won&rsquo;t be unnecessary barriers to deliver and assess results. Conversely, it is much harder to devise strategy and/or execute work when the mission is not widely and consistently understood. Making a bad&nbsp;organization&nbsp;into a good organization. When Ben Horowitz realized that one of his middle managers had neglected to meet with his subordinates for 6 months, he explained his displeasure with the situation and used the power of his leadership position to demand change (through threat of termination if the problem was not corrected within 24 hours). The transformation of an organization (in Horowitz's terms of bad to good) likely will require some sort of culture change. A lot of time we'll see a big reorganization or new managers specifically&nbsp;introduced into a company to be a catalyst for culture change. The opportunities with the best chance for effective organizational change seem to have all of the following elements:A leadership providing a concrete vision of the futureManagement tools in place to track/measure executionRuthless use of both positive and negative reinforcementA recent study profiled some instances where some high profile executives were able to enact culture change. Here is there guidance:&lt;ul &gt;Do come with a clear vision of where you want the organization to go and promulgate that vision rapidly and forcefully with leadership storytelling. Do identify the core stakeholders of the new vision and drive the organization to be continuously and systematically responsive to those stakeholders. Do define the role of managers as enablers of self-organizing teams and draw on the full capabilities of the talented staff. Do quickly develop and put in place new systems and processes that support and reinforce this vision of the future. Do introduce and consistently reinforce the values of radical transparency and continuous improvement. Do communicate horizontally in conversations and stories, not through top-down commands. Don&rsquo;t start by reorganizing. First clarify the vision and put in place the management roles and systems that will reinforce the vision. Don&rsquo;t parachute in a new team of top managers. Work with the existing managers and draw on people who share your vision. &lt;/ul&gt;Conclusions and Observations&nbsp;Having a strong culture is not the end all be all. There are tons of companies with great cultures that don't become financially viable. You need the right product mix/business model if you are going to succeed. And there are plenty of companies with weak cultures that make boatloads of money (often&nbsp;blue oceans). &nbsp;&nbsp;Culture is the one thing you can control in business. It's so tough to accurately manage customers expectations/actions and predict market conditions, but it much easier to set the tone to effectively manage operational activities (such as customer service, engineering, fulfillment, etc. ) within your company. I would love to always work for Ben Horowitz's definition of a  good organization , because those are the organizations that maximize on the opportunities they can fully control. &nbsp;&nbsp;Relativity has a lot to do with how you perceive the culture you are in. After spending many years in consulting, I'd often view an organization's culture as  what it takes to get something done here . What do I need to do and who do I need to involve/appease to get the desired result?&nbsp;As an full time employee for a large company the past 5+ years, my perspective is more about aligning my objectives and execution with the corporate vision. &nbsp;A lot of people I know consider perks (like having company provided alcohol/entertainment, company wide video game tournaments, hackathons, etc. )&nbsp; as culture. While it may be a great reward that distinguishes a company from others, they&nbsp;will not establish a core value that drives the business and help promote in perpetuity. They are not specific with respect to what your business aims to achieve. &nbsp;&nbsp;When I worked at Dell, one of messages pushed down form the top was &nbsp;their commitment to continuous improvement (through Six Sigma). I have since seen a number of companies that don't systematically prioritize process improvements and their teams lose efficiency from repeatedly facing the same common problems. "
    }, {
    "id": 49,
    "url": "http://localhost:4000/build-vs-buy/",
    "title": "When to Build vs. Buy",
    "body": "2012/10/22 - I had been lucky enough to get to work on a diverse variety of business models and approaches to solving problems over the past 20 years. For most of the major initiatives these companies face, the stakeholders must usually choose between the following: Do it yourselfGet some helpThis is often referred to as a Build vs. Buy scenario. Building&nbsp;is when a company decides to use existing resources (or bring on new resources) to create some thing or acquire the capability it needs. Buying&nbsp;is when a company looks for outside help to fulfill a need. This may not necessarily involve spending money. Basic Build vs. Buy Categories In a perfect world, I see 3 different categories for Build vs. Buy scenarios Build only Key ItemsBuy/Automate commodity business processes; build when you're dealing with the core processes that differentiate your company. In many cases, there may not be an option to buy something to adequately accommodate the differentiators. There is a reason most of the Financial Services firms employ large custom development teams, as their needs are often highly unique and complex. In addition, a company's Intellectual Property is likely something that makes its business unique and it is not something it would want to share with competitors. If it builds the key software and processes internally, then they have greater control over it. &nbsp;Buying Something Not Easily BuiltThere are opportunities where it makes sense for companies to look to Buy key differentiators rather building them. Market Share is certainly a key differentiator to many businesses. While it may be feasible for a company to gain market share in established verticals (red ocean) through quality offerings and execution, it usually takes a long time to gradually win market share from competitors. So it sometimes makes sense to buy market share - via an acquisition or partnering relationship. In the past 2 years, Facebook bought Instagram and Microsoft bought Skype for billions (at multipliers far beyond there projected earnings). Facebook and Microsoft each have tremendous resources and internal talent, and probably could have rolled out competing offerings to the ones they bought. But they decided acquisition and market share growth was key. Supplementing the Build strategy by Buying (hybrid)Buying may be a great way to initiate growth and/or progress. It sometimes makes sense to buy components of the solution to make the build approach move faster and in a more stable way. When buying a company, the brand, capital, intellectual property, market share, and resources (including employees) all come with it. In the tech world, I read about Acqui-hires, where companies are bought as a way to acquire talented individuals to supplement their own teams. Another example may be to bring in a Subject Matter Experts (SME). While I know many people choose to design/renovate parts of their homes, supplementing the process with a quality architect often leads to better results and fewer overages. A third example may be to use/review as a commercial off the shelf (COTS) software as a starting point for your application. I'll include open source when referring to COTS for the purposes of this post. In many cases, the software&rsquo;s developers have spent time considering the problem and likely have accounted for valid use cases you may not have considered. Looking at the way different ecommerce represent products, product variants, and promotions in their data models (databases) can be a great help even when you choose the build option. Since these decisions are always driven by some business initiative, it is not so easy to simply lump your problem into one of these buckets. You are going have lots of cases where it looks obvious on the surface whether to simply build or buy, but there is more to consider (detailed in the next section): Factors to consider in the decision Let's assume there is problem you need to solve and there is both a potentially viable Build and Buy option. The following factors (from&nbsp;http://www. nevo. com/our-knowledge/whitepapers/BuildVsBuy. pdf) I've used for decisions concerning software platforms, but many apply to the other categories: Significance&nbsp;The level of importance the problem this decision needs to solve as compared to other competing initiatives. &nbsp;&nbsp;Coverage&nbsp;When considering buying a COTS software, you need to consider the gap between its functionality and your requirements. &nbsp;&nbsp;Direction&nbsp;The solution needs to support the planned future direction for the initiative and company. Consider the flexibility, maintainability, and extensibility the application. Total Cost of Ownership (TCO)&nbsp;TCO includes not only the cost of acquisition, configuration, and customization, but also the ongoing support, maintenance, and evolution of the application. It is quite common for lifetime costs to dwarf acquisition costs. Scale&nbsp;Consider the application&rsquo;s ability to handle the current and foreseeable future scaling requirements. As the business need grows, how will the application perform and how will the cost structure (licensing) grow?&nbsp;&nbsp;Timing&nbsp;Conventional wisdom held that implementing a packaged solution was faster than custom development. As one might expect, this is an oversimplification. The process of installing, configuring, customizing, and completing data conversion for packaged solutions routinely involves tasks that are as complex and extensive as custom development, with far less flexibility in phasing and timing. &nbsp;COTS packages may offer greater predictability with respect to implementation time, but that is largely a reflection of the restrictions they impose on capabilities and flexibility&nbsp;Standards&nbsp;Standards may be the most important criterion of all. COTS vendors market the notion that the cost of software development can be spread across that a large user community, thereby reducing the cost to each individual customer. &nbsp;Volatility / Risk&nbsp;By &ldquo;volatility&rdquo; we mean the frequency and complexity of new releases. Greater volatility means increased support and maintenance costs, since a common characteristic of packaged solutions is that customers must test, integrate, and install each release, whether it contains desired enhancements or not. &nbsp;Each new release presents substantial risk to the stability and availability of the system to users. The burden of validating content, testing. &nbsp;Business Process&nbsp;While it is common for packaged solution vendors to claim complete flexibility in configuring their solution to meet existing business process this is rarely the case. Most often client organizations are urged to modify their business practices to conform to the range of choices that the package offers. &nbsp;Scenario when I Built For a few years I looked everywhere for a site where I could find accurate market values for sports cards. While I could find companies offering a retail suggestion (like blue book for cars), there wasn't anything that provided guidance on market values in close to real time. I looked into quite a few platforms and frameworks for general price determination, learning from some of their principles. I decided that none were offered the functionality I required to specifically support this vertical, so I decided to develop my own. The platform went on to become SportsCardDatabase, and has 1000's of registered users visiting daily. Scenario when I Bought When I wanted to add a chat function to one of my sites I was faced with this scenario. My first gut instinct was to think about how I wanted it to work. I quickly surfed around for some Jquery plugins, started mapping how I could persist the data, designing some interfaces for RESTful services. I figured it would be a fun learning experience that would probably eat up a few weeks of my time. Before writing any actual code, I did a few quick searches and found an affordable package that met most of my requirements. In a few hours of playing with the demo, I satisfied the key enhancement I required - integrating the chat login with my site's member accounts. As much I thought it would be fun to build a chat platform, that is not the key differentiator for my business. I knew that buying and implementing the package made complete sense. About 2 years later, I revisited the decision when I had requirements that the package I had bought could not accommodate. The first thing I did was to look to  buy  something. I was able to quickly find a free open source alternative that was more robust and user friendly than my previous package, and that is what my site uses today. I love the fact that I have the source code, so I was able to make some small enhancements that are specific for my site's requirements. Scenario when I both Bought and Built About 7-8 years ago, I took on a consulting project to build a web site for a client. I had the ability to design the site and code all the functions required, but I wanted to make the best use of my time and the client's budget. Instead of spending the considerable amount of time trying to sell a creative design (not really my strength), I decided it would be a good idea to let the client choose the design from a choice of templates. I directed him to&nbsp;templatemonster. com, and he was able to select a design he liked in a day (for around $100). If I was to try to design something for him, it would have taken many iterations of proposing design ideas and review - and certainly cost more than $100. I used that design template and was able to spend all my time building his site - the part I liked most. The project came in below the estimated budget, and ahead of schedule. "
    }, {
    "id": 50,
    "url": "http://localhost:4000/bayes-theory/",
    "title": "Bayesian Theory and the Theory of Life",
    "body": "2012/10/04 - One of the reasons I love working in Manhattan is all the cool opportunities to learn and the networks of interesting people. Every Monday I receive an email from tech investor&nbsp;Charlie O&rsquo;Donnell&nbsp;&nbsp;with classes a list of classes and meetups for entrepreneurs going on around New York City. Last week I found out and signed up for a high level class on Bayesian Theory and probability (called&nbsp;Bayesian Theory and the Theory of Life)&nbsp;that met during lunch time a few blocks from my office. The class offered through Skillshare (interesting but seemingly immature startup) and the instructor was&nbsp;Albert Wegner, a partner with venture capital firm Union Square Ventures. I am someone who generally questions a lot of accepted hypotheses, baselines, and beliefs. So I was interested to see this presentation how we could use Bayes Theory to test whether a prior belief is correct in relation to what we measure. His agenda was written on the white board as follows: &nbsp;Why does uncertainty exist?Why do we suck at dealing with #1?How do we think correctly about uncertainty?What does it mean for life?Some Background Exercises and Concepts So Wegner started the class having us answer some multiple choice and open ended questions, and they helped to illustrate some excellent points: &nbsp;The source of uncertainty is limited knowledge of the world. &nbsp;Can we ever really be certain about anything?o&nbsp;&nbsp;&nbsp;Maybe we can have certainty for really small systems. Even then it is very likely there is some level of the unknown that could affect our certainty of an outcome. o&nbsp;&nbsp;&nbsp;We also need to consider how the cost of observations affects our certainty. Are we sacrificing something important to make those observations? o&nbsp;&nbsp;&nbsp;We also need to consider how the speed at which we can make observations affects our certainty. Can we make and process observations quickly enough for them to be relevant? o&nbsp;&nbsp;&nbsp;Quantum uncertainty &ndash; if we observer something too closely we may change what you are trying to measure. One of the scenarios described an introverted shy, nerdy individual and then asked which was a more likely career path &ndash; librarian or truck driver. The correct answer was truck driver, because there are 1. 5 million of those positions available vs. 150,000 librarians. Even if there is a possible social predisposition, it would be difficult to assume that it would weigh greater than the 10x multiplier difference of the population dynamics. This concept is referred to as the&nbsp;Base Rate Fallacy. &nbsp; He gave an example that the Bill Gates &nbsp;Foundation did a study that the best schools in the US were small schools, the foundation and government gave small schools lots of money. He then commented that small schools also accounted for the worst schools, while larger schools seemed to have a more middle of the road position. He noted that the law of small numbers could be in play here, as analyzing small populations can lead to misleading extremes. It is very popular a small school could have a majority of either good or bad students. The larger populations will have both good and bad averaged together, accounting for a blended score. My favorite question was &ldquo;Intelligent women tend to marry less intelligent men. Why?&rdquo; The asked this to half the class, and the other have had intelligent men with less intelligent women. &nbsp;He drew the following diagram on the board for us:In a fairly common linear regression scenario, we can draw a line that that shows the trend for all the individual data points. He then drew an oval around the data points. He explained that as the smartness of woman got to the higher end, the number of men under the line in the oval increased proportionally. The inverse would be true as the smartness of men increased. He mentioned that smartness of men and woman are imperfectly correlated &ndash; meaning that they do not go up together and one may go down as the other goes up. We don&rsquo;t observe probability distributions, only outcomes. &nbsp; So we try to tie our expectations about uncertainty to outcomes. &nbsp;&nbsp;2 types of modes our brain works in:o&nbsp;&nbsp;&nbsp;Type 1 &ndash; assumptions and thought processes we just make automatically o&nbsp;&nbsp;&nbsp;Type 2 &ndash; we knowingly realize we need deeper analysis, and take a step back to do the analysis His explanation Bayesian Theory&nbsp; Bayes Theory is about testing hypothesis against the observed outcomes. &nbsp;He gives an example about home HIV tests. Here are the numbers we&rsquo;ll dive into: -&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The US population had 309 million people and 1. 2 million had HIV (0. 3%). -&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The home test is 99. 8% accurate in measuring people who don&rsquo;t have HIV. -&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The home test is 92% accurate in measuring people who do have HIV. &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;1 million people&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;|&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; |&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;------------------------------------------------------------&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;3,000&nbsp;&nbsp;&nbsp; | &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; | &nbsp;997,000&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;| &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; |&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;HIV &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; NO HIV&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;| &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; |&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;| &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; |&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;----------------- &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;-----------------&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 92% &nbsp;| &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; |&nbsp;&nbsp;&nbsp; 8% &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;0. 2% &nbsp;| &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;| 99. 8%&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; | &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; | &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; | &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;|&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;2,760 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 240 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 1,994 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 995,006&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;+ &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;- &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;+ &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; - -&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The 995,006 have correctly tested negative -&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The 1,994 have tested false positive -&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The 240 have tested false negative -&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;The 2,760 have correctly tested positive E=Evidence, H=Hypothesis, P=Probability &nbsp; E !E H P(H&amp;E) P(H&amp;!E) !H P(!H%E) P(!H&amp;!E) &nbsp; P(H) = 0. 3%(the hypothesis or prior belief) P(H|E) = 92%(probability of hypothesis given evidence observed) P(E) = P(H&amp;E) + P(!H%E) = 2760+1994 = 4754&nbsp;(total of expected positive with false positive) P(H&amp;E) &nbsp;/ P(E) &nbsp;= 2760/(2760+1994) = 58% &nbsp;(divide expected positive by total of expected positive with false positive) Our goal is to calculate the probability that the hypothesis is accurate, given the fact that the evidence observed. &nbsp;Using the formula for Bayes' theorem, we have: Bayes Theorum:P(E|H) &nbsp;/ P(E) * P(H) = . 92 / . 04754 * . 0003 = . 0058(divide the probability of hypothesis by expected positive the divide &nbsp;by total of expected positive with false positive and then multiply by the hypothesis) Any time P(E|H) is greater than P(E), you likely have a good test that will provide lift. In this case the 0. 58% of total people attributed to having HIV is increased compared to the hypothesis of 0. 3%. Strong prior belief leads to lesser updating. People tend to hold beliefs and signals (tests) will be less impactful. My take on the class I very much enjoyed the course content and my refresher on probability (it has been 15+ years since college since I had classes covering probability). The instructor did a good job with the background exercises, as they did a good job illustrating the concepts. I would have liked him to spend a little more time explaining the value of Bayes Theory before jumping into the scenario. It became a little confusing (even for the instructor) whether the HIV test example represents a good (reliable) test based on the statistics. I came out of the class thinking it was about validating a hypothesis, but I see it now being more about understanding the relationship between correlated probabilities. In the last 2 minutes of the class he was trying to tie in the discussion of the class to how we can deal with life's uncertainties. I thought it was a bit of a stretch, but it was entertaining. Applying it to one of my projects: After the class, I read up on Bayes Theorum a little bit online and found some descriptions a little easier for me to understand. The Problem I am interested in how visitation relates to conversion rates between free membership and paid subscriptions for SportsCollectors. Net. Using Bayes to figure it out -&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Out of 33,000 registered users, 6,842 have subscribed (20. 7%) -&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Members that visit the site 50 or more times become subscribers 74% of the time. -&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Members that the site less than 50 times become subscribers 8% of the time. P(S) = Probability of member subscription = . 207 P(A) = Probability member has 50+ visits = &nbsp;. 23 P(S|A) = Probability subscriber is a member with 50+ visits = &nbsp;. 74 P(B) = Probability member has under 50 visits = &nbsp;. 77 P(S|B) = Probability subscriber is member with under 50 visits = . 08 P(A|S) = &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;P(A)P(S|A)&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;. 23&nbsp;* . 74 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; . 1702&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ----------------------------------&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;= ----------------------------&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; =&nbsp; ------------&nbsp; &nbsp;= &nbsp;&nbsp;. 7343&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; P(A)P(S|A) + P(B)P(S|B)&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;. 23&nbsp;* . 74 + . 77 * . 08 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;. 2318 The probability of a person visiting the site 50+ times is going to buy a subscription is at 73%. P(B|S) = &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;P(B)P(S|B)&nbsp;&nbsp;&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; . 77&nbsp;* . 08 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; . 01816&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; ----------------------------------&nbsp;&nbsp; &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;= ----------------------------&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; =&nbsp; ------------&nbsp; &nbsp;= &nbsp;&nbsp;. 2657&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; P(B)P(S|B) + P(S)P(S|A) &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;. 77&nbsp;* . 08 +&nbsp;. 23&nbsp;* . 74 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; . 2318 The probability of a person visiting the site less than 50 times is going to buy a subscription is under 27%. Analysis: From these numbers, I&rsquo;m going to consider increasing my attention to getting non premium members to visit the site more regularly (with freemium features) if a goal is to convert them to paying subscribers. I am not sure it is realistic to expect a new user to visit the site 50 times. I will re-run the procedure with thresholds of 10, 20 ,and 25, 35 visits to see how the relationship of visitation on subscription rates changes. Pseudo SQL Code that could be used to get this info: DECLARE&nbsp;@threshold&nbsp;intDECLARE&nbsp;@TotalUsers&nbsp;intDECLARE&nbsp;@TotalSubcribers&nbsp;intDECLARE&nbsp;@SubcriptionProbability&nbsp;decimal(5,2)DECLARE&nbsp;@TotalUsersAboveThreshold&nbsp;intDECLARE&nbsp;@TotalSubcribersAboveThreshold&nbsp;intDECLARE&nbsp;@SubcriptionProbabilityAboveThreshold&nbsp;decimal(5,2)DECLARE&nbsp;@TotalUsersBelowThreshold&nbsp;intDECLARE&nbsp;@TotalSubcribersBelowThreshold&nbsp;intDECLARE&nbsp;@PercentUsersAboveThreshold&nbsp;decimal(5,2)DECLARE&nbsp;@PercentUsersBelowThreshold&nbsp;decimal(5,2)DECLARE&nbsp;@SubcriptionProbabilityBelowThreshold&nbsp;decimal(5,2)DECLARE&nbsp;@ProbablityAboveThresholdWillSubscribe&nbsp;decimal(5,2)DECLARE&nbsp;@ProbablityBelowThresholdWillSubscribe&nbsp;decimal(5,2)&nbsp; SET&nbsp;@threshold=50SET&nbsp;@TotalUsers=(SELECT&nbsp;COUNT(USERID)&nbsp;From&nbsp;UserInfo)SET&nbsp;@TotalSubcribers=(SELECT&nbsp;COUNT(USERID)&nbsp;From&nbsp;UserInfo&nbsp;WHERE&nbsp;IsPremium = true)SET&nbsp;@SubcriptionProbability=CAST(@TotalSubcribers&nbsp;AS&nbsp;decimal)/CAST(@TotalUsers&nbsp;ASdecimal) &nbsp; SET&nbsp;@TotalUsersAboveThreshold=(SELECT&nbsp;COUNT(USERID)&nbsp;From&nbsp;UserInfo&nbsp;WHERE&nbsp;VisitCount&nbsp;&gt;=@Threshold)SET&nbsp;@PercentUsersAboveThreshold=CAST(@TotalUsersAboveThreshold&nbsp;ASdecimal)/CAST(@TotalUsers&nbsp;AS&nbsp;decimal)SET&nbsp;@TotalSubcribersAboveThreshold=(SELECT&nbsp;COUNT(USERID)From&nbsp;UserInfo&nbsp;WHERE&nbsp;IsPremium = true&nbsp;AND&nbsp;&nbsp;VisitCount&nbsp;&gt;=&nbsp;@Threshold)SET&nbsp;@SubcriptionProbabilityAboveThreshold=CAST(@TotalSubcribersAboveThreshold&nbsp;ASdecimal)/CAST(@TotalUsersAboveThreshold&nbsp;AS&nbsp;decimal) SET&nbsp;@TotalUsersBelowThreshold=(SELECT&nbsp;COUNT(USERID)&nbsp;From&nbsp;UserInfo&nbsp;WHERE&nbsp;VisitCount&nbsp;&lt;@Threshold)SET&nbsp;@PercentUsersBelowThreshold=CAST(@TotalUsersBelowThreshold&nbsp;ASdecimal)/CAST(@TotalUsers&nbsp;AS&nbsp;decimal)SET&nbsp;@TotalSubcribersBelowThreshold=(SELECT&nbsp;COUNT(USERID)From&nbsp;UserInfo&nbsp;WHERE&nbsp;IsPremium = true&nbsp;AND&nbsp;&nbsp;VisitCount&nbsp;&lt;&nbsp;@Threshold) SET&nbsp;@SubcriptionProbabilityBelowThreshold=CAST(@TotalSubcribersBelowThreshold&nbsp;ASdecimal)/CAST(@TotalUsersBelowThreshold&nbsp;AS&nbsp;decimal)&nbsp; SET&nbsp;@ProbablityAboveThresholdWillSubscribe=(@PercentUsersAboveThreshold*@SubcriptionProbabilityAboveThreshold)/((@PercentUsersAboveThreshold*@SubcriptionProbabilityAboveThreshold)+((1. 00-@PercentUsersAboveThreshold)*@SubcriptionProbabilityBelowThreshold))SET&nbsp;@ProbablityBelowThresholdWillSubscribe=(@PercentUsersBelowThreshold*@SubcriptionProbabilityBelowThreshold)/((@PercentUsersBelowThreshold*@SubcriptionProbabilityBelowThreshold)+((1. 00-@PercentUsersBelowThreshold)*@SubcriptionProbabilityAboveThreshold)) &nbsp;&nbsp; PRINT&nbsp;'Threshold: '&nbsp;+&nbsp;CAST(@Threshold&nbsp;AS&nbsp;varchar(255))PRINT&nbsp;'TotalUsers: '&nbsp;+&nbsp;CAST(@TotalUsers&nbsp;AS&nbsp;varchar(255))PRINT&nbsp;'TotalSubcribers: '&nbsp;+&nbsp;CAST(@TotalSubcribers&nbsp;AS&nbsp;varchar(255))PRINT&nbsp;'SubcriptionProbability: '&nbsp;+&nbsp;CAST(@SubcriptionProbability&nbsp;AS&nbsp;varchar(255))PRINT&nbsp;' 'PRINT&nbsp;'TotalUsersAboveThreshold: '&nbsp;+&nbsp;CAST(@TotalUsersAboveThreshold&nbsp;AS&nbsp;varchar(255))PRINT&nbsp;'PercentUsersAboveThreshold: '&nbsp;+&nbsp;CAST(@PercentUsersAboveThreshold&nbsp;ASvarchar(255))PRINT&nbsp;'TotalSubcribersAboveThreshold: '&nbsp;+&nbsp;CAST(@TotalSubcribersAboveThreshold&nbsp;ASvarchar(255))PRINT&nbsp;'SubcriptionProbabilityAboveThreshold: '&nbsp;+CAST(@SubcriptionProbabilityAboveThreshold&nbsp;AS&nbsp;varchar(255))PRINT&nbsp;' 'PRINT&nbsp;'TotalUsersBelowThreshold: '&nbsp;+&nbsp;CAST(@TotalUsersBelowThreshold&nbsp;AS&nbsp;varchar(255))PRINT&nbsp;'PercentUsersBelowThreshold: '&nbsp;+&nbsp;CAST(@PercentUsersBelowThreshold&nbsp;ASvarchar(255))PRINT&nbsp;'TotalSubcribersBelowThreshold: '&nbsp;+&nbsp;CAST(@TotalSubcribersBelowThreshold&nbsp;ASvarchar(255))PRINT&nbsp;'SubcriptionProbabilityBelowThreshold: '&nbsp;+CAST(@SubcriptionProbabilityBelowThreshold&nbsp;AS&nbsp;varchar(255))PRINT&nbsp;' ' PRINT&nbsp;'@ProbablityAboveThresholdWillSubscribe=(@PercentUsersAboveThreshold*@SubcriptionProbabilityAboveThreshold)/((@PercentUsersAboveThreshold*@SubcriptionProbabilityAboveThreshold)+((1. 00-@PercentUsersAboveThreshold)*@SubcriptionProbabilityBelowThreshold))'PRINT&nbsp;CAST(@ProbablityAboveThresholdWillSubscribe&nbsp;AS&nbsp;varchar(255))&nbsp;+&nbsp;'=('+CAST(@PercentUsersAboveThreshold&nbsp;AS&nbsp;varchar(255))&nbsp;+&nbsp;'*'&nbsp;+CAST(@SubcriptionProbabilityAboveThreshold&nbsp;AS&nbsp;varchar(255))&nbsp;+&nbsp;')/(('&nbsp;+CAST(@PercentUsersAboveThreshold&nbsp;AS&nbsp;varchar(255))&nbsp;+'*'+CAST(@SubcriptionProbabilityAboveThreshold&nbsp;AS&nbsp;varchar(255))&nbsp;+&nbsp;')+((1. 00-'&nbsp;+CAST(@PercentUsersAboveThreshold&nbsp;AS&nbsp;varchar(255))&nbsp;+&nbsp;')*'&nbsp;+CAST(@SubcriptionProbabilityBelowThreshold&nbsp;AS&nbsp;varchar(255))&nbsp;+&nbsp;'))'PRINT&nbsp;'ProbablityAboveThresholdWillSubscribe: '&nbsp;+CAST(@ProbablityAboveThresholdWillSubscribe&nbsp;AS&nbsp;varchar(255))PRINT&nbsp;'ProbablityBelowThresholdWillSubscribe: '&nbsp;+CAST(@ProbablityBelowThresholdWillSubscribe&nbsp;AS&nbsp;varchar(255))&nbsp; "
    }, {
    "id": 51,
    "url": "http://localhost:4000/agency_reunion/",
    "title": "Reunion from the Internet Boom - 12 years later",
    "body": "2012/09/21 - In June 1999, I moved to New York City and took a job as a Site Manager working Agency. com. The job was to manage a team of front end coders responsible for taking a Photoshop design and turning it into a web site. Agency. com had some amazing design folks and really strong client management/strategy, so they had big time clients and did quite well. It was the stereotypical Internet Boom company from that time period: It was located in a great location - 665 Broadway (right around the Village) with open work spaces. Lots of young people working really long hours. The official work day was 10AM-7PM. Incredible creative talent wowing clients. Excellent social experiences - beer on Fridays, lots of food, late night counterstrike battles, booze cruise, and frequent long nights at the bars. &nbsp;The company was doing so well that they public that December. . . . strike price $26 and IPO was $95. I remember it well, because the IPO day was the same day as the Christmas party with&nbsp;Tito Puente was the entertainment. Got to love the excess of the boom times. Although I decided to move to join some friends at another company that February, Agency was truly a really amazing place to work. I learned so much, met so many really good people, did some interesting work, and have so many fond memories. Since then I have worked for 3 more companies and I lost touch with every person from those days except for a handful. &nbsp; Fast forward to yesterday. I logged on to Facebook and noticed that some former colleagues had set up an event for a reunion. I scrolled through the thread and saw so many names I thought I recognized. I figured I would join the fun. I walked into a bar near Delancey Street and noticed there were about 100 people there. Everybody filled out name tags with their name and the years they worked for Agency. In classic downtown style, they had trays of pork dumplings and the beer was open bar for the first hour or so. It was pretty comical seeing so many folks looking at each other's name tags and trying to remember who the other person was after 12 years. &nbsp; I got to meet a creative director that worked at Agency. com for some of the time when I was there. It was pretty funny when we realized that we have both been at my current company for the past 5 years together and never knew each other. New York City is a very small and fun world. Lots of people seem to have moved on to successful roles. Some are executives. Others are running their own companies or at start ups. Others plugging away at corporate jobs. It was good to see so many people following their passions - be it designing handbags, illustrating comics, collecting baseball cards (me), or making fantastic apps. &nbsp; It wound up being a very fun night of catching up people, many I have not seen in 12 years. We reminisced about the crazy projects we worked on, the really interesting antics in the office, the many colorful personalities, and how much we really had fun. &nbsp; "
    }, {
    "id": 52,
    "url": "http://localhost:4000/willingness_to_fail/",
    "title": "My Willingness To Fail Gives Me The Ability To Succeed",
    "body": "2012/09/13 - I was watching the LiveStream from the TechCrunch Disrupt conference, and I heard &nbsp;co-founder of Sun Microsystems and venture capitalist Vinod Khosla say the title of this post: My Willingness to Fail is what allows me to Succeed Khosla was referring to the high risk, high reward investments as part of his multibillion dollar fund. In his world, one really good exit can offset a hundred less successful ones - and he has been pretty successful in finding ambitious tech companies that have had big exits. &nbsp;While I don't quite play in his world as far as monetary risk or reward, I really think his words apply to me and so many people I know. Most businesses fail Business Failure refers to a company ceasing operations following its inability to make a profit or to bring in enough revenue to cover its expenses. A profitable business can fail if it does not generate adequate cash flow to meet expenses. Research from the U. S. Bureau of Labor Statistics&nbsp;suggests that most failures of American startups will occur in the first two years of their existence. &nbsp;After that, the rate of business failure slows: &ldquo;The data show that, across sectors, 66 percent of new establishments were still in existence 2 years after their birth, and 44 percent were still in existence 4 years after. (See chart 1. ) It is not surprising that most of the new establishments disappeared within the first 2 years after their birth, and then only a smaller percentage disappeared in the subsequent 2 years. These survival rates do not vary much by industry. &rdquo; That means by the end of 4 years that over 80% of businesses will have failed. So if you try to start a business, the data supports that you will likely fail. &nbsp;So why do so many people try to? Is failing actually a deterrent or an acceptable outcome?&nbsp; Why people start businesses Building your own company sounds like a really cool thing to do and it can be incredibly rewarding on many levels. But it is insanely hard, often thankless work. I can't even begin to tell you the number of sleepless months I have went through and some of the opportunity costs I probably should have considered in pursuing my dreams. For me it's worth it. From a recent survey among new entrepreneurs, here are the top 5 reasons people start new businesses: Personal Freedom. Plenty of people are frustrated by their daily grind. It's sounds pretty cool to be able to go on your own, control your own fate, and work when you want to. Be careful, because you may get what you ask for. &nbsp;Starting a new business is typically more work, longer hours, and you will be tougher on yourself than &nbsp;yourself. Personal Development. Lots of folks feel pigeon holed or burnt out from work. They don't feel there is an opportunity for personal growth. Starting a new business will absolutely give you the opportunity to see all sorts of new perspectives. You&rsquo;ll have to learn an insane amount (most of which will be stuff outside your existing realm of expertise / comfort zone), you&rsquo;ll grow immensely as a business person and human being. Running a business is hard. It's not all cool stuff, there's usually administrative stuff and customer service that will distract you from personal development opportunities. You get stressed, you get tired, you get distracted. all sabotaging your personal growth. Ego. Many entrepreneurs have a healthy ego. You&rsquo;ll need it. You might even need a slightly unhealthy ego to drive yourself day in and day out to succeed. No matter how great or small your ego you can succeed; but recognizing your own value and the value of your work will improve your success. Ego drives people to say, &ldquo;I can do this better than so-and-so. &rdquo; Often, people realize they can do a better job with what they&rsquo;re already doing as an employee, but on their own. Money. We read about all these amazing exits. &nbsp;Instagram&nbsp; makes no money and&nbsp;Facebook buys them for a billion dollars. It's so easy for us to dream of our idea will be in that a very small percentage of super successful entrepreneurs make more money with their businesses than they would as employees. But is that really the case for everyone? Rob over at&nbsp;businesspundit&nbsp;tells us: The truth is that you are likely to make less money over your career as an entrepreneur than you would in the corporate world. Research shows that only the top quartile of entrepreneurs make more wages than their corporate employed counterparts. That means 75% of entrepreneurs would be better off financially with a regular job. You could counter by saying that wages may suffer but you can build wealth through your business, but you would be wrong again. The majority of entrepreneurs have $20,000 or less in business equity, and 30% of them have zero business equity. Surprising? Not everyone starts a business to replace their full time gig. Lots of people are looking for part time or supplemental income, and not ridiculous wealth. Passion. This is the best reason to start a company. It's something you love and/or a problem you are obsessed with solving. It keeps you from sleeping at night. It's all you want to talk about, think about, work on. Entrepreneurs are a more passionate crowd than non-entrepreneurs. It takes a lot of bravery, faith, and commitment to choose chasing after something you want than more comfortable alternatives. &nbsp; Why do people who want to start businesses never start one So not everyone is meant to be an entrepreneur. It's often an incredibly challenging task to build something new, with a relatively low chance for success. You have to be really committed to what you are doing. &nbsp; I hear plenty of people tell me that their dream is to be entrepreneurial and build a company, but they never move forward in any way. Some of these folks are extremely well educated (MBA, PHD) and have had considerable success in corporate scenarios. Most of them will talk a big game, but aren't willing to try. Here's some of the reasons I routinely hear: I would be working on something right now if I just had an ideaI don't have the money or funding,&nbsp;I don't have a developer&nbsp;I don't think my idea is good enough to make moneyI don't have time or energyI am too busy, it's not worth my timeTo me, these all sound like excuses where people are putting off following their dreams/ambitions in fear of failure. The most important part of building a startup is to start. No matter how you prepare, business is unpredictable and you won't have all the answers beforehand. You are going to make mistakes, you going to not succeed as you'd hope in certain scenarios, you are going to learn, and you are going to change your mind. If you accept there will be some aspect of failure along the way, then there is no reason to fear failing. I would argue that it is a good idea embracing that failure will happen, so we should plan how to quickly find when failures happen and plan to mitigate the impact (referred to as failing fast). We are talking about making your dream become a reality, or at least attempting to. For me, it had better be something really insurmountable to keep me from even trying. My path to entrepreneurship While I was 18-19 years old, I paid for school by working in the warehouse (picking, packing, unloading trucks, inventory) for fast growing and successful sports&nbsp;apparel company. One of the founders had&nbsp;left a senior level position in well established company to build something of his own, changing the industry with his innovations in many ways. &nbsp;He was one of those rare leaders who understood his vision and was able to build a culture of extremely dedicated employees focusing on execution. There were many things I learned from the environment, but the following two statements made to me are what stuck with me: If you don't really try to go after your dreams, you will always live with regret. I am not the smartest person you will ever meet, but no one will ever out work me. 60-80 hours a week of manual labor at $7/hour (while going to college full time) was not my favorite work experience, but I learned an awful lot. I got to see many interesting things I could have never learned in any class - like how the company was organized, the way people interacted, who moved up through the ranks, and the logistics of how product went from design to manufacturing to sales to fulfillment. Since this was how I paid for school, I knew I needed to make the most of the experience to move into something I would find more personally and finanacially&nbsp;rewarding. After my time with this company ended in 1994 (it was sold to competitor for a healthy exit), I knew I really wanted to find something I was passionate about. Since I have loved sports and collecting sports cards ever since I was a little kid, so I figured this would be something I would like to pursue. I also was interested by computers, math, and economics, so I kind of figured these things might give me an opening once I found the Usenet groups in the college computer lab. After finishing a very introductory  Intro to Computers  course during my freshman year, I was determined to build a system to keep track of fantasy sports - as it took forever to compile the scoring manually. I really had no idea what I was doing, but I stayed up many nights&nbsp;on end in the computer lab&nbsp;(neglecting homework and my social life) to figure out how to link text files in excel and ran a bunch of macros to calculate the statistics for my Fatnasy Basketball league. The Com Sci students working in the lab were in shock I was able to make it all work. &nbsp;I wound up building similar spreadsheets to support leagues for football and baseball leagues&nbsp;- and a new business was born. &nbsp;I was able to charge local fantasy leagues for the service for the next year or so. Eventually email and the Internet became more mainstream and the business wasn't really viable. I decided to try something else computer related, it led me to spend time exploring when I found the Internet in the campus computer labs. I taught myself HTML, built my first web site on free hosting from angelfire, and sold sports cards from my apartment my senior year (way before amazon and eBay were mainstream). I also ran auctions via Usenet groups and email groups. While this was quite time consuming, I only made a little bit of profit. The opportunity costs of failure You invested time/resources trying to build your business and it failed. Obviously that investment had some value, that likely did not reach the desired result. &nbsp;I would argue that time is the most valuable asset you can invest because, unlike money or favors, you can never get time back. &nbsp; In a&nbsp;previous post, i defined&nbsp;Opportunity Cost&nbsp;as the benefit that could have been gained from an alternative use of the same resource. So objectively measure what the failure cost you, you'd need to consider other things you could have used your time/resources to achieve. Perhaps working on another venture, spending time with family, sleeping, vacationing, buying a new car/house, etc. . This is an exercise based completely on speculation of what could have happened, and the impact of the cost will vary for each person and scenario. For me, I sacrificed lots of personal time and maybe some performance in my schoolwork. Maybe it would have led to me having a very different outlook and taking a completely different path in my career. Who knows? Failure is not all bad So you fail. . . is it a bad thing? Did you become an entrepreneur to  not fail  or was it something else? Did you want to be inspired, solve a problem, learn something, challenge yourself?&nbsp; Sometimes the process can be more valuable than boolean result of success or failure. Even in some the most classic failures (pets. com anyone), there are great opportunities for personal growth and learning. There's relationships you make along the way. Your failures can often lead to inspiration for something successful to later follow. &nbsp; Since both of my college ventures were not really scalable or sustainable, I guess they would classified as failures. &nbsp;While they didn't make me rich or famous or lead to viable long term employment, I feel like they were a huge success. They taught me that if I defined what I wanted to accomplish and worked really hard, there was tremendous opportunity for me to achieve my goals. The owner of my previous company was right, that I should go after what I wanted and make sure I put in an honest effort to get there. &nbsp; Some good things definitely came from these experiences: My personal interest along with the skills I had picked up led me down a career path in software development and project management, something very different than what I had studied in my college coursework. These projects also helped shape my vision&nbsp;&nbsp;for what I really wanted to create - the ultimate web portal for sports card and autograph collectors. In 2000, I went on to build what would become the largest sports autograph community in&nbsp;SportsCollectors. Net. Fast forward to today (12 years later), the site is loaded with many unique features, 30K+ members, year over year growth, and it has been profitable each of the last 10 years. While I am not wildly wealthy, I have been able to deliver some significant value for my day jobs over the years and to thousands of people that happily pay to subscribe to&nbsp;&nbsp;SportsCollectors. Net&nbsp;every year. I would call both of these things successes. And these successes would have never happened without the experiences of my failures. The next level The most important things I learned from my failures were not apparent right away. In certain projects, I was completely unorganized and made every decision by the&nbsp;seat of my pants. I took some actions that I would have done differently had I planned better. After some time to reflect and spending a few years working in software professional services organizations (where I got to see how other organizations functioned), I realized that that I needed to get better organized to be successful. I needed a to find a&nbsp;systematic approach to pursue any future startups. Here's what i came up with as a phased process for starting a company (that I used when creating&nbsp;SportsCardDatabase. com): Discovery&nbsp;- Honestly assess the feasibility of the opportunity. I was the target customer for SprotsCardDatabase, so I thought I had a decent idea what was viable. I then asked 100's of potential customers about their pain points and what they envision a solution to be. &nbsp;I came up with high level description of what I thought it should look like (with some messy diagrams), some revenue opportunities, and list of constraints. I assessed all of these and decided it would make sense to move forwardThere are plenty of scenarios that you may want to follow, but are not viable. I'd love to play center for the Boston Celtics, but I am 5'10 and not overly athletic. So it's obviously not a feasible scenario and I shouldn't spend any time moving forward. &nbsp;Requirements&nbsp;- Document the vision and the goals. There are many different styles and methods to defining and documenting your business. Some folks write mission statements, some write use cases / functional specifications, and/or some write lengthy business plans. The important thing is to formalize what you want to do and how you will measure your success. &nbsp;Analyze&nbsp;- Design an execution plan to accomplish those goals. Here is were you are going to figure the steps you'll need to take. You'll figure how to break the requirements in workable tasks, how long it take, how much it will cost, what is the best way to get it done, who do I need to get help from. &nbsp;Build&nbsp;- Execute on the plan. Do the work that was laid out in the plan. Build/implement something. Deliver value. &nbsp;Validate&nbsp;- Assess progress and learning. Determine if what delivered is working how you expected. &nbsp;This goes far beyond what most people would call Quality Assurance (QA) - seeing if a feature has the desired functionality. It's about seeing if users actually like the deliverable, if they actually use it, and how they use it. This is the opportunity to learn the most about the viability and impact of what you are delivering. &nbsp;This is also the place where you decide what your next course of action should be. Is it building on top of what you just delivered? Is it moving on to the next item in a punch list of features? Is it something completely different that has been inspired by your learning? Is it taking a break or spending more time doing analysis? Is it abandoning the project?&nbsp;Iterate&nbsp;- Repeat steps 2-5. &nbsp;I am big believer in dividing an ambitious amount of work into smaller pieces that can be delivered individually. This allows me to deliver value faster and gain learning/feedback from how people use them. &nbsp;This allows you to observe successes and failures at a smaller level and make adjustments much faster. In fact, there is a popular term called  Fail Fast  which means you want minimize your risk by assessing failures quickly and putting in mitigation plans immediately. So far I am happy with the progress of the project. I have changed the focus of the site's functionality based on the learning from small failures. I fully expect that as I continue to work on the project, there will be some missteps as I go along and I'll adjust from what I learn. I have no regrets (even from the biggest failures) and I am eternally grateful that I was able to pursue building some things I am passionate about. "
    }, {
    "id": 53,
    "url": "http://localhost:4000/optimized_selling_price/",
    "title": "Is the Right Selling Price an Optimized Price?",
    "body": "2012/09/10 - I think every person running a business needs to regularly ask themself the following question: Given our operating constraints, what is the best mix of products and/or services for us to produce and sell in the period, and at what prices, to generate the highest expected revenue? Whether you are selling penny candy or you are a retainer lawyer with a huge hourly fee, the basic concepts are the same. You need understand your constraints (cost structure, liabilities, resources), your market, and your target customers to formulate a plan to attain maximize performance. Your pricing strategy is going to be a big part of whether you are successful. Today's Landscape of Selling Online Big Data has been really hot the past few years and it's only growing. Now more than ever, we continually initiate more actions that end up as data facts in someone's database. There is a huge urgency for companies to perform data mining/analysis to better understand the our interactions. The end goal usually is to understand our behaviors, better market something to us, and get us to buy something. We have established the synergistic&nbsp;relationship between price and demand&nbsp;in an earlier post. &nbsp;The more organized retailors&nbsp;(Amazon, Target, Walmart)&nbsp;and verticals (grocery chains, airlines, car rentals, insurance) make heavy use of this data in determining the overall demand for a product, your potential demand for that product, and the appropriate price point to offer it. Ther processes often allow them to use predictive analytics to make &nbsp;decisions about their product mix and inventory levels. As its name suggests, demand-based pricing is a method that uses the buyer&rsquo;s demand, based on an estimate of a good&rsquo;s or service&rsquo;s perceived value to the buyer, as the central element in setting price. Pricing strategies are most important because they can have a disproportionate impact (positive and negative) on a company&rsquo;s bottom line. Managing prices has always been an activity of keen interest, but it has become even more so over the past decade as a result of the constrained pricing environment. Price and revenue optimization (PRO, or Optimized Pricing) is a business discipline used to effect demand-based pricing; it applies market segmentation techniques to achieve strategic objectives such as increased profitability or higher market share. PRO first came into wide use in the airline and hospitality industries in the 1980s as a way of maximizing returns from less flexible travelers (such as people on business trips) while minimizing the unsold inventory by selling incremental seats on flights or hotel room nights at discounted prices to more discretionary buyers (typically vacationers). Today, it is a well-developed part of any business strategy in the travel industry and increasingly used in others. Methodologies for determining Product Prices Cost-plus pricing Cost-plus pricing&nbsp;is a description of several types of pricing methods used by companies that require relatively little information. In cost-plus pricing, you calculate the cost of the product and then an additional amount is included to represent the profit percentage. &nbsp; The method uses the direct, indirect and fixed costs of the production of a product or service, although one of the failures of the model is that it includes all costs, whether related to the actual production, marketing or sale of the product or not. The method also requires that the firm select a desirable markup percentage, which will then be applied to costs. &nbsp; One equation used to come up with cost-plus prices is: P = (AVC +FC%)* (1+ MK%), where P = price, AVC = average variable cost, FC = percentage apportionment of fixed costs, and MK% = percentage markup. Variable costs are those that vary as the output level varies, and fixed costs are costs that do not vary with the level of output. Fixed costs include costs such as property, equipment and labor. AVC or average variable cost is variable cost divided by level of output. Although this method pretty much ignores demand and many market factors, it is extremely common by less sophisticated sellers for its ease to implement and understand. &nbsp; Yield Management&nbsp; Yield management&nbsp;is a business practice used to maximize revenue for resource-limited goods and services. &nbsp;Yield management is particularly suitable when selling perishable products, i. e. goods that become unsellable at a point in time (examples scenarios are seats for a baseball game, seats on a airline flight, hotel rooms, and tables at a restaurant). Because the seller has fixed costs (baseball stadium, the airline flight, hotel or restaurant has to stay open and keep staff paid for the entirety of their business hours), the more resources that get used, the better. &nbsp; Yield management has one specific goal: getting customers to buy a resource that might otherwise go empty at the highest possible price. A good example of Yield management&nbsp;is how movie theaters offer discounted prices for the first shows daily, as these times are less popular and they would rather make some money then have empty seats. Another good example is how baseball tickets may be raised when they are in high demand - such as when when Yankees play the Red Sox at Boston. The Red Sox home stadium (Fenway Park) is the smallest in the majors and has sold out every home game for the past 7 years, so tickets are in extremely high demand when they play the Yankees (their biggest rival). &nbsp; Dynamic Pricing Dynamic pricing is one method of price discrimination, which is the practice of charging different prices to different consumers for similar goods. This is part of the producer's intent to capture what economists call  consumer surplus --the difference between what a consumer is willing to pay for a good and the amount they actually have to pay. Economists refer to the price that a consumer is willing to pay as the  reservation price , and if producers could find out a way to calculate what a specific consumer's reservation price was for a good, they could charge the exact highest amount that the consumer would pay for the good before walking away, capturing all of the consumer surplus. However, as it is highly difficult for firms to judge individual consumers' reservation prices, price discrimination is more about separating consumers into groups than aiming at individual consumers. For retailers, the motivation behind dynamic pricing is a bit different than yield management. The object of their game is to get as many goods out the door and into customer's hands as possible, while making the maximum profit. I found a new laser printer on Amazon. com that I was set to buy (directly through Amazon, not from a seller in their marketplace). I had to take a break from the computer to feed my son (for under an hour). When I checked back to make the purchase, the price had gone up by 23%. Amazon uses dynamic pricing algorithms to measure a product's market and demand, adjusting their prices accordingly. Time-Based Pricing&nbsp; Time-based pricing refers to a type offer or contract by a provider of a service or supplier of a commodity, in which the price depends on the time when the service is provided or the commodity is delivered. The rational background of time-based pricing is expected or observed change of the supply and demand balance during time. Time-based pricing includes fixed time-of use rates for electricity and public transport, dynamic pricing reflecting current supply-demand situation or differentiated offers for delivery of a commodity depending on the date of delivery (futures contract). Most often time-based pricing refers to a specific practice of a supplier. Time-based pricing is the standard method of pricing in the tourist industry. Higher prices are charged during the peak season, or during special-event periods. In the off-season, hotels may charge only the operating costs of the establishment, whereas investments and any profit are gained during the high season. &nbsp;Another example is all the sales you see around Memorial and Veterans Day at major retailers. Identity Based Pricing Identity-Based Pricing is offering different prices to each customer. &nbsp;Online commerce sites are also experimenting with identification-based pricing, which prices items based on what is known about the customer, such as their buying history and browsing behavior. This can have both good and bad implications for the shopper. On the plus side, if a site recognizes that this is the umpteenth time the same customer has window shopped for a particular item, an algorithm may try a lower price, just for that customer, to see if that will close the sale. Newegg and Amazon will send offers to active buyers tailored to their buying habits, often with discounted prices. &nbsp; Auction Based Pricing Some verticals have moved to the auction model for pricing, and I am not just talking about selling stuff on eBay. In this scenario, buyers are asked to submit offers (bids) and they get compared to offers of other potential buyers. &nbsp; When you want purchase advertising via Google's AdWords product, you actually have to supply a minimum bid you are willing to pay for each click. Your bid gets compared to that of other advertisers wishing interested in the same keywords, with the highest getting the preferred placements. Considerations for implementing Optimized Pricing Yield Management,&nbsp;Dynamic Pricing,&nbsp;Time-Based Pricing, and Identity-Based Pricing are all methods for Optimized Pricing, where sellers can use data analysis and algorithms to regularly find optimal price points. Segmentation Strategy Another way to extend these methodologies is segment one's product line, offering multiple products and price points. Optimization can help sellers adjust prices and to allocate capacity among market segments to maximize expected revenues. This can be done based on different Key Performance Indicators: by goods (such as a seat on a flight or a seat at an opera production)by group of goods (such as all the seats to a ball game against contending rivals)by market (such as sales from Seattle and Minneapolis for a flight going Seattle-Minneapolis-Boston)overall (on all the routes an airline flies, or all the seats during an opera production season)In this way, PRO's overall aim is to provide an optimal mix of goods at a variety of price points at different points in time or for different baskets of features. The system will try to maintain a distribution of purchases over time that is balanced as well as high. Good PRO maximizes (or at least significantly increases) revenue production for the same number of units, by taking advantage of the forecast of high demand/low demand periods, effectively shifting demand from high demand periods to low demand periods and by charging a premium for late bookings. While PRO systems tend to generate higher revenues, the revenue streams tends to arrive later in the booking horizon as more capacity is held for late sale at premium prices. By doing this, they have actually increased demand by selectively introducing many more price points, as they learn about and react to the diversity of interests and purchase drivers of their customers. When to Re-Price The key to much business success is to understand the competitive environment you are in and react accordingly. Accordingly, a good time to consider price changes is when opportunity for revenue maximization is seen/discovered. At any time, we have 3 choices: prices can go up, prices can go down, or prices can stay the same. Dropping Prices - Sellers faced with lack of pricing power sometimes turn to PRO as a last resort. After a year or two using PRO, many of them are surprised to discover they have actually lowered prices for the majority of their opera seats or hotel rooms or other products. That is, they offer far higher discounts more frequently for off-peak times, while raising prices only marginally for peak times, resulting in higher revenue overall. &nbsp;Raising Prices - But if we&lsquo;re willing to lower our prices when demand is low, we should be willing to raise them when demand is high. I have seen conferences and &nbsp;presenters who were delighted to sell out completely months in advance. Form a revenue perspective, selling out isn&lsquo;t good news. If all the tickets sell out months in advance, it tells me the price was too low. &nbsp;No Change to Prices - Often the right strategy is to keep prices where they are. It certainly simplifies the entire sales process for sales people and for buyers, and may avoid logistical complexity. So how do we go about capturing the value of ALL the tickets? One way is the method an extremely iterative approach. This allows tickets to be assigned to prices as they sell. The drawback? It&lsquo;s fairly labor intensive to set up and manage, and assumes a very flexible logistics and point of sales systems. Here is an example: Build the seating projections&nbsp;for each season, reserving rows/sections of each&nbsp;price off-sale. After a period of&nbsp;sales reveals trends in demand, these rows&nbsp;are priced according to response, either as&nbsp;part of the lower-priced seating block or&nbsp;as part of the adjacent higher-priced&nbsp;section, depending on which price band is&nbsp;selling faster. &nbsp; Alternatively, you can build sales curves or charts of sales for comparable performances. Taking a historical look at the speed of ticket sales in the run-up to events will generally reveal patterns. You can set up the charts to record ―T minus x‖ days (or the number of days in advance of the event) and set T to 100%. Overlay several comparable performances and you will begin to understand how your current sales compare. The advantage to this method? We have seen over-performing example events at 30% or less of the stadium sold, and been able to ratchet prices up to match the curves to capture extra revenue on most of the tickets sold. It can also revealed under-performing events long in advance of a crisis point, which is also useful. Patterns &amp; Observations In the change to dynamic pricing, we often see customers treated as the people who should be the last to know. Sellers will try hide/obfuscate their prices until the time of the purchasing decision. The practices (and results) of five typical US companies that currently use dynamic pricing are illustrated below. Their schemes have several things in common: These companies don&lsquo;t publish prices (other than on their websites, which are constantly updated)They rarely raise the lowest available priceThey report few or no patron complaintsThey have weekly reviews of sales and pricingThey report total season revenue increases of up to 4%AND they differ in a number of ways: Some companies apply small price changes broadly across many performances, while others impose quite large changes infrequentlySome organisations spend just an hour or two a week reviewing pricing, while others have staff managing prices nearly full-timeSome adjust prices for specific seats or sections, others for all unsold seats. We see PRO clearly dominates static pricing&nbsp;when consumers do not behave strategically. This fact means that the presence of naive&nbsp;buyers in our marketplace will only increase our revenues when we price dynamically. So what&nbsp;can take from this analysis?&nbsp; When your supply is higher than the demand, post one set of prices and stick with them&nbsp;or advertise your price changes from the outset. With the right prices to begin with, you&rsquo;ll&nbsp;maximize revenue. &nbsp;When demand is higher than supply, dynamic pricing will maximize your revenue. &nbsp;The presence of non-strategic buyers will increase sales at higher prices. You&nbsp;may find that over time, some patrons wise up and act early to save money. This will,&nbsp;however, not stop others from paying more. &nbsp;Example for Pricing Optimization A good example to further analyze is tickets for a sporting event on a specific date. Revenues from the event are based on the&nbsp;finite number of tickets that can be sold. For years, ticket prices were static, only varying by seat location. They were a set price set months in advance. Often a small discount is offered if tickets are bought in quantity or well in advance. This looks like a classic opportunity for yield management, An obvious option would be for organizers (sports teams) to look at the supply and demand for tickets, reacting accordingly. &nbsp;Imposing price increases when a set percentage of the tickets are sold can result in some extra revenue. If there is genuine excess demand (economist-speak for buyer enthusiasm) for a product or event, you&lsquo;ve let 80% of your tickets go at a too low a price. You could have sold all (or most) of those tickets at the eventual higher price. An&nbsp;article from ESPN&nbsp;last year shows that some Major League baseball teams are in fact using PRO for setting ticket prices. &nbsp; Proceed with Care In the ESPN article, the author writes the following excerpt: But the other reality is that a team not only wants to make as much as it can on that game, but it wants to foster a long-term relationship with its fans. &nbsp; Dynamic pricing can&rsquo;t all be one way -- up. &nbsp; Prices need to be adjusted down, too, offering bargains to fans. &nbsp; &ldquo;Teams are very much aware that the fans are driving the team,&rdquo; Eglen says. &ldquo;They don&rsquo;t want to engage in gouging every last dime they can get out of the fans. They want a long-term positive relationship. &rdquo;&nbsp; If you consistently discount often, custiomers will become conditioned to expect these discounts. It's important to CONSIDER THE LONG TERM RELATIONSHIP. . . . Don't gouge and piss off your customers. Some More Considerations There are quite a few factors that may not be obvious, but can definitely influence pricing strategies: There is&nbsp;cost of holding inventory. &nbsp;If you are selling a physical product, you likely need to consider the logistics associated with&nbsp;storing the inventory and delivering the item. If it is a seasonal item, does it make more sense to heavily discount than to store the item for an entire year. You'll often holiday items go on sale after the holiday passes and snow shovels discounted after the winter. &nbsp;Accounting methods can influence pricing. The method a particular uses to value inventory can drive price. If you got a great deal on buying additional inventory of a specific product, companies using First-In-First-Out (FIFO) will likely recognize their cost structure differently than companies using Last-In-First-Out (LIFO). Product Segmentation can cannibalize other segments or selling opportunities. Companies need to make a strong case for customers to buy something other than the lowest priced offering. The value add must be clearly apparent and actionable. Conclusions We live in an information economy, where information can be the most valuable asset your company has in the quest revenue and efficiency maximization, With that said, optimized pricing really makes sense for a seller in many ways. It really amazes me that the vast majority of sellers in this country do have the knowledge/resources to take advantage of PRO and take the Cost-Based Pricing approach. A new kind of Marketplace fully based on PRO The popularity of eBay auctions have been a really amazing phenomenom to watch. The bidding process naturally pushes for a dynamically determined price by the auctions closing time. What would be amazing to see is dynamic environment of eBay, but without a set end date. &nbsp;It would make shopping for commodity goods much like investing in stocks and trying to gauge financial markets. Earlier I spoke about the way the printer I was interested in buying from Amazon changed prices in a short period of time. &nbsp;While some items in storefronts (generally considered fixed price marketplaces) may sometimes shift, they are largely consistent. Imagine if a marketplace (the size of Amazon or eBay) continually shifted with a PRO approach, it would be an incredibly interesting and fun to shop as you would only be able to speculate whether the price at that point in time was a good value. &nbsp; There are many examples where applying gaming strategies to various consumer experiences has worked to improve user interest and spending. I think gamifying the shopping experience based on PRO would be really cool and could be a successful business differentiator if executed correctly. "
    }, {
    "id": 54,
    "url": "http://localhost:4000/red_or_blue_ocean/",
    "title": "Are You Swimming in a Red or Blue Ocean?",
    "body": "2012/09/06 - I am quite fascinated how people/companies are able to innovate, make the most of their opportunities, and disrupt the established norms. It amazes me how some companies can tackle a small nuance differently than their competitors and win market share. &nbsp;I remember reading an interesting quote that stuck with me: play a different game on the same field as the competition. Last year a colleague introduced me to the concept of a Blue Ocean after hearing about it in as part of his MBA program (made popular by the book&nbsp;Blue Ocean Strategy). Since then I have spent quite a few hours in dicussions about them with various friends. In the book, there is the metaphor of red and blue oceans describes the market universe. Here's how the authors define them: Red Oceans&nbsp;are all the industries in existence today &ndash; the known market space. In the red oceans, industry boundaries are defined and accepted, and the competitive rules of the game are known. Here companies try to outperform their rivals to grab a greater share of product or service demand. As the market space gets crowded, prospects for profits and growth are reduced. Products become commodities or niche, and cutthroat competition turns the ocean bloody. Hence, the term red oceans. Blue Oceans, in contrast, denote all the industries not in existence today &ndash; the unknown market space, untainted by competition. In blue oceans, demand is created rather than fought over. There is ample opportunity for growth that is both profitable and rapid. In blue oceans, competition is irrelevant because the rules of the game are waiting to be set. Blue ocean is an analogy to describe the wider, deeper potential of market space that is not yet explored. The cornerstone of Blue Ocean Strategy is 'Value Innovation'. A blue ocean is created when a company achieves value innovation that creates value simultaneously for both the buyer and the company. The innovation (in product, service, or delivery) must raise and create value for the market, while simultaneously reducing or eliminating features or services that are less valued by the current or future market. The authors criticize Michael Porter's idea that successful businesses are either low-cost providers or niche-players. Instead, they propose finding value that crosses conventional market segmentation and offering value and lower cost. &nbsp; The strategy for creating a Blue Ocean It's really hard to come up with a financially viable idea that is new ground and execute on implementing it well. Let's face it, most business fail miserably. The red ocean seems to be the more logical starting point for most scenarios, as there is less risk and the market is better understood. It's a lot easier to download some canned ecommerce software and start selling commodity products than to become a manufacturer of a brand new product line. The book's author writes:  The only way to beat the competition is to stop trying to beat the competition. In red oceans, the industry boundaries are defined and accepted, and the competitive rules of the game are known. In blue oceans, competition is irrelevant because the rules of the game are waiting to be set. . . . The companies caught in the red ocean followed a conventional approach, racing to beat the competition by building a defensible position within the existing industry order. The creators of blue oceans, surprisingly, didn't use the competition as their benchmark. . . . Instead of focusing on beating the competition, they focus on making the competition irrelevant by creating a leap in value for buyers and your company, thereby opening up new and uncontested market space. &hellip;Value innovation is based on the view that market boundaries and industry structure are not 'given' and can be reconstructed by the actions and beliefs of industry players. &hellip;To fundamentally shift the strategy canvas of an industry, you must begin by reorienting your strategic focus from competitors to alternatives, and from customers to non-customers of an industry. As you shift your strategic focus from current competition to alternatives and non-consumers, you gain insight into how to redefine the problem the industry focuses on and thereby reconstruct buyer value elements that reside across industry boundaries &nbsp; One of the key tools presented in the book is the Strategy Canvas (shown below). The basic idea is to choose business factors (Key Performance Indicators) that you can heavily focus on to create a new type of offering. &nbsp; &nbsp; In 2003, Friendster was the hottest thing imaginable in social networking and looked they would be pretty much insurmontable (because it was pretty much the first social network and were growing exponentially). There was a huge explosion of competing social networks in the next year or so, including one I built and ran (Hipstir. com). Facebook seemed to market properly at the right time to the right demographic and blew everyone away. &nbsp; What was so brilliant about Facebook's story is the way they concentrated key factors (such as clean design and limiting membership to only students at certain colleges) that set their offering apart from others. If you look at competitive landscape today, they have such incredible reach that there really is no competition for them. Blue Ocean - Go Big or Go Home I look at Blue Oceans as classic high risk, high reward scenarios. Sure you may fail (as most businesses do), but you have the chance to hit a big time home run by defining a brand new market with no real competition. &nbsp; The image below shows how a blue ocean product/service has the potential to reach far greater audience than the tiny red ocean it almost completely covers up. &nbsp; eBay is another great Blue Ocean success story, as they disrupted the selling landscape in the late 90's by making it easy for anyone to buy and sell in auction format. Traditional auction houses make their revenues from charging the buyers premiums (fees) on top of the purchase price. eBay changed it up by making buying free and charging smaller fees only to the seller using their service as a distribution channel. &nbsp; eBay has become the defacto marketplace for a great many product types. Their reach exceeds pretty much all other auction, stores, and marketplaces (the only possible exceptions may be the huge marketing budgets of amazon and walmart). Making your Ocean go from Red to Blue It's pretty common that your first idea is not necessarily your best. As you iteratively learn about your audience and gather feedback, you have ability to make adjustments. This has been popularized by Eric Reis's Lean Startup movement and by agile development methodologies. &nbsp; When I decided to create&nbsp;SportsCardDatabase. com, the inital goal was to create the best engine for measuring market value for sports cards. While I firmly believed my methodology for determining values was superior to what I had seen from the established competitors, I wasn't really offering something radically different. As I began looking at all of the data I had collected for my sports card valuation algorithim, I realized that I wanted more (as a user of the service). Not only did I want to know the value for a sku, but I wanted to know where that sku was being sold and where I could buy it below that market value. In fact,&nbsp;I never wanted miss out on a deal again. I went on to add comparison shopping and deal finder functionality (features that no one else in this niche have). These new features have become more popular than the original idea. I have been so inspired by the site's success and learned so much from it, that I have some new&nbsp; unique functionality developed that I am planning to introduce. These items will&nbsp;further move SportsCardDatabase into a market of its own for helping sport card collectors: ways to quantitatively measure demand for a sku vs. others in our catalogvolatility measures and arbitrage functionality&nbsp;"
    }, {
    "id": 55,
    "url": "http://localhost:4000/routine_vs_commitment/",
    "title": "There’s a big difference between routine and commitment",
    "body": "2012/09/05 - I am a very big fan of the New England Patriots. In the early 1990's, they were terrible (going 1-15 twice in 3 years). Then Bill Parcells was hired as the coach and turned around the fortunes of the team. In 1995, he let go of the team's best running back Marion Butts and used a third round draft pick on Curtis Martin from Pittsburgh. Martin immediately became his starter, went on to have 3 really great years in New England (including a Super Bowl run in 1997), and then followed Parcells to play for the Jets. I was watching&nbsp;Curtis Martin's Hall of Fame induction speech&nbsp;(which is the most heart felt speech I have seen from an athlete), and there was a story that stood out to me. Martin was always known as one of the hard working players anyone will ever meet and he always got the most out of his talent. He would outwork other players, the coaches, the trainers - that's just who he was. He tells a story how Parcells called him over during a practice and told him: &ldquo;There&rsquo;s a big difference between routine and commitment. &rdquo; &nbsp; Parcells was known for being able to push his players and get them to play to their potential. Martin went on to say that this comment got him to raise his workouts to a whole new level and become a better player. This got me thinking about my own self, and about the way I have approached some of the things I have been working. Are they part of my routine, or am I committed to something?&nbsp; Let's define both terms: Routine&nbsp;can be defined as&nbsp;customary or regular course of procedure. &nbsp; Commitment&nbsp;can be defined as the state or quality of being dedicated to a cause, activity, etc. Routines Having a routine is not a bad thing at all. I love having predictable outcomes. If no one established routines, our lives would be completely inconsistent and chaotic. Routines are all about consistently knowing the outcome, allow us to establish&nbsp;baselines for our&nbsp;expectations. I definitely appreciate many routines in my life - such as having Monday-Friday work week, getting paid on the same interval, knowing what days garbage gets collected, knowing the day my mortgage bill is due, etc. Being someone who creates software, my days are often about defining and creating routines. If I want a function in my script/program (a. k. a. a software routine) to accomplish XYZ, then I will always expect an established set of inputs. Developers need to be completely sure that function will work as expected, that we often automate series of tests to validate the functions work as expected (unit and integration tests). Once a routine is set, how often do we revisit it and assess if it optimally accomplishing its goals. It's really easy to be happy with having a predictable outcome and moving on to solve the next problem. We know there may be room to refactor the routine to make it better or more efficient, but it is worth the effort? &nbsp;Is it something important enough to you to want to make improvement? Are you committed enough to achieving the best possible outcome that you will dedicate the time/energy/resources to making the improvements? I really think everyone should review the routines related to the more important things in their life periodically, especially creative people and&nbsp;entrepreneurs. It can really make you think about why you wake up every day and what you are working towards. Are you actions just  turning the crank  to make the business go, or are you really trying to make the best use of your time to meet your goals?&nbsp;Commitments I saw an interesting&nbsp;post&nbsp;related to 2 distinct types of commitments: To me, there are two kinds of commitments, in the general sense. One kind is the kind where you are merely fulfilling obligations, promises. You enter it knowing full well what is your role and what are your responsibilities, and you will see that you fulfill those tasks set upon you. Whether you try to look at those obligations positively or try to execute it the most fun way possible depends on your approach. The other kind of commitment is the one you enter into willingly, voluntarily, totally. You enter it with hopes and determination to make your commitment enjoyable and satisfying. Of course there will be rough patches, but those are just phases and you know that can be overcome, because in this commitment there isn&rsquo;t just will, there&rsquo;s also passion. And even though there might be better offers coming along, you will stay true to your commitment because you know that even though the grass seems greener on the other side, what you have committed to is still incomparable, despite whatever shortcomings or flaws it has. The first category of commitment are the things you have to do, whether you like it or not. You may not really care about how you get them done, but you know you have get them done. We'll often build routines around them and forget about them - turning the crank. Paying my bills is a good example of a commitment I have that I know just needs to get done, &nbsp;but I don't want to focus much attention on once I have the autopay set up to take care of it. The second type of commitment is the one I like to focus on, the ones we willingly take on and are often passionate about. These are the ones we want to do well. I see these fall into&nbsp;3 larger categories that I see people tend to be more committed to: work, job, or businessfriends and familypersonal interestsParcells challenged Curits Martin about his commitment for something he loved, training for his football career. Since Martin loved it, he was willing to look for ways to optimize his workouts and optimize his routine. He found ways to continually get better, and he led the NFL in rushing yards gained in his final season. Personal interests is another really good example, as people seem to put in a huge amount of time on them. For over a decade I have run a subscription based community for people who collect sport autographs (SportsCollectors. Net). It's amazing how passionate the folks on the site are, how much they enjoy working on the projects and enjoy their hobby. It's pretty common for such collectors to establish a routine of searching a list of sites for the items they would like within a target price range (and you could even automate this if you desired efficiency). Then there are some who are so passionate (including me) that they take time off and travel distances hoping to find items they'd like to add to their collection. There are times when collectors desire the item so much that will far exceed their targeted price point to acquire it. These are examples of commitment, as these collectors are willing to break their routine to accomplish their goals. &nbsp; Measuring the Cost of Commitment I always look to for a way to measure things quantitatively as I try to understand them. I have searched quite a bit out there looking to find a find an established formula to measure one's commitment and how it relates to routines, but my search has come up empty. Commitment is a very subjective concept that varies so much by the person and situation. Shannon Sharpe (another Hall of Famer) started his induction speech apologizing to his children for all the important things he missed while they were growing up in order to be the best football player he could be. He said he was sorry, but he would not change it. He was willing to sacrifice certain family and personal related things in order to make his career successful. The underlying theme I am finding is that one's level of commitment has a relationship to the extent the person is willing to sacrifice some other things. &nbsp;In economics, an&nbsp;Opportunity Cost&nbsp;is the benefit that could have been gained from an alternative use of the same resource. So let's look at a practical example. I want to upgrade the Private Messaging feature on SportsCollectors. Net that I originally coded in 2005. I have a good idea of some things I'd like to enhance - performance, UI, search, and tagging functionality. In order to take on this work, I will need to find time competing with other things in my life such as: personal timeother functionality on this projectother projectssleep&nbsp;So I can try to quantify what the opportunity cost is if I go forward with this work by: Estimating the time it would take to accomplish this worklist all the other things I could do with this timePrioritizing the list. Seeing the list helps me prioritize its importance compared to the alternatives. &nbsp;If I have strong commitment to this work item, then I will take it on and accept that I will have sacrifice other items on the list. Some Personal Reflection, Am I committed? I'd like to think that I am a somewhat organized person. I have defined my vision and the appropriate goals to get there. I think the execution of my plan to accomplish those goals is where we see if my actions are routine or a product of my commitment. Professionally, building my projects and going to my day job has become my routine. They are a comfort zone for me, knowing I can feel good about making progress. I somewhat look forward to my train commute where I have undistracted time to think (and blog) about what interests me. I've always felt that solving an interesting problem is more addictive than any other stimulant. &nbsp;I get so much satisfaction knowing that others enjoy using my projects and find them to be valuable that it makes me want to make them better. &nbsp;The place I see my commitment is when I see a scenario for learning something interesting &nbsp;or the opportunity to make a significant improvement. When I really want a piece of functionality or explanation on an interesting subject and can't find it, that's where I see myself getting really excited to bridge the gap. &nbsp; "
    }, {
    "id": 56,
    "url": "http://localhost:4000/making-big-decisions/",
    "title": "Framework for making Big Decisions",
    "body": "2012/08/24 - Choosing a fulfilling career path is such a difficult task, and I see many people have no idea where to even start (including myself). &nbsp;I routinely find myself asking my friends and coworkers  what do you want to do long term in your career  or  what do you want to be when you grow up . I seem to hear the following responses repeatedly: I am OK with what I am doing now (most of the time this is not actually true)I don't know / I am not really sureI want to get to the next level (be promoted to a manager) of what I am currently doingI'd like to or I am going to school and the degree is what is stopping me from getting to that next levelI have an idea for starting a company or building a new product, but I don't know how to startI'd like to start a company, but I don't have an idea. &nbsp;My time working for DELL instilled me the process of defining the problem statement, defining goals needed to solve the problem, defining&nbsp;S-M-A-R-T objectives&nbsp;to meet the goals. This has been a process I have used as a manager and seen it work with decent effectiveness in certain situations. It is certainly better than not having a structured approach to determine a career path. Last year I randomly stumbled on a post on Hacker News with the attention getting title  How I made a Principled decision to quit my Six Figure job  posted by&nbsp;Tawheed Kader (TK). I was not so impressed that someone was leaving a stable&nbsp;Hedge Fund&nbsp;job to chase the startup dream, but I was really interested how he detailed his decision making process. He wrote the following about the indecision he faced, which I think a good many people also experience: You see, in my gut I knew, I knew that what I really wanted was something of my own. Something more cutting edge than Enterprise Products in a Financial Services Firm, and most importantly, something that had the potential to change the world, but that alone wasn&rsquo;t enough for me to just quit my job and give up everything I had been working toward. I needed more. And even in needing more, there seemed like so many possibilities, that sense of &ldquo;obvious next step&rdquo; just wasn&rsquo;t there. Do I keep working and grow my career? Do I quit and join a startup? Do I apply to YCombinator? Do I bootstrap? Do I do consulting? There were just so many factors, so many possible paths!&nbsp;I didn&rsquo;t want just soft soul-searching, I wanted a raw and hard analytical assessment of what I wanted out of life and how I could go about attaining it. TK came up with a really interesting framework asking someone to answer 5 questions. It works really well when you write out the questions and answers on a big whiteboard&nbsp;(as TK did below),&nbsp;as opposed to thinking about them or putting them on paper.  The Framework Step 1 &mdash; What are my values? This about defining  Who am I? what are my values?&rdquo; All constraints, circumstances and situations barred, what&rsquo;re the things that make you tick? Step 2 &mdash; What do I want my life to be about? Armed with a set of values that deeply resonated with with who you are, then you can venture off to answer the question &ldquo;What do I want?&rdquo; &mdash; more concretely, given your values and the things that are important to you, what do I want to be doing on this planet? I like to equate Step 1 and 2 to  Defining the Problem . Understanding what is important to you and the high level of what you want. Step 3 &mdash; Elaborate on what you want and value This is where you set some kind of goals to align your values and what you want your life to be about. This could be statements like - I want to be happy, I want to learn XYZ, I want to be well compensated, I want independence or autonomy, I want to spend time with my friends, I want to build something cool, etc. It may be a good idea to further elaborate on these statements to clarify their meaning. If it's something that you truly value or want, you should be able to come up with a clear definition of what the statement means to you. Step 4 &mdash; Compare/Contrast between your desired reality and your current So, here's where you do a quick Gap Analysis - you'll need to compare your desires/goals to your reality. It&rsquo;s up to you how much you choose to write down for this step, or how much you want to visualize. For me, it was more about looking at my current reality and visualizing what the reality that I wrote out in my picture would look like and comparing and contrasting between the two. Step 5 &mdash; NOW, devise your plan Now that you&rsquo;ve got a set of things that you want, a visualization of your current reality and an idea of the difference between the two, It's time for you to set real objectives and plan around achieving them. The idea is to put together a plan that you will likely be accomplish and measure that the end state is satisfactory. I am a big believer in setting S-M-A-R-T objectives. &nbsp;SMART stands for SIMPLE, MEASURABLE, ATTAINABLE, REALISTIC, TIMELY. For each realistic goal that you set, you should be able to define a concrete set of tasks that allows you to meet that goal. &nbsp; The Framework in Action So I have gone through the exercise of answering the questions of the framework for myself and I have moderated similar sessions for some of my friends. It forces you to be honest with yourself and clearly define what is most important to you - not necessarily what is expected of you by family/friends/employer. If being creative is really important to you, than write it down and build a plan to get yourself there. It usually takes about 1-2 hours to fully work through all the questions. Everyone I persuaded to try it has had positive feedback about it. One of my friends decided to change professions after experiencing this framework. Another decided to stop procrastinating, and start working on startup idea he has been putting off. Personally, it has helped me quite a bit. It opened me up to consider things I had previously de-prioritized. It has validated that some of the things I was doing were in support of my values/goals. I decided to launch this blog site as a direct response to one of the objectives I set. "
    }, {
    "id": 57,
    "url": "http://localhost:4000/building_valuation_platform/",
    "title": "Methodology for Building a Product Valuation Platform",
    "body": "2012/08/23 - As I have mentioned in previous posts, understanding pricing and market values has been a passion of mine for over 20 years. With many problems I encounter (especially those related to business or technology), I try to follow these steps borrowed from agile software development methodology: DEFINE - Define the problem/goals, identifying clear and measurable requirements. &nbsp;ANALYZE - Analyze the each requirement to identify or design potential solutions. &nbsp;IMPLEMENT - Establish/Implement tested and repeatable processes to accommodate each of the requirements. &nbsp;AUTOMATE - Implement in a process/method to regularly look for opportunities for optimization and to identify new important requirements. So I tried to follow these steps to find a way to determine market values for products. Define the Problem The initial goals (requirements) of the platform were: Ability to determine an accurate market value for any product (card) in my database. Value determination process must be repeatable/consistent and the platform be able to eventually support millions of products in the targeted product set. Since I was specifically interested in determining the values for sports cards, I focused on that product set. &nbsp;Analysis I like to tackle each requirement one at time (when possible), before moving on to the next one. The first one is obviously the most important one, so I needed to come up with a way to determine value for a given sports card record. Up to that time, I had accumulated a fairly significant amount of transaction data related to sports card sales across many venues. I knew looking at that data was going to be an excellent opportunity for learning. &nbsp;&nbsp; I wanted to create a standard algorithm to consume listing/transaction data and return a market value. &nbsp;For each product, I quickly was able to calculate averages and medians of recent sale prices and for the asking prices of active listings. I noticed some pretty huge variance depending on the time I was querying for the same product, and that didn't seem like it would be a reliable market value - certainly not usable for making purchasing decisions. &nbsp; So I needed to look deeper Key Performance Indicators (KPI) is a set of quantifiable measures used to gauge or compare performance in terms of meeting their strategic and operational goals. KPIs vary between companies and industries, depending on their priorities or performance criteria. I decided I would try to identify a collection of such attributes of sales transactions and the product that would effect market value. Some of the initial items I came up with were: selling format of listing - fixed price, auction, reverse auction&nbsp;marketplace/site where the item is being sold &nbsp;condition of card&nbsp;shipping chargestime of day/week/month/year when item is offered and when sale/auction/listing completedstarting price for the listing (for auctions, reverse auctions)length of time the item is offered before sellingstandard deviation of sales and price elasticitynumber of listings available for product over time period (week, month, 3 months, 6 months, year)number of sales for product over time period (week, month, 3 months, 6 months, year)sales values for product over time (week, month, 3 months, 6 months, year)&nbsp;the age the item was producedif the product was autographedif the product contained game worn memoribiliaif the product &nbsp;had a limited print runcalculated demand score for the itemMy plan was to come up with a method for weighing the importance of each of the KPI's in relation to one another, in order to establish an optimized algorithm. &nbsp; In data mining, there are a few different options we could choose for determining relative importance of KPI's. I chose to construct an Artificial Neural Network and loaded several hundred representative random samples to determine the weighting for each KPI. I also created test cases that could be run after each execution of the valuation algorithm. The test cases validate that the market values were being calculated correctly. I added some exception conditions (such as thresholds for high standard deviations or huge spikes in listed items) that would be recorded and later analyzed. Solution Implementation The purpose of this post is to share the methodology of how I went about solving the problem at a high level. I am not going to get into the details of my algorithms, the data I am using, or the code that I wrote. Since there are many different ways to accomplish these items (many programming languages, operating systems, data integration methods, and databases to persist the data), I'll let others have the fun figuring out them like I did. Once the data structure and factor weightings were determined, I built an application to query for the necessary inputs, calculate the market value, and save the result. I then set up an additional task to run this application for every product in the SportsCardDatabase product catalog. I also implemented a testing framework that allowed me to validate each calculated market value (Test Driven Development). Automation Since there are hundreds of thousands of products in our database, there was no way I could possibly execute my process for each one. I set up automated tasks that would query the list of products, generate calculated values for each, validate the values, and save the valid ones. I then added functionality to monitor the system's health, compile aggregate statistics for the data processed, &nbsp;and automatically send alerts when there were issues. In order to understand if there are market changes that would effect the weightings of the KPI's, I've set up a process that randomly picks products with adequate samples and runs them through the artificial neural network. When weightings are returned exceeding certain thresholds, I am alerted so I can further investigate. Conclusions So far the results have been really good. I have been able to personally use the information for arbitrage scenarios (to be discussed in a future post). Once I launched&nbsp;SportsCardDatabase, I have received mostly positive feedback about what the site offers. I realize that this is likely a very different approach compared to other companies/groups that offer guidance on the market values for sports cards. The values on&nbsp;SportsCardDatabase&nbsp;are lower because they are meant to represent the market values and not retail values (such as Beckett, TuffStuff, etc. ). There are some newer sites (much like eBay searches) that simply list sale prices for individual cards (vintagecardprices, cardpricer, valuemycards etc. ), and I discussed the limitation of this approach yesterday at http://www. jaygrossman. com/market_values_from_ebay/. My algorithm is a continual work in progress. Admittedly, I'd like to dedicate some time looking to better accommodate non commodity products (low print runs, rarities) and products with limited listing/sales data to analyze. For these types, I have seen some unexpected abnormalities that I have to think were outliers being considered too heavily because of small populations. &nbsp; Future of the Platform I'm really excited about this. This same type of process could be used to determine market value for other commodity product sets. It would require a well defined catalog of products, well defined KPi's for each product, and access to the sales/listing data. Comic books, stamps, coins, electronics, car parts, who knows what could be next. . . but it could be fun to try! "
    }, {
    "id": 58,
    "url": "http://localhost:4000/market_values_from_ebay/",
    "title": "Is Searching eBay a reliable way to determine Market Value?",
    "body": "2012/08/22 - Disclaimer: I am a huge fan of eBay and the significant disruption they brought about for buyers and sellers. I have been a frequent buyer and occasional seller on the site since 1998 and continue to use it every day. Nothing in this post is meant to put eBay (the company or marketplace) in a negative light. I have seen a good many people base their selling and purchasing decisions solely on data they can find from&nbsp;the closed listings from the past 60 days for a particular search on eBay. I have heard them refer to this as  Market Value . &nbsp;While I think this is extremely valuable and easily accessible information, I am not sure it is a complete and accurate picture for market value. Background Info and Definitions First, let's establish definitions for some common terminology: Market&nbsp;is the opportunity to buy or sell, extent of demand for merchandise. &nbsp;More simply, a market represents the total population of available offerings for a given commodity (product/service) or security. Marketplace&nbsp;is a location where goods and services are exchanged. &nbsp; Market Value&nbsp;(also known as Fair Market Value) refers to the price that a seller of&nbsp;commodity (product/service) or security&nbsp;can expect to receive from a buyer in a fair and open negotiation. &nbsp; Standard Deviation&nbsp;if the statistical measure for&nbsp;how much variation or  dispersion  exists from the average (mean, or expected value). A low standard deviation indicates that the data points tend to be very close to the mean, whereas high standard deviation indicates that the data points are spread out over a large range of values. &nbsp;In addition to expressing the variability of a population, standard deviation is commonly used to measure confidence in statistical conclusions. Confidence&nbsp;is used to indicate the reliability of an estimate. &nbsp;It is an observed interval (i. e. it is calculated from the observations), in principle different from sample to sample, that frequently includes the parameter of interest, if the experiment is repeated. &nbsp; Searching for current items available and completed listings on eBay eBay is a marketplace for buying and selling various items. In fact, their tagline was once  the world's largest marketplace . To best illustrate how to search ebay I will use an example, gathering information on a commodity item - an ungraded 1960 Topps Sandy Koufax baseball card. &nbsp; On ebay. com, we can search for the keyowrds:1960 Topps #343 Sandy Koufax -psa -bgs -sgc -bccg -bvg -jsa -reprint -rp The search would currently return 46 total results. 10 are in auction format (1 with the buy now option) and 36 in fixed price format. The average price right now is $79. 87, but we should expect that average price to rise for this set of listings as the auctions will likely receive more bids. If you are new to eBay, you may be interested to know that it is easy for any registered user to view the results of recently ended listings. &nbsp;No doubt that this is fantastic information and should definitely be considered, and many of us are grateful they make it freely available to us now (I believe they charged for historical sales data at one point through a partner). The search results for completed listings of that same search term (pictured below) show there were 70 total items: &nbsp; Out of those 70 completed listings, 60 show they resulted in successful sales ranging between $1. 36 and $124. 50 for an average of $39. 09. 48 of those 60 sales were auction format for an average of $35. 71. Of the 10 items that did not sell, 7 were fixed price format and 3 were auction format with an average asking price of $61. 50. When we look the data from only the past 2 weeks, we see that 22 closed but only12 resulted in a sales. This is because the results screen will only display listings that did not sell from the past 14 days. Most people probably don't realize this, and can easily make false assumptions about demand and various other market dynamics. In looking at the data from both searches, it is interesting to note that the majority of listings are fixed price (78%), but fixed price ones represent only 20% of the items that result in sales. This means that while the auction format is not the primary method for offering this product, it most often results in sales (although at a lower price point). &nbsp; Is eBay representative of the market? eBay is a single (very large) marketplace, but it does not constitute the entire market for selling commodity items. There are other individual sellers and marketplaces offering the same commodity goods that maybe we should consider. &nbsp; If I do searches on Google and Bing, I can find over 150 copies of the same ungraded 1960 Topps Sandy Koufax card for sale on sites other than eBay. Every one of them is being offered in a fixed price format and they have a 15% higher average price than we saw from the closed listings on eBay. &nbsp; Determinants of Market Value What about other factors relevant in determining Market Value that can not be easily evaluated by a few searches? Here are a few to consider: Demand - In the last post, we recognized the&nbsp;important relationship between pricing and demand. Are these 2 searches a good indication of demand for the product? Is there other information on eBay we can use to assess demand?&nbsp;Time - Since demand is relative to time and location, we should probably factor in the timing of the sales. Is the sale happening at an off hour or slower day of the week, and how should this information be considered?Seller - Do items offered by certain sellers consistently sell more often and for more money than others (due to marketing, customer service, reputation, location, etc. )?&nbsp;Price Elasticity of Demand - Do the sale figures in the completed listings tell us anything valuable about the item's elasticity? If so, how would we consider this in determining market value?Selling format - How does the selling format value impact value guidance? Should we consider items sold in fixed priced, auction, reverse auction formats differently. Location - How does the physical location (state or country of seller) or relative location (the site the product is advertised) influence price?Consumers - Is there something unique to the consumers on eBay that is different than the rest of the market? &nbsp;Weighting factors - Should all factors be evaluated evenly, or should be weighted more heavily? If weighting is uneven, how to determine the weighting?Differences between the findings from the eBay searches and the rest of the market&nbsp; Since we know eBay is not the entire market, we must ask ourselves if the sample we are seeing from eBay representative of the entire market data we see for this product?&nbsp;In our example (the 1960 Topps Sandy Koufax card), we saw the following differences between eBay and the overall market: eBay sales were made up of fixed price and auction formats, while the rest of the market was fixed price format. eBay sales figures had a higher standard deviation of sales figures than the rest of the market. &nbsp;eBay likely has greater inventory of many products than other sites, providing a better opportunity for competitive shopping in a single marketplace. Items from other stores/marketplaces likely will not have the exposure, competitive bidding, and variance&nbsp;due to limited choices. Assessing completed sales values may not be readily (freely) available as they are from eBay. Conclusions&nbsp; I believe eBay by itself can be an effective indicator for market value. How much will depend greatly on the particular product, its market, and its determinants. While a good indicator is certainly nice to have as a reference, I am not sure I would be comfortable basing purchasing decisions completely off of it. Here's why: I went through the exercise of measuring the effect of the above mentioned determinants along with quite a few sports card specific ones. After quite a bit of processing, I came up with an algorithm that has led me to effectively make informed purchasing decisions - leading to discounted purchases. These determinants may be as important as price - depending on the particular product and market. The fact that most items offered in the market are via fixed price and most sales on eBay take place via auction tells us that eBay may not be representative of the market. Selling format, in particular, is a key determinant. &nbsp;We may consider excluding or lowering the weight for the auction items from the&nbsp;closed listings, as this selling format likely has a large effect on buyer interest and sale price (especially auctions with low starting bids). If we look only at the fixed price closed listings in our example, we see the average price rise and move closer to that of other sites. By doing this, we reduce our sample size by 80% (from 60 to 12). We'll see that the fixed price closed listings are more similar, meaning they have a smaller standard deviation and would allow us to be more confident with our value estimate. &nbsp;&nbsp;Seeing a normalized distribution for sales is very desirable state when assessing a product's value. It implies that there aren't many statistical outliers being considered, which would skew the results. Since we saw a higher standard deviation, it means that eBay sales experience greater volatility (likely due to the auction format) and inspires lower confidence in the overall number. If we deem the factors such as demand, selling format, time, seller, etc. as statistically relevant to assessing the market value (which I would), it seems like it would be a challenge to properly consider these weighted factors easily by doing a few searches manually. Volatility is often sought by buyers, as it can lead to find exceptional buying opportunities. We will discuss arbitrage scenarios in a future post where buyers can exploit such volatility for profit. &nbsp;The example we saw on the Koufax card was actually pretty close to the market value that I independently calculated&nbsp; suggests for this card of $40. 85&nbsp;(on&nbsp;SportsCardDatabase)&nbsp; - the eBay average was about 12% lower. &nbsp;If you are a retailer and you consistently sell for 12% below the optimal value, your business is missing out on opportunity. &nbsp;eBay sales averages for commodity baseball cards from the 1960's are more often fall 18%-20% below the optimal market values. I have noticed other categories of commodity products consistently selling for 50% below market value and others selling 50% above market value. I created a platform to determine market value to fill my own needs, as I wanted to better understand market values for sports cards. It has worked out really well for me personally, as I have been able to find some items at significantly lower prices than I would paid without it. Combining data mining with automation has allowed me to build a solution that can fairly accurately assess the market values for millions of individual products on a daily basis - providing me with a much deeper and more accurate understanding I had when I would manually search on eBay. "
    }];

var idx = lunr(function () {
    this.ref('id')
    this.field('title')
    this.field('body')

    documents.forEach(function (doc) {
        this.add(doc)
    }, this)
});
function lunr_search(term) {
    document.getElementById('lunrsearchresults').innerHTML = '<ul></ul>';
    if(term) {
        document.getElementById('lunrsearchresults').innerHTML = "<p>Search results for '" + term + "'</p>" + document.getElementById('lunrsearchresults').innerHTML;
        //put results on the screen.
        var results = idx.search(term);
        if(results.length>0){
            //console.log(idx.search(term));
            //if results
            for (var i = 0; i < results.length; i++) {
                // more statements
                var ref = results[i]['ref'];
                var url = documents[ref]['url'];
                var title = documents[ref]['title'];
                var body = documents[ref]['body'].substring(0,160)+'...';
                document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML + "<li class='lunrsearchresult'><a href='" + url + "'><span class='title'>" + title + "</span><br /><span class='body'>"+ body +"</span><br /><span class='url'>"+ url +"</span></a></li>";
            }
        } else {
            document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = "<li class='lunrsearchresult'>No results found...</li>";
        }
    }
    return false;
}

function lunr_search(term) {
    $('#lunrsearchresults').show( 400 );
    $( "body" ).addClass( "modal-open" );
    
    document.getElementById('lunrsearchresults').innerHTML = '<div id="resultsmodal" class="modal fade show d-block"  tabindex="-1" role="dialog" aria-labelledby="resultsmodal"> <div class="modal-dialog shadow-lg" role="document"> <div class="modal-content"> <div class="modal-header" id="modtit"> <button type="button" class="close" id="btnx" data-dismiss="modal" aria-label="Close"> &times; </button> </div> <div class="modal-body"> <ul class="mb-0"> </ul>    </div> <div class="modal-footer"><button id="btnx" type="button" class="btn btn-danger btn-sm" data-dismiss="modal">Close</button></div></div> </div></div>';
    if(term) {
        document.getElementById('modtit').innerHTML = "<h5 class='modal-title'>Search results for '" + term + "'</h5>" + document.getElementById('modtit').innerHTML;
        //put results on the screen.
        var results = idx.search(term);
        if(results.length>0){
            //console.log(idx.search(term));
            //if results
            for (var i = 0; i < results.length; i++) {
                // more statements
                var ref = results[i]['ref'];
                var url = documents[ref]['url'];
                var title = documents[ref]['title'];
                var body = documents[ref]['body'].substring(0,160)+'...';
                document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML + "<li class='lunrsearchresult'><a href='" + url + "'><span class='title'>" + title + "</span><br /><small><span class='body'>"+ body +"</span><br /><span class='url'>"+ url +"</span></small></a></li>";
            }
        } else {
            document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = "<li class='lunrsearchresult'>Sorry, no results found. Close & try a different search!</li>";
        }
    }
    return false;
}
    
$(function() {
    $("#lunrsearchresults").on('click', '#btnx', function () {
        $('#lunrsearchresults').hide( 5 );
        $( "body" ).removeClass( "modal-open" );
    });
});