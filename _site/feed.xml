<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>JayGrossman.com</title>
    <description>Sharing My Fun with Software, Data, Sports, &amp; Collecting!</description>
    <link>http://localhost:4000/</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Sun, 22 Oct 2023 00:00:06 -0400</pubDate>
    <lastBuildDate>Sun, 22 Oct 2023 00:00:06 -0400</lastBuildDate>
    <generator>Jekyll v4.3.2</generator>
    
      <item>
        <title>New JayGrossman.com</title>
        <description>&lt;p&gt;Welcome to the new version of my blog!!! I migrated over 10+ years of content to a platform and a fresh (cleaner) look.&lt;/p&gt;

&lt;h3&gt;The Old Blog&lt;/h3&gt;

&lt;p&gt;In 2012, I was doing a lot more development in ASP.NET/C#, so I decided to move my blog to a platform that was more familiar to me. I set up my blog with  &lt;a href=&quot;https://blogengine.io/&quot; target=&quot;_blank&quot;&gt;blogengine.io&lt;/a&gt; that I self-hosted. At the time, it had some decent &lt;a href=&quot;https://blogengine.io/features/&quot; target=&quot;_blank&quot;&gt;features&lt;/a&gt; and I was able to modify a theme I liked (design shown below):&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/old_blog.png&quot; alt=&quot;&quot; style=&quot;border:1px solid black;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;There were some things I didn&apos;t like about my set up:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;It was a pain to set up and maintain.&lt;/li&gt;
&lt;li&gt;It had become slower.&lt;/li&gt;
&lt;li&gt;The WYSIWYG editor made some really ugly HTML.&lt;/li&gt;
&lt;li&gt;The widgets were not flexible and easy to customize.&lt;br /&gt;
(compiling old .NET to make changes was a pain).&lt;/li&gt;
&lt;li&gt;A white background would be better for reading content.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3&gt;Moving on to something more modern&lt;/h3&gt;

&lt;p&gt;I remembered that my friend &lt;a href=&quot;https://www.linkedin.com/in/collinmeyers/&quot; target=&quot;_blank&quot;&gt;Collin Meyers&lt;/a&gt; had mentioned that he &lt;a href=&quot;https://docs.github.com/en/pages/getting-started-with-github-pages/creating-a-github-pages-site&quot; target=&quot;_blank&quot;&gt;set up a blog hosted with Github Pages&lt;/a&gt;.  So I took a look and it looked pretty straight forward to set up.&lt;/p&gt;

&lt;p&gt;Github Pages natively supports the ruby based blog framework - &lt;a href=&quot;https://jekyllrb.com/&quot; target=&quot;_blank&quot;&gt;Jekyll&lt;/a&gt;. I had never used it before, but it was very easy to set up locally with rbenv, install the gems and get it working. There were some things I immediately liked:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;There were plenty of themes to choose from (free and paid).&lt;br /&gt;
I chose the &lt;a href=&quot;https://jekyllthemes.io/theme/mediumish&quot; target=&quot;_blank&quot;&gt;Mediumish&lt;/a&gt; because it was clean and simple.&lt;/li&gt;
&lt;li&gt;The scripting is easy to customize the theme.&lt;/li&gt;
&lt;li&gt;There are tons of examples on google and ChatGPT.&lt;/li&gt;
&lt;li&gt;I can use both HTML and markdown.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;There are many great tutorials online that will walk you through setting up a blog with Github Pages and Jekyll. I found this one to be the most helpful: &lt;a href=&quot;https://www.smashingmagazine.com/2014/08/build-blog-jekyll-github-pages/&quot; target=&quot;_blank&quot;&gt;Build A Blog With Jekyll And GitHub Pages&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Here is a nice video I watched by Spencer Pao:&lt;/p&gt;

&lt;p&gt;&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/g6AJ9qPPoyc?si=rq5dJJOkYtBAFR9Z&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;&lt;/p&gt;

&lt;p&gt;There were some gotchas in setting up a custom domain. Here is a helpful video from Kenny Yip:&lt;/p&gt;

&lt;p&gt;&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/rIXWUJ5U8bY?si=UZvbFq2r62IR_B-u&quot; title=&quot;YouTube video player&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;&lt;/p&gt;

&lt;p&gt;Since I migrated from a different platform to Jekyll, it was helpful to set up redirects so my old URLs resolve with my new site. This GEM was helpful to set up page level redirects:&lt;br /&gt;
&lt;a href=&quot;https://github.com/jekyll/jekyll-redirect-from&quot; target=&quot;_blank&quot;&gt;https://github.com/jekyll/jekyll-redirect-from&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The new platfrom will certaily make it easier for me to post new content. Now I just need to find time and motivation to add more to this blog consistently.&lt;/p&gt;

</description>
        <pubDate>Sat, 21 Oct 2023 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/new-jaygrossman.com/</link>
        <guid isPermaLink="true">http://localhost:4000/new-jaygrossman.com/</guid>
        
        
      </item>
    
      <item>
        <title>Creating Singer Target to Send Data to Web Endpoint</title>
        <description>&lt;table style=&quot;width: 100%; border-color:red;&quot; border=&quot;1&quot; cellpadding=&quot;5&quot;&gt;
&lt;tr&gt;
&lt;td&gt;
   &lt;strong&gt;&lt;i&gt;Please Note:&lt;/i&gt;&lt;/strong&gt;&lt;br /&gt;
   If you are new to Singer, you may want to check out my last post &lt;a href=&quot;/creating-singer-tap-to-capture-ebay-completed-items/&quot; target=&quot;_blank&quot;&gt;Creating Singer Tap to Capture Ebay Completed Items&lt;/a&gt;. It provides a high level background of the specification and how taps &amp;amp; targets work together.
    &lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;h3&gt;The Challenge&lt;/h3&gt;
&lt;p&gt;Last week I created a walk through of&amp;nbsp;&lt;a href=&quot;http://jaygrossman.com/post/2023/06/19/Creating-Singer-Tap-to-Capture-Ebay-Completed-Items.aspx&quot; target=&quot;_blank&quot;&gt;Creating Singer Tap to Capture Ebay Completed Items&lt;/a&gt;. While it&apos;s great to capture data, it&apos;s not overly useful without persisting the data to a target destination.&lt;/p&gt;
&lt;p&gt;There are some useful targets&amp;nbsp;&lt;a href=&quot;https://www.singer.io/#targets&quot; target=&quot;_blank&quot;&gt;posted on signer.io&lt;/a&gt;&amp;nbsp;and&amp;nbsp;&lt;a href=&quot;https://hub.meltano.com/loaders/&quot; target=&quot;_blank&quot;&gt;posted on meltano&lt;/a&gt;&amp;nbsp;for writing to a nice variety of standard destinations (databases, cloud data warehouses, S3, csv, json, etc.). However my site has an API and I could not find a target to send to a web endpoint.&lt;/p&gt;
&lt;h2&gt;&lt;span style=&quot;color: #ff0000;&quot;&gt;&lt;em&gt;&lt;strong&gt;I want to able to pipe data from a Singer Tap to my own API endpoints&lt;/strong&gt;&lt;/em&gt;&lt;/span&gt;&lt;/h2&gt;
&lt;h3&gt;Creating target-web_endpoint&lt;/h3&gt;
&lt;p&gt;Code for this Singer Target is posted on github (click on image below):&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/jaygrossman/target-web_endpoint&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;/assets/images/github_target-web_endpoint.png&quot; alt=&quot;&quot; style=&quot;border:1px solid blue;&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h4&gt;Project Scope&lt;/h4&gt;
&lt;p&gt;So the goal is to create a Singer target that will allow us to take data piped from a Singer tap and send it to a web endpoint (via a HTTP GET or HTTP POST). Below is a visual illustration:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/headers/target-web_endpoint.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This target must be able to support the following requirements:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Sending record data (piped from a tap) to a url endpoint via HTTP GET or HTTP Post.&lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;Configuration of basic auth credentials and HTTP Headers for HTTP Post method.&amp;nbsp;&lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;Configuration to map source data field names to target system&apos;s data field names.&amp;nbsp;&lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;Configuration to specify additional properties (with static values) to send to endpoint.&amp;nbsp;&lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;Configuration to specify VERY BASIC filter rules based on the record values.&lt;/li&gt;
&lt;/ol&gt;
&lt;div&gt;&amp;nbsp;&lt;/div&gt;
&lt;h4&gt;Helpful links to get background on Developing Singer Targets&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Singer provides&amp;nbsp;&lt;a href=&quot;https://github.com/singer-io/getting-started/blob/master/docs/RUNNING_AND_DEVELOPING.md#developing-a-tap&quot; target=&quot;_blank&quot;&gt;getting started docs&lt;/a&gt;&amp;nbsp;on creating targets.&amp;nbsp;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://meltano.slack.com/&quot; target=&quot;_blank&quot;&gt;Meltano&apos;s slack&lt;/a&gt;&amp;nbsp;has a dedicated channel&amp;nbsp;&lt;a href=&quot;https://meltano.slack.com/?redir=%2Farchives%2FC01RKUVUG4S&quot; target=&quot;_blank&quot;&gt;#singer-target-development&lt;/a&gt;&amp;nbsp;for help developing targets&amp;nbsp;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4&gt;Setting up development for Singer Target&lt;/h4&gt;
&lt;p&gt;In order to develop a tap, we need to install the Singer library:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;pip install singer-python
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Next we&apos;ll install&amp;nbsp;&lt;a href=&quot;https://www.cookiecutter.io/&quot; target=&quot;_blank&quot;&gt;cookiecutter&lt;/a&gt;&amp;nbsp;and download the&amp;nbsp;&lt;a href=&quot;https://github.com/singer-io/singer-target-template&quot; target=&quot;_blank&quot;&gt;target template&lt;/a&gt;&amp;nbsp;to give us a starting point:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;pip install cookiecutter
cookiecutter https://github.com/singer-io/singer-targer-template.git
project_name [e.g. &apos;tap-facebook&apos;]: target-web_endpoint
package_name [target_web_endpoint]:target_web_endpoint
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4&gt;Configuration file for the Target&lt;/h4&gt;
&lt;p&gt;There is a template you can use at&amp;nbsp;&lt;em&gt;config.json.example&lt;/em&gt;, just copy it to&amp;nbsp;&lt;em&gt;config.json&lt;/em&gt;&amp;nbsp;in the repo root and update the following values:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;{
    &quot;method&quot; : &quot;POST&quot;,
    &quot;url&quot;: &quot;https://api.some_web_site.com/lisitngs/&quot;,
    &quot;username&quot;: &quot;my_username&quot;,
    &quot;password&quot;: &quot;my_password&quot;,
    &quot;post_headers&quot; : {
        &quot;Content-Type&quot;: &quot;application/x-www-form-urlencoded&quot;
    },
    &quot;property_mapping&quot;: {
        &quot;field1&quot;: { &quot;target_field_name&quot;: &quot;target_field_1&quot;},
        &quot;field2&quot;: { &quot;target_field_name&quot;: &quot;target_field_2&quot;},
        &quot;field3&quot;: { &quot;target_field_name&quot;: &quot;target_field_3&quot;},
        &quot;field4&quot;: { &quot;target_field_name&quot;: &quot;field_4&quot;},
        &quot;field5&quot;: { &quot;target_field&quot;: &quot;field_5&quot;}
    },
    &quot;additional_properties&quot;: {
        &quot;system_id&quot;: 12,
        &quot;special_key&quot;: &quot;0cf18148-1687-11ee-be56-0242ac120002&quot;
    },
    &quot;filter_rules&quot;: {
        &quot;field1&quot;: { &quot;type&quot;: &quot;equals&quot;, &quot;value&quot;: true },
        &quot;field2&quot;: { &quot;type&quot;: &quot;not_equals&quot;, &quot;value&quot;: &quot;123&quot; },
        &quot;field3&quot;: { &quot;type&quot;: &quot;is_empty&quot;, &quot;value&quot;: false }
    }
} 
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;table border=&quot;1&quot; cellspacing=&quot;0&quot; cellpadding=&quot;0&quot;&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&quot;padding: 5px;&quot;&gt;&lt;strong&gt;Variable&lt;/strong&gt;&lt;/td&gt;
&lt;td style=&quot;padding: 5px;&quot;&gt;&lt;strong&gt;Description&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&quot;padding: 5px;&quot;&gt;method&lt;/td&gt;
&lt;td style=&quot;padding: 5px;&quot;&gt;method for calling url (GET or POST), default is GET&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&quot;padding: 5px;&quot;&gt;url&lt;/td&gt;
&lt;td style=&quot;padding: 5px;&quot;&gt;endpoint url&amp;nbsp;&lt;strong&gt;REQUIRED&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&quot;padding: 5px;&quot;&gt;username&lt;/td&gt;
&lt;td style=&quot;padding: 5px;&quot;&gt;user name for basic auth (only for POST)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&quot;padding: 5px;&quot;&gt;post_headers&lt;/td&gt;
&lt;td style=&quot;padding: 5px;&quot;&gt;dict of headers to pass (only for POST)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&quot;padding: 5px;&quot;&gt;property_mapping&lt;/td&gt;
&lt;td style=&quot;padding: 5px;&quot;&gt;define the properties received from tap to be sent to the endpoint. You can update the target property names)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&quot;padding: 5px;&quot;&gt;additional_properties&lt;/td&gt;
&lt;td style=&quot;padding: 5px;&quot;&gt;define additional properties with hard coded values that will be sent to the endpoint&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&quot;padding: 5px;&quot;&gt;filter_rules&lt;/td&gt;
&lt;td style=&quot;padding: 5px;&quot;&gt;configure rules to only include records when matching all criteria.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;text-decoration-line: underline;&quot;&gt;Notes about filter_rules:&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;1. Records will be sent to the endpoint only when they are valid for all the configured rules.&lt;br /&gt;2. You can only identify one rule for each field.&lt;br /&gt;3. There are 5 supported types of rules:&lt;/p&gt;
&lt;table border=&quot;1&quot; cellspacing=&quot;0&quot; cellpadding=&quot;0&quot;&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&quot;padding: 5px;&quot;&gt;&lt;strong&gt;Rule Type&lt;/strong&gt;&lt;/td&gt;
&lt;td style=&quot;padding: 5px;&quot;&gt;&lt;strong&gt;Description&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&quot;padding: 5px;&quot;&gt;equals&lt;/td&gt;
&lt;td style=&quot;padding: 5px;&quot;&gt;the field&apos;s value must equal the configured value&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&quot;padding: 5px;&quot;&gt;not_equals&lt;/td&gt;
&lt;td style=&quot;padding: 5px;&quot;&gt;the field&apos;s value must not equal the configured value&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&quot;padding: 5px;&quot;&gt;contains&lt;/td&gt;
&lt;td style=&quot;padding: 5px;&quot;&gt;the field&apos;s value must contain the configured value&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&quot;padding: 5px;&quot;&gt;not_contains&lt;/td&gt;
&lt;td style=&quot;padding: 5px;&quot;&gt;the field&apos;s value must not contain the configured value&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&quot;padding: 5px;&quot;&gt;is_empty&lt;/td&gt;
&lt;td style=&quot;padding: 5px;&quot;&gt;if true, the field&apos;s value must not be empty. if false, the field&apos;s value must be empty&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;h4&gt;Setting up to run the Target&lt;/h4&gt;
&lt;p&gt;Let&apos;s create a virtual environment to run our tap within:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;cd target-web_post
python3 -m venv ~/.virtualenvs/target-web_endpoint
source ~/.virtualenvs/target-web_endpoint/bin/activate
git clone git@github.com:jaygrossman/target-web_endpoint.git
cd target-web_endpoint
pip install requests
pip install -e .
deactivate 
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We can pipe the output of a tap to our target with the following command (after the | symbol):&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;run_your_tap | ~/.virtualenvs/target-web_endpoint/bin/target-web_endpoint
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4&gt;EXAMPLE: Running Tap-Csv + Target-web_endpoint&amp;nbsp;&lt;/h4&gt;
&lt;p&gt;I created a sample_data folder in the project&apos;s github repo that includes:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;sample_data.csv file contains a github keyword search&lt;/li&gt;
&lt;li&gt;tap-csv.config.json file contains config for the tap-csv&lt;/li&gt;
&lt;li&gt;target-web_endpoint.config.json file contains config for the target-web_endpoint&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Calling this thread will try to search github (https://github.com/search) via a HTTP GET request with the keywords supplied in sample_data.csv.&lt;/p&gt;
&lt;p&gt;Install tap-csv:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;python3 -m venv ~/.virtualenvs/tap-csv
source ~/.virtualenvs/tap-csv/bin/activate
pip install git+https://github.com/MeltanoLabs/tap-csv.git
deactivate
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We can run tap-csv piped to our target-web_endpoint with the following command:&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;~/.virtualenvs/tap-csv/bin/tap-csv --config sample_data/tap-csv.config.json | ~/.virtualenvs/target-web_endpoint/bin/target-web_endpoint --config sample_data/target-web_endpoint.config.json
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The command outputs the following:&lt;/p&gt;
&lt;p style=&quot;padding-left: 30px;&quot;&gt;2023-06-29 23:17:10,957 | INFO&amp;nbsp; &amp;nbsp; &amp;nbsp;| tap-csv&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; | Beginning full_table sync of &apos;seaches&apos;...&lt;br /&gt;2023-06-29 23:17:10,957 | INFO&amp;nbsp; &amp;nbsp; &amp;nbsp;| tap-csv&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; | Tap has custom mapper. Using 1 provided map(s).&lt;br /&gt;2023-06-29 23:17:10,957 | INFO&amp;nbsp; &amp;nbsp; &amp;nbsp;| singer_sdk.metrics&amp;nbsp; &amp;nbsp;| METRIC: {&quot;type&quot;: &quot;timer&quot;, &quot;metric&quot;: &quot;sync_duration&quot;, &quot;value&quot;: 0.000225067138671875, &quot;tags&quot;: {&quot;stream&quot;: &quot;searches&quot;, &quot;context&quot;: {}, &quot;status&quot;: &quot;succeeded&quot;}}&lt;br /&gt;2023-06-29 23:17:10,957 | INFO&amp;nbsp; &amp;nbsp; &amp;nbsp;| singer_sdk.metrics&amp;nbsp; &amp;nbsp;| METRIC: {&quot;type&quot;: &quot;counter&quot;, &quot;metric&quot;: &quot;record_count&quot;, &quot;value&quot;: 1, &quot;tags&quot;: {&quot;stream&quot;: &quot;searches&quot;, &quot;context&quot;: {}}}&lt;br /&gt;url: https://github.com/search?q=tap-ebaycompleted, response: &amp;lt;Response [200]&amp;gt;&lt;br /&gt;{&quot;bookmarks&quot;: {&quot;searches&quot;: {}}}&lt;/p&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;
</description>
        <pubDate>Fri, 30 Jun 2023 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/creating-singer-target-to-send-data-to-web-endpoint/</link>
        <guid isPermaLink="true">http://localhost:4000/creating-singer-target-to-send-data-to-web-endpoint/</guid>
        
        <category>data engineering</category>
        
        <category>singer</category>
        
        <category>python</category>
        
        
      </item>
    
      <item>
        <title>Creating Singer Tap to Capture Ebay Completed Items</title>
        <description>&lt;h3&gt;What is Singer&lt;/h3&gt;
&lt;p&gt;The &lt;a href=&quot;https://www.singer.io/&quot; target=&quot;_blank&quot;&gt;Singer specification&lt;/a&gt; bills itself as &lt;em&gt;&quot;the open-source standard for writing scripts that move data&quot;&lt;/em&gt;. &lt;a href=&quot;https://transferwise.github.io/pipelinewise/&quot; target=&quot;_blank&quot;&gt;PipelineWise&lt;/a&gt; and &lt;a href=&quot;https://meltano.com/&quot; target=&quot;_blank&quot;&gt;Meltano&lt;/a&gt; are popular open source platforms that use the Singer specification to accommodate ingest and replication of data from various sources to various destinations.&lt;/p&gt;
&lt;p&gt;Singer describes how data extraction scripts&amp;mdash;called &lt;strong&gt;&quot;taps&quot;&lt;/strong&gt;&amp;nbsp;&amp;mdash;and data loading scripts&amp;mdash;called &lt;strong&gt;&quot;targets&quot;&lt;/strong&gt;&amp;mdash; should communicate, allowing them to be used in any combination to move data from any source to any destination. Send data between databases, web APIs, files, queues, and just about anything else you can think of.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/ebaytap_1.png&quot; alt=&quot;&quot; /&gt;&lt;br /&gt;
&lt;small&gt;ETL Pipeline in Singer, image credit: &lt;a href=&quot;https://blog.panoply.io/etl-with-singer-a-tutorial&quot; target=&quot;_blank&quot;&gt;panoply&lt;/a&gt;&lt;/small&gt;&lt;/p&gt;

&lt;h3&gt;Completed Items on eBay&lt;/h3&gt;
&lt;p&gt;For over 20 years, eBay has allowed users to search by keyword for auctions and Buy It Now listings that have recently ended. This is very helpful to provide buyer and sellers with a directional idea of how much comparable items sell for.&lt;/p&gt;
&lt;p&gt;On the left side of any search page (near the bottom of the search results), users have the ability to filter by Completed Items as shown below:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/ebaytap_2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The screen below shows a search for the search term &quot;iphone 14&quot;:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/ebaytap_3.png&quot; alt=&quot;&quot; /&gt;&lt;br /&gt;
&lt;small&gt;URL:&amp;nbsp;&lt;a href=&quot;https://www.ebay.com/sch/i.html?LH_Complete=1&amp;amp;_nkw=iphone+14&quot; target=&quot;_blank&quot;&gt;https://www.ebay.com/sch/i.html?LH_Complete=1&amp;amp;_nkw=iphone+14&lt;/a&gt;&lt;/small&gt;&lt;/p&gt;

&lt;h3&gt;The Challenge&lt;/h3&gt;
&lt;p&gt;For a long time I&apos;ve wanted a standard method to be able to capture data about items that have sold on eBay and have an easy + repeatable way to save the data (to files, a database, etc.). I have pieced together various scripts in different languages (python, powershell, C#, php) at different times to accomplish this, but I wanted it to be based off a more standard and extensible framework.&lt;/p&gt;
&lt;p&gt;eBay offers&amp;nbsp;&lt;a href=&quot;https://developer.ebay.com/develop/apis&quot; target=&quot;_blank&quot;&gt;developer APIs&lt;/a&gt;&amp;nbsp;to query their data which I have used. While I could use the API&apos;s&amp;nbsp;&lt;a href=&quot;https://developer.ebay.com/devzone/finding/callref/findCompletedItems.html&quot; target=&quot;_blank&quot;&gt;findCompletedItems&lt;/a&gt;&amp;nbsp;end point, it is a far more straight forward implementation and faster learning opportunity for me to use some common python libraries to get the data from eBay&apos;s public web site.&lt;/p&gt;
&lt;div&gt;&amp;nbsp;&lt;/div&gt;
&lt;h3&gt;tap-ebaycompleted&lt;/h3&gt;
&lt;p&gt;Code for this Singer Tap is posted on github (click on image below):&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/jaygrossman/tap-ebaycompleted&quot; target=&quot;_blank&quot;&gt;&lt;img src=&quot;/assets/images/github_tap-completed.png&quot; alt=&quot;&quot; style=&quot;border:1px solid blue;&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

&lt;h4&gt;Project Scope&lt;/h4&gt;
&lt;p&gt;So the goal is to create a Singer tap that will allow us to generate out properly formatted JSON data with the details of the listing, that can be consumed by a Singer target (such as writing to a .csv file, an API or PostGres database). Below is a visual illustration:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/ebaytap_5.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h4&gt;Helpful links to get background on Developing Singer Taps&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Singer provides&amp;nbsp;&lt;a href=&quot;https://github.com/singer-io/getting-started/blob/master/docs/RUNNING_AND_DEVELOPING.md#developing-a-tap&quot; target=&quot;_blank&quot;&gt;getting started docs&lt;/a&gt;&amp;nbsp;on creating taps.&amp;nbsp;&lt;/li&gt;
&lt;li&gt;Meltano provides &lt;a href=&quot;https://sdk.meltano.com/en/latest/code_samples.html&quot; target=&quot;_blank&quot;&gt;code samples&lt;/a&gt; for developing taps&amp;nbsp;&lt;/li&gt;
&lt;/ul&gt;
&lt;div&gt;There are 2 functions in the &lt;a href=&quot;https://github.com/singer-io/singer-python&quot; target=&quot;_blank&quot;&gt;singer python library&lt;/a&gt;&amp;nbsp;for my simple tap that we must care about:&lt;/div&gt;
&lt;div&gt;&lt;ol&gt;
&lt;li&gt;singer.write_schema - responsible for defining the JSON schema that data will be outputted&lt;/li&gt;
&lt;li&gt;singer.write_records - responsible for outputting each record (data row)&lt;/li&gt;
&lt;/ol&gt;&lt;/div&gt;
&lt;h4&gt;&amp;nbsp;&lt;/h4&gt;
&lt;h4&gt;Setting up development for Singer Tap&lt;/h4&gt;
&lt;p&gt;In order to develop a tap, we need to install the Singer library:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;pip install singer-python
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&amp;nbsp;Next we&apos;ll install &lt;a href=&quot;https://www.cookiecutter.io/&quot; target=&quot;_blank&quot;&gt;cookiecutter&lt;/a&gt; and download the &lt;a href=&quot;https://github.com/singer-io/singer-tap-template&quot; target=&quot;_blank&quot;&gt;tap template&lt;/a&gt; to give us a starting point:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;pip install cookiecutter
cookiecutter https://github.com/singer-io/singer-tap-template.git
project_name [e.g. &apos;tap-facebook&apos;]: tap-ebaycompleted
package_name [tap_ebaycompleted]:
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4&gt;Defining the schema&lt;/h4&gt;
&lt;p&gt;The JSON schema defines the structure that the data will be outputted by the tap. Below I am writing a small inline python function to do so:&amp;nbsp;&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;def get_schema():
schema = {
    &quot;properties&quot;: {
        &quot;search_term&quot;: {&quot;type&quot;: &quot;string&quot;},
        &quot;title&quot;: {&quot;type&quot;: &quot;string&quot;},
        &quot;price&quot;: {&quot;type&quot;: &quot;string&quot;},
        &quot;bids&quot;: {&quot;type&quot;: &quot;string&quot;},
        &quot;buy_it_now&quot;: {&quot;type&quot;: &quot;boolean&quot;},
        &quot;condition&quot;: {&quot;type&quot;: &quot;string&quot;},
        &quot;image&quot;: {&quot;type&quot;: &quot;string&quot;},
        &quot;link&quot;: {&quot;type&quot;: &quot;string&quot;},
        &quot;end_date&quot;: {&quot;type&quot;: &quot;string&quot;}
        &quot;has_sold&quot;: {&quot;type&quot;: &quot;boolean&quot;}
        &quot;id&quot;: {&quot;type&quot;: &quot;string&quot;}
        }
    }
return schema
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We later call this function to create the schema object:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;schema = get_schema()
singer.write_schema(&quot;completed_item_schema&quot;, schema, &quot;id&quot;)
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4&gt;Getting data from eBay&apos;s Completed Items Search Results&lt;/h4&gt;
&lt;p&gt;In python, there are a few different libraries to parse the contents of a web page. In this tap, I use:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://requests.readthedocs.io/en/latest/&quot; target=&quot;_blank&quot;&gt;requests&lt;/a&gt;&amp;nbsp;for getting the HTML of the page&amp;nbsp;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://beautiful-soup-4.readthedocs.io/en/latest/&quot; target=&quot;_blank&quot;&gt;BeautifulSoup 4&lt;/a&gt;&amp;nbsp;for parsing the elements on the page&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;While I added some extra logic in the actual tap, below will provide the basic idea of how we can parse the page if you are new to these libraries:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
47
48
49
50
51
52
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;search_term = &apos;iphone 13&apos;
url = &quot;https://www.ebay.com/sch/i.html?LH_Complete=1&amp;amp;_sop=13&amp;amp;_nkw={search_term}&quot;

response = requests.get(url)
html_content = str(response.content)
soup = BeautifulSoup(html_content, &quot;html.parser&quot;)

# Each completed item is within a &amp;lt;li&amp;gt; tag with class=s-item s-item__pl-on-bottom
listings = soup.find_all(&quot;li&quot;, class_=&quot;s-item s-item__pl-on-bottom&quot;)

# Iterate over completed listings
for listing in listings:
    title = listing.find(&quot;div&quot;, class_=&quot;s-item__title&quot;).text
    price = listing.find(&quot;span&quot;, class_=&quot;s-item__price&quot;).text
    condition = listing.find(&quot;span&quot;, class_=&quot;SECONDARY_INFO&quot;).text
    image= listing.find(&quot;img&quot;)[&apos;src&apos;]
    link = listing.find(&quot;a&quot;, class_=&quot;s-item__link&quot;)[&apos;href&apos;]
    id = link[0:link.index(&quot;?&quot;)].replace(&quot;https://www.ebay.com/itm/&quot;, &quot;&quot;)
    bids = &quot;&quot;
    try:
        bids = listing.find(&quot;span&quot;, class_=&quot;s-item__bids s-item__bidCount&quot;).text
    except:
        bids = &quot;&quot;
    buy_it_now = False
    try:
        if listing.find(&quot;span&quot;, class_=&quot;s-item__dynamic s-item__buyItNowOption&quot;).text == &quot;Buy It Now&quot;:
            buy_it_now = True
    except:
        buy_it_now = False
    has_sold = False
    try:
        if listing.find(&quot;div&quot;, class_=&quot;s-item__title--tag&quot;).find(&quot;span&quot;, class_=&quot;clipped&quot;).text == &quot;Sold Item&quot;:
            has_sold = True
            end_date=listing.find(&quot;div&quot;, class_=&quot;s-item__title--tag&quot;).find(&quot;span&quot;, class_=&quot;POSITIVE&quot;).text
        else:
            end_date = listing.find(&quot;div&quot;, class_=&quot;s-item__title--tag&quot;).find(&quot;span&quot;, class_=&quot;NEGATIVE&quot;).text
    except:
        end_date=&quot;&quot;

    record = {
        &quot;search_term&quot;: search_term,
        &quot;title&quot;: title,
        &quot;price&quot;: price,
        &quot;condition&quot;: condition,
        &quot;image&quot;: image,
        &quot;link&quot;: link,
        &quot;id&quot;: id,
        &quot;bids&quot;: bids,
        &quot;buy_it_now&quot;: buy_it_now,
        &quot;end_date&quot;: end_date,
        &quot;has_sold&quot;: has_sold
    } 
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We can then pass the schema (&quot;completed_item_schema&quot;) that we defined earlier and the record to singer.write_records:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;singer.write_records(&quot;completed_item_schema&quot;, [record])
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4&gt;Building and Configuring the Tap&lt;/h4&gt;
&lt;p&gt;The template created a directory named tap-ebaycompleted. Since this is a very simple tap, I removed the contents and created a single file called &lt;a href=&quot;https://github.com/jaygrossman/tap-ebaycompleted/blob/main/tap_ebaycompleted/__init__.py&quot; target=&quot;_blank&quot;&gt;__init__.py&lt;/a&gt;. In the main function, I provided the logic specified in the sections above with a little bit of additional logic.&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;text-decoration: underline;&quot;&gt;Create a configuration file&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;There is a template you can use at &lt;em&gt;config.json.example&lt;/em&gt;, just copy it to &lt;em&gt;config.json&lt;/em&gt; in the repo root and update the following values:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;{
    &quot;search_terms&quot;: [&quot;iphone 13&quot;, &quot;iphone 14&quot;],
    &quot;page_size&quot;: 240,
    &quot;min_wait&quot;: 2.1,
    &quot;max_wait&quot;: 4.4
} 
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;table border=&quot;1&quot; cellspacing=&quot;0&quot; cellpadding=&quot;0&quot;&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&quot;padding: 5px;&quot;&gt;&lt;strong&gt;Variable&lt;/strong&gt;&lt;/td&gt;
&lt;td style=&quot;padding: 5px;&quot;&gt;&lt;strong&gt;Description&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&quot;padding: 5px;&quot;&gt;search_terms&lt;/td&gt;
&lt;td style=&quot;padding: 5px;&quot;&gt;list of terms that the tap will search for &lt;strong&gt;REQUIRED&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&quot;padding: 5px;&quot;&gt;page_size&lt;/td&gt;
&lt;td style=&quot;padding: 5px;&quot;&gt;number of records to return (values can be 240,120,60), default is 120&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&quot;padding: 5px;&quot;&gt;min_wait&lt;/td&gt;
&lt;td style=&quot;padding: 5px;&quot;&gt;minimum amount of time between searches, default is 2 seconds&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&quot;padding: 5px;&quot;&gt;max_wait&lt;/td&gt;
&lt;td style=&quot;padding: 5px;&quot;&gt;maximum amount of time between searches, default is 5 seconds&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;
&lt;h4&gt;Running the Tap&lt;/h4&gt;
&lt;p&gt;Let&apos;s create a virtual environment to run our tap within:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;cd tap-ebaycompleted
python3 -m venv ~/.virtualenvs/tap-ebaycompleted
source ~/.virtualenvs/tap-ebaycompleted/bin/activate
git clone git@github.com:jaygrossman/tap-ebaycompleted.git
cd tap-ebaycompleted
pip3 install requests
pip3 install BeautifulSoup4
pip3 install .
deactivate 
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We can run our tap with the following command:&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;~/.virtualenvs/tap-ebaycompleted/bin/tap-ebaycompleted -c config.json
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Below is a sample record representing a completed item:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;{
    &quot;type&quot;: &quot;RECORD&quot;, 
&quot;stream&quot;: 
&quot;completed_item_schema&quot;, 
&quot;record&quot;: {
    &quot;search_term&quot;: &quot;iphone 13&quot;, 
    &quot;title&quot;: &quot;Apple iPhone 13 - 128GB - Midnight&quot;, 
    &quot;price&quot;: &quot;$411.00&quot;, 
    &quot;condition&quot;: &quot;Pre-Owned&quot;, 
    &quot;image&quot;: &quot;https://shorturl.at/bqrsK&quot;, 
    &quot;link&quot;: &quot;https://www.ebay.com/itm/314645218752&quot;, 
    &quot;id&quot;: &quot;314645218752&quot;, 
    &quot;bids&quot;: &quot;&quot;, 
    &quot;buy_it_now&quot;: false
    &quot;end_date&quot;: &quot;Jun 21, 2023&quot;
    &quot;has_sold&quot;: True
    }
}
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4&gt;Running the Tap with a Target to Export the Data to a .csv File&lt;/h4&gt;
&lt;p&gt;Install target-csv:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;python3 -m venv ~/.virtualenvs/target-csv
source ~/.virtualenvs/target-csv/bin/activate
pip3 install target-csv
deactivate
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We can run our tap piped to target-csv with the following command:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;~/.virtualenvs/tap-ebaycompleted/bin/tap-ebaycompleted -c config.json |
~/.virtualenvs/target-csv/bin/target-csv 
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Below is the first couple of lines of the .csv file the command generated:&lt;/p&gt;
&lt;p style=&quot;padding-left: 30px;&quot;&gt;search_term,title,price,condition,image,link,id,bids,buy_it_now,end_date,has_sold&lt;/p&gt;
&lt;p style=&quot;padding-left: 30px;&quot;&gt;iphone 13,Apple iPhone 13 - 128GB - Midnight (Verizon),$170.00,Parts Only,https://i.ebayimg.com/thumbs/images/g/AKkAAOSw55lkhLVt/s-l300.jpg,https://www.ebay.com/itm/225623521919,225623521919,0 bids,False,&quot;Jun 21, 2023&quot;,True&lt;/p&gt;

&lt;h3&gt;Conclusions and What&apos;s Next&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;You should not consider this a production quality tap. eBay often changes the format of the HTML on their search results page and this Tap will need to be updated when this occurs.&amp;nbsp;&amp;nbsp;&lt;br /&gt;&lt;br /&gt;The&amp;nbsp;&lt;a href=&quot;https://github.com/singer-io/tap-ebay&quot; target=&quot;_blank&quot;&gt;tap-ebay&lt;/a&gt;&amp;nbsp;project (developed by &lt;a href=&quot;https://www.linkedin.com/in/drewbanin/&quot; target=&quot;_blank&quot;&gt;Drew Banin&lt;/a&gt; of Fishtown/dbt) is well done. It uses the fulfillment API to allow a seller to query for their orders received via eBay&apos;s marketplace.&amp;nbsp; I may refactor this tap using some of Drew&apos;s concepts.&lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;Singer is very robust and powerful. It has pretty slick support for Authentication to data source and State management that I did not need in this very simple idempotent example.&lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;This python code only returns the first page of results for the provided search terms. It would be beneficial to add in support to iterate through multiple pages of results.&lt;br /&gt;&lt;br /&gt;&lt;span style=&quot;color: #ff0000;&quot;&gt;&lt;strong&gt;Update 2023-06-27:&lt;/strong&gt;&lt;/span&gt; Added support to configure the maximum search result pages to capture records from.&lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;I&apos;d like to extend the tap with the option of retrieving the list of search terms getting pulled from web url or an API call.&lt;br /&gt;&lt;br /&gt;&lt;span style=&quot;color: #ff0000;&quot;&gt;&lt;strong&gt;Update 2023-06-28:&lt;/strong&gt;&lt;/span&gt;&amp;nbsp;Added support to configure a public url where search terms and corresponding SKUs can be fed in from.&lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;I will probably build a new Singer target to persist the data from this tap&amp;nbsp;to my private API . And then I can orchestrate the pipeline on a cron with either &lt;a href=&quot;https://dagster.io/&quot; target=&quot;_blank&quot;&gt;Dagster&lt;/a&gt; or try out Pipelinewise.&lt;br /&gt;&lt;br /&gt;&lt;span style=&quot;color: #ff0000;&quot;&gt;&lt;strong&gt;Update 2023-06-30:&lt;/strong&gt;&lt;/span&gt;&amp;nbsp;Created a configurable singer target with blog post at&amp;nbsp;&lt;a href=&quot;/creating-singer-target-to-send-data-to-web-endpoint/&quot;&gt;Creating Singer Target to Send Data to Web Endpoint&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&amp;nbsp;&amp;nbsp;&lt;/p&gt;
</description>
        <pubDate>Mon, 19 Jun 2023 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/creating-singer-tap-to-capture-ebay-completed-items/</link>
        <guid isPermaLink="true">http://localhost:4000/creating-singer-tap-to-capture-ebay-completed-items/</guid>
        
        <category>data engineering</category>
        
        <category>singer</category>
        
        <category>ebay</category>
        
        <category>python</category>
        
        
      </item>
    
      <item>
        <title>Modern Data Stack in a box (all Open Source)</title>
        <description>&lt;p&gt;This week I participated in a hackathon for fun with a couple of friends. There were 3 rules for each of us:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;Upgrade something you are unhappy with in a side project&lt;/li&gt;
&lt;li&gt;Try some new piece of tech&lt;/li&gt;
&lt;li&gt;Document what you did&lt;/li&gt;
&lt;/ol&gt;

&lt;h3&gt;My project&lt;/h3&gt;

&lt;p&gt;Over the past 10 or so years, I have had a lot of fun running&amp;nbsp;&lt;a href=&quot;http://www.collectz.com/&quot; target=&quot;_blank&quot;&gt;CollectZ&lt;/a&gt;&amp;nbsp;- Research and Arbitrage Platform for collectibles categories.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;TL;DR - The platform informs what undervalued collectibles I should consider buying and when + where + at what price I should sell my inventory.&lt;/p&gt;
&lt;p&gt;If you are trying to run arbitrage (exploiting inefficiencies between marketplaces) effectively with physical (non-liquid) assets, it is rather important to be able to have a pretty diverse data set and be able do analysis + reporting + run models across a variety of features.&amp;nbsp;&lt;/p&gt;
&lt;h3&gt;My problem&lt;/h3&gt;

&lt;p&gt;As I went along building out my platform, I had data sitting in different diverse repositories and in many formats. Some of the exploration and reporting tools I cobbled together over the years suffered because they were:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;fragile to keep running (things broke unexpectedly)&lt;/li&gt;
&lt;li&gt;hard to troubleshoot for data lineage issues&lt;/li&gt;
&lt;li&gt;performing slowly with more advanced queries (based off relational databases and file stores)&lt;/li&gt;
&lt;li&gt;labor intensive to extend&amp;nbsp;&lt;/li&gt;
&lt;li&gt;lacking the general features I wanted&amp;nbsp;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The tech debt became more than just noticeable, as it was impacting the effectiveness of the platform and more importantly effectiveness of my trades.&amp;nbsp;&lt;/p&gt;
&lt;h3&gt;Requirements for something better&lt;/h3&gt;
&lt;p&gt;Before I go about building anything, it is a really good idea to write down some requirements.&amp;nbsp; &amp;nbsp;&lt;/p&gt;
&lt;p&gt;When thinking about my reporting and data exploration needs for CollectZ, I had some requirements:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;I want a fully open source (free) stack that I can configure to run locally on my laptop and could be run in a cloud provider (likely AWS).&lt;/li&gt;
&lt;li&gt;I want to use dbt to build my models&lt;/li&gt;
&lt;li&gt;I want relatively a fast centralized data store (preferably columnar based)&lt;/li&gt;
&lt;li&gt;I want a way to keep the reporting sync&apos;d regularly from production / source systems&lt;/li&gt;
&lt;li&gt;I want an off the shelf tool for self service data exploration and BI&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;The Data&lt;/h3&gt;
&lt;p&gt;For this exercise, I am using a subsection of recent transaction and valuation data for Sports Cards category.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;I have exported data from 7 tables into .csv files and posted them to S3. Below is a relationship diagram source data model we will be working with:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/data_stack_1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;table style=&quot;width: 608.8px;&quot; border=&quot;1&quot; cellpadding=&quot;3&quot;&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&quot;padding: 3px;&quot;&gt;&lt;strong&gt;Table&lt;/strong&gt;&lt;/td&gt;
&lt;td style=&quot;padding: 3px;&quot;&gt;&lt;strong&gt;Description&lt;/strong&gt;&lt;/td&gt;
&lt;td style=&quot;padding: 3px;&quot;&gt;&lt;strong&gt;Record Count&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&quot;padding: 3px;&quot;&gt;Item&lt;/td&gt;
&lt;td style=&quot;padding: 3px;&quot;&gt;Reference information about a sports card in our catalog&lt;/td&gt;
&lt;td style=&quot;padding: 3px;&quot;&gt;116,459&amp;nbsp;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&quot;padding: 3px;&quot;&gt;ItemValue&lt;/td&gt;
&lt;td style=&quot;padding: 3px;&quot;&gt;Weekly calculated values for each item in different conditions&lt;/td&gt;
&lt;td style=&quot;padding: 3px;&quot;&gt;5,746,066&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&quot;padding: 3px;&quot;&gt;CardSet&lt;/td&gt;
&lt;td style=&quot;padding: 3px;&quot;&gt;The sport, year and name for each set associated with the card&lt;/td&gt;
&lt;td style=&quot;padding: 3px;&quot;&gt;691&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&quot;padding: 3px;&quot;&gt;Sport&lt;/td&gt;
&lt;td style=&quot;padding: 3px;&quot;&gt;The sport associated with the card set&lt;/td&gt;
&lt;td style=&quot;padding: 3px;&quot;&gt;4&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&quot;padding: 3px;&quot;&gt;ItemTransaction&lt;/td&gt;
&lt;td style=&quot;padding: 3px;&quot;&gt;Recent sales for selected items&lt;/td&gt;
&lt;td style=&quot;padding: 3px;&quot;&gt;3,175,362&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&quot;padding: 3px;&quot;&gt;ActiveItem&lt;/td&gt;
&lt;td style=&quot;padding: 3px;&quot;&gt;Items available for sale&lt;/td&gt;
&lt;td style=&quot;padding: 3px;&quot;&gt;4,489,085&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&quot;padding: 3px;&quot;&gt;Source&lt;/td&gt;
&lt;td style=&quot;padding: 3px;&quot;&gt;Sources of data for ItemTransaction and ActiveItem&lt;/td&gt;
&lt;td style=&quot;padding: 3px;&quot;&gt;12&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;table style=&quot;width: 100%; border-color:red;&quot; border=&quot;1&quot; cellpadding=&quot;5&quot;&gt;
&lt;tr&gt;
&lt;td&gt;
   &lt;strong&gt;&lt;i&gt;Please Note:&lt;/i&gt;&lt;/strong&gt;&lt;br /&gt;
    I am not able to share actual Collectz data on my blog and how we get this (let&apos;s assume there are lots of fun data engineering pipelines at work here).
    &lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3&gt;Solution Architecture&lt;/h3&gt;
&lt;p&gt;I have run data teams that build out data products for my day job, so I knew I could streamline and modernize on some best practice tooling.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;The components of my stack:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/data_stack_2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://duckdb.org/&quot; target=&quot;_blank&quot;&gt;DuckDB&lt;/a&gt;&amp;nbsp;- our local data warehouse&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://airbyte.com/&quot; target=&quot;_blank&quot;&gt;Airbyte&lt;/a&gt;&amp;nbsp;- for populating data into DuckDB&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://www.getdbt.com/&quot; target=&quot;_blank&quot;&gt;dbt&lt;/a&gt;&amp;nbsp;- for building models for analysis&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://www.metabase.com/&quot; target=&quot;_blank&quot;&gt;Metabase&lt;/a&gt;&amp;nbsp;- BI tool&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://jupyter.org/&quot;&gt;Jupyter&lt;/a&gt;&amp;nbsp;- for ad hoc analysis via Notebooks&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;DuckDB&lt;/h4&gt;
&lt;p&gt;I have a good amount of experience with commercial cloud data warehouses (Snowflake, BigQuery, DataBricks). While they are great products, they are all pretty expensive and I don&apos;t have budget for a personal project like CollectZ. I wanted something free + open source that I could run locally for development and later in AWS for production.&lt;/p&gt;
&lt;p&gt;My first thought was to go with Postgres, but I wasn&apos;t overly excited to manage a relational database (index, keys, etc.) and I like columnar databases for analytics. I also thought about Druid, but that would involve some overhead to set up and surprisingly doesn&apos;t yet have dbt integration.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;My friend Rob told me about how DuckDB offered much of the functionality I liked in Snowflake, but it was super easy to manage and would run locally on my laptop.&lt;/p&gt;
&lt;p&gt;DuckDB is an in-process SQL OLAP database management system. AND I really like the idea of their tagline:&lt;/p&gt;
&lt;p&gt;&lt;em&gt;All the benefits of a database, none of the hassle.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;After playing around with it, I found the main points and features of DuckDB?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Simple, clean install, with very little overhead.&amp;nbsp;&lt;/li&gt;
&lt;li&gt;Feature-rich with SQL, plus CSV and Parquet support.&amp;nbsp;&lt;/li&gt;
&lt;li&gt;In-memory option, high-speed, parallel processing.&amp;nbsp;&lt;/li&gt;
&lt;li&gt;Open-source.&amp;nbsp;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;DuckDB isn&amp;rsquo;t meant to replace MySQL, Postgres, and the rest of those relational databases. In fact, they tell you DuckDB is very BAD at &amp;ldquo;High-volume transactional use cases,&amp;rdquo; and &amp;ldquo;Writing to a single database from multiple concurrent processes.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;This 10 minute Youtube video from &lt;a href=&quot;https://www.linkedin.com/in/ryguyrg/&quot; target=&quot;_blank&quot;&gt;Ryan Boyd&lt;/a&gt; does a good job with more background on DuckDB:&lt;br /&gt; &lt;br /&gt; &lt;iframe title=&quot;YouTube video player&quot; src=&quot;https://www.youtube.com/embed/5GewuzicW7k&quot; frameborder=&quot;0&quot; width=&quot;560&quot; height=&quot;315&quot;&gt;&lt;/iframe&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Installation Instructions:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Reference from &lt;a href=&quot;https://duckdb.org/docs/installation/&quot; target=&quot;_blank&quot;&gt;https://duckdb.org/docs/installation/&lt;/a&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;brew install duckdb&lt;/li&gt;
&lt;/ol&gt;

&lt;h4&gt;Airbyte&lt;/h4&gt;
&lt;p&gt;I have implemented&amp;nbsp;&lt;a href=&quot;https://www.fivetran.com/&quot; target=&quot;_blank&quot;&gt;FiveTran&lt;/a&gt;&amp;nbsp;at 2 of my day jobs (ElysiumHealth and Luma), and it is a very easy to configure tool to set up pipelines to sync data from various sources into a data warehouse. Overall it works well if you do not have low latency requirements (it is great for hourly syncs), but it is volume based and can be quite expensive once you go past syncing 500,000 records in a month on their&amp;nbsp;&lt;a href=&quot;https://www.fivetran.com/pricing/free-plan&quot; target=&quot;_blank&quot;&gt;new free tier offering&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;For this project, I wanted to try the closest Open Source equivalent I could find - Airbyte.&amp;nbsp;&lt;/p&gt;

&lt;table style=&quot;width: 100%; border-color:red;&quot; border=&quot;1&quot; cellpadding=&quot;5&quot;&gt;
&lt;tr&gt;
&lt;td&gt;
   &lt;strong&gt;&lt;i&gt;Please Note:&lt;/i&gt;&lt;/strong&gt;&lt;br /&gt;
    The latest version or Airbyte 0.443, only supports DuckDB 0.6.1 (version 39). All the other tools in this post I am configuring are using DuckDB 0.8.0 (which launched within the past month), so I would not be able to use Airbyte in this full stack until its DuckDB version support is aligns with Metabase and dbt.&amp;nbsp;&amp;nbsp;&lt;br /&gt;&lt;br /&gt;Airbyte is a valuable tool for syncing data (I also tried it syncing to Postgres) and I feel that is worthwhile to share my learnings of how I would/will use it.
    &lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Installation Instructions&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Reference from&amp;nbsp;&lt;a href=&quot;https://docs.airbyte.com/quickstart/deploy-airbyte/&quot; target=&quot;_blank&quot;&gt;https://docs.airbyte.com/quickstart/deploy-airbyte/&lt;/a&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Install Docker on your workstation (see instructions). Make sure you&apos;re on the latest version of docker-compose.&lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;Run the following commands in your terminal:&lt;br /&gt;&lt;br /&gt;git clone https://github.com/airbytehq/airbyte.git&lt;br /&gt;cd airbyte&lt;br /&gt;./run-ab-platform.sh&lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;Once you see an Airbyte banner, the UI is ready to go at http://localhost:8000&lt;br /&gt;By default, to login you can use: username=airbyte and password=password&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;When I opened up Docker Desktop, you can see airbyte has 12 dockers it has running:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/data_stack_3.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;br /&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Configuring a Pipeline in Airbyte&amp;nbsp;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;1) The first step is to set up a Source. In this case I am setting up a &quot;File&quot; source that can read from sport.csv in my S3 bucket. I will need to provide my AWS creds as shown below:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/data_stack_4.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;2) Next we will need to set up a destination where we can write the data. This&amp;nbsp;&lt;a href=&quot;https://docs.airbyte.com/integrations/destinations/duckdb/&quot; target=&quot;_blank&quot;&gt;link&lt;/a&gt;&amp;nbsp;explains how to update the .env file so that a local directory on my machine will be mounted within the airbyte-server docker image.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/data_stack_5.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;3) We will need to configure a connection that will schedule running syncs from our source to our destination:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/data_stack_6.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;4) We see the result of the connection syncing data to my destination:&amp;nbsp;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/data_stack_7.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;5) I checked the DuckDB database and saw the following data synced over:&amp;nbsp;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/data_stack_8.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The _airbyte_data column in the screenshot above contains json representing the data in the csv file. This would need to be transformed into a relational format for reporting via a BI tool (such as metabase).&lt;/p&gt;

&lt;h4&gt;dbt support for DuckDB&lt;/h4&gt;

&lt;p&gt;dbt is a popular tool for allowing developers and analysts to create data models in SQL (and now Python), managing graph dependencies and supporting tests. I have been a user of dbt since 2017 when my team at Rent the Runway first starting use it to standardize large parts of our pipelines.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Installation + Set Up Instructions:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Reference from&amp;nbsp;&lt;a href=&quot;https://github.com/jwills/dbt-duckdb&quot; target=&quot;_blank&quot;&gt;https://github.com/jwills/dbt-duckdb&lt;/a&gt;:&lt;/p&gt;

&lt;p&gt;1) Install dbt and dbt-duckdb:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;pip3 install dbt-core
pip3 install dbt-duckdb
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;2) Configure the ~/.dbt/profile.yml file with the following:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;collectz:
target: dev
outputs:
    dev:
    type: duckdb
    path: /path_to_file/collectz.db
    threads: 4
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;3) Create a new directory&amp;nbsp; and create new dbt project:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;mkdir dbt
cd dbt
dbt init
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;4) I entered the name for your project&amp;nbsp;&lt;em&gt;collectz&lt;/em&gt;&amp;nbsp;when prompted&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Building models in dbt&lt;/strong&gt;&amp;nbsp;&lt;/p&gt;
&lt;p&gt;This post assumes you have working knowledge of how dbt works. If you do not, their&amp;nbsp;&lt;a href=&quot;https://docs.getdbt.com/quickstarts/manual-install?step=1&quot; target=&quot;_blank&quot;&gt;quick start&lt;/a&gt;&amp;nbsp;is helpful.&lt;/p&gt;
&lt;p&gt;Since I could not use Airbyte due to version incompatibilities mentioned above, I made simple models that created VIEWS on top of local copies of our 7.csv files.&lt;/p&gt;
&lt;p&gt;Example is _raw_active_item.sql (it uses the read_csv_auto function to read in a csv) as shown below:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;FROM read_csv_auto(&apos;/path_to _data_file/Active_Item.csv&apos;)
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;table style=&quot;width: 100%; border-color:red;&quot; border=&quot;1&quot; cellpadding=&quot;5&quot;&gt;
&lt;tr&gt;
&lt;td&gt;
   &lt;strong&gt;&lt;i&gt;Please Note:&lt;/i&gt;&lt;/strong&gt;&lt;br /&gt;
    I tried to use&amp;nbsp;&lt;em&gt;dbt seed&lt;/em&gt;&amp;nbsp;which would make the larger datasets persisted as tables, but it was pretty inefficient to copy over millions of rows when DuckDB is designed to efficiently process data in files at request time. I found the read_csv_auto() function performed well on data sets with 8M+ rows.
    &lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Now that I have &quot;raw&quot; tables, I created SQL files that would be my BI reporting schema containing the appropriate transformations and aggregations.&lt;/p&gt;

&lt;h4&gt;Metabase&lt;/h4&gt;
&lt;p&gt;I have used a bunch of different BI / Dashboard tools over the years. I feel like Looker is probably the best of the commercial tools, but set up is time consuming and it has become really expensive. Metabase is an open source alternative that I have found really easy to set up, easy to use, and has some of the advanced features I would want.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;I found Maz&apos;s 6 minute&amp;nbsp;&lt;a href=&quot;https://www.metabase.com/demo&quot; target=&quot;_blank&quot;&gt;Demo video&lt;/a&gt;&amp;nbsp;posted below walks you through the basics from an analyst perspective doing some data exploration and then incorporating it into dashboards:&amp;nbsp;&amp;nbsp;&lt;/p&gt;
&lt;iframe src=&quot;https://www.loom.com/embed/97107a8c8b054cd8bfa8dacb297d2c04&quot; frameborder=&quot;0&quot; width=&quot;500&quot; height=&quot;300&quot;&gt;&lt;/iframe&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Installation Instructions:&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;install java - I used this link to install Eclispe Temurin:&lt;br /&gt;&lt;a href=&quot;https://www.metabase.com/docs/latest/installation-and-operation/running-the-metabase-jar-file#:~:text=of%20JRE%20from-,Eclipse%20Temurin,-with%20HotSpot%20JVM&quot;&gt;https://www.metabase.com/docs/latest/installation-and-operation/running-the-metabase-jar-file#:~:text=of%20JRE%20from-,Eclipse%20Temurin,-with%20HotSpot%20JVM&lt;br /&gt;&lt;br /&gt;&lt;/a&gt;
&lt;p class=&quot;p1&quot; style=&quot;margin: 0px; font-variant-numeric: normal; font-variant-east-asian: normal; font-variant-alternates: normal; font-kerning: auto; font-optical-sizing: auto; font-feature-settings: normal; font-variation-settings: normal; font-stretch: normal; font-size: 11px; line-height: normal; font-family: Menlo; color: #000000;&quot;&gt;&lt;span class=&quot;s1&quot; style=&quot;font-variant-ligatures: no-common-ligatures;&quot;&gt;% java --version&lt;/span&gt;&lt;/p&gt;
&lt;p class=&quot;p1&quot; style=&quot;margin: 0px; font-variant-numeric: normal; font-variant-east-asian: normal; font-variant-alternates: normal; font-kerning: auto; font-optical-sizing: auto; font-feature-settings: normal; font-variation-settings: normal; font-stretch: normal; font-size: 11px; line-height: normal; font-family: Menlo; color: #000000;&quot;&gt;&lt;span class=&quot;s1&quot; style=&quot;font-variant-ligatures: no-common-ligatures;&quot;&gt;openjdk 17.0.7 2023-04-18&lt;/span&gt;&lt;/p&gt;
&lt;p class=&quot;p1&quot; style=&quot;margin: 0px; font-variant-numeric: normal; font-variant-east-asian: normal; font-variant-alternates: normal; font-kerning: auto; font-optical-sizing: auto; font-feature-settings: normal; font-variation-settings: normal; font-stretch: normal; font-size: 11px; line-height: normal; font-family: Menlo; color: #000000;&quot;&gt;&lt;span class=&quot;s1&quot; style=&quot;font-variant-ligatures: no-common-ligatures;&quot;&gt;OpenJDK Runtime Environment Temurin-17.0.7+7 (build 17.0.7+7)&lt;/span&gt;&lt;/p&gt;
&lt;p class=&quot;p1&quot; style=&quot;margin: 0px; font-variant-numeric: normal; font-variant-east-asian: normal; font-variant-alternates: normal; font-kerning: auto; font-optical-sizing: auto; font-feature-settings: normal; font-variation-settings: normal; font-stretch: normal; font-size: 11px; line-height: normal; font-family: Menlo; color: #000000;&quot;&gt;&lt;span class=&quot;s1&quot; style=&quot;font-variant-ligatures: no-common-ligatures;&quot;&gt;OpenJDK 64-Bit Server VM Temurin-17.0.7+7 (build 17.0.7+7, mixed mode)&lt;br /&gt;&lt;br /&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;Download the&amp;nbsp;&lt;a href=&quot;https://www.metabase.com/docs/latest/installation-and-operation/running-the-metabase-jar-file#:~:text=2.%20Download%20Metabase-,Download%20the%20JAR%20file%20for%20Metabase%20OSS,-.&quot; target=&quot;_blank&quot;&gt;JAR file for Metabase OSS&lt;/a&gt;&lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;Download the&amp;nbsp;&lt;a href=&quot;https://github.com/AlexR2D2/metabase_duckdb_driver/releases/tag/0.1.6#:~:text=duckdb.metabase%2Ddriver.jar&quot; target=&quot;_blank&quot;&gt;DuckDB driver for metabase&lt;/a&gt;&amp;nbsp;&lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;run the following commands from command line to create the metabase directory and move the jar files:&lt;br /&gt;&lt;br /&gt;mkdir ~/metabase&lt;br /&gt;mv ~/Downloads/metabase.jar ~/metabase&lt;br /&gt;cd ~/metabase&lt;br /&gt;mv ~/Downloads/duckdb.metabase-driver.jar ~/metabase/plugins&amp;nbsp;&lt;br /&gt;&lt;br /&gt;
&lt;/li&gt;
&lt;li&gt;Run the metabase jar file with the following commands:&lt;br /&gt;&lt;br /&gt;cd ~/metabase&lt;br /&gt;java -jar metabase.jar&lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;got to http://localhost:3000 in your browser&lt;/li&gt;
&lt;/ol&gt;

&lt;table style=&quot;width: 100%; border-color:red;&quot; border=&quot;1&quot; cellpadding=&quot;5&quot;&gt;
&lt;tr&gt;
&lt;td&gt;
   &lt;strong&gt;&lt;i&gt;Please Note:&lt;/i&gt;&lt;/strong&gt;&lt;br /&gt;
    To productionize metabase, we will want to review&amp;nbsp;&lt;a href=&quot;https://www.metabase.com/docs/latest/installation-and-operation/running-the-metabase-jar-file&quot; target=&quot;_blank&quot;&gt;this page&lt;/a&gt;&amp;nbsp;with instructions how to:&lt;br /&gt;
&amp;nbsp; &amp;nbsp; 1. run metabase with a postgres as the data store instead of using the default h2 file.&lt;br /&gt;
&amp;nbsp; &amp;nbsp; 2.run the java process as a service using systemd&amp;lt;
    &lt;/td&gt;
&lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Data Configuration in Metabase:&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Once you log into Metabase&apos;s Admin section, you set up a connection to register your DuckDB database as shown below:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/data_stack_9.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Under the Data Model area, you can define elements of your data model from any registered databases shown below. Here you can set which tables are visible and how tables join together (similar to primary-foreign keys).&amp;nbsp; &amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/data_stack_10.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h3&gt;The Deliverable&lt;/h3&gt;
&lt;p&gt;So I made some widgets and dashboards that make it easy enough for me to explore my data, quickly see some overviews of products with dashboards, and troubleshoot issues.&amp;nbsp;&amp;nbsp;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Data Exploration Example - Finding undervalued items:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Since DuckDb doesn&apos;t have a nice web based SQLRunner like Snowflake or BigQuery, Metabase made it really easy for me to write a SQL query joining across tables with minimal aggregation + filtering to visualize results.&lt;/p&gt;
&lt;p&gt;It took me about 2 minutes to write the query below that returns underpriced items:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/data_stack_11.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;br /&gt;Item Search and Dashboard:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;I was able to make a search screen where I can search by the year, sport and wildcard on the card&apos;s Description. Below I am searching for cards of Nolan Ryan from 1981 (the links take me to card details report):&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/data_stack_12.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The card details page below shows me:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Details about the card&lt;/li&gt;
&lt;li&gt;If I have it in my inventory, where I am selling it, the condition and the price&lt;/li&gt;
&lt;li&gt;Time Series of&amp;nbsp;Values (for graded and ungraded categories)&lt;/li&gt;
&lt;li&gt;Time Series of Volumes of sales with pricing&lt;/li&gt;
&lt;li&gt;Time Series of Volume of the item available (buy it now format)&lt;/li&gt;
&lt;li&gt;Where the item is available and for how much&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/data_stack_13.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h3&gt;Thoughts&lt;/h3&gt;
&lt;h4&gt;For me, this hackathon was a great success.&lt;strong&gt;&lt;br /&gt;&lt;br /&gt;&lt;/strong&gt;&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Metabase had made it easy to more effectively operationalize parts of this business with enhanced reporting and alerting.&lt;/li&gt;
&lt;li&gt;I now have a much easier way to answer my ad hoc and research questions quickly.&lt;/li&gt;
&lt;li&gt;I had the opportunity to play with a great new technology - DuckDB.&amp;nbsp; And I got to integrate it with other products I like (dbt, metabase, airbyte).&lt;/li&gt;
&lt;li&gt;This blog post does a decent job of documenting the problem, solution approach and some implementation details + learnings.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4&gt;Other Learnings / Observations:&lt;br /&gt;&lt;strong&gt;&lt;br /&gt;&lt;/strong&gt;&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;It is so much easier to manage running BI reporting off a columnar database vs. a relational database. DuckDB is pretty amazing for side projects if you can deal with the constraints.&amp;nbsp;&lt;/li&gt;
&lt;li&gt;I would consider adding in an orchestration layer with &lt;a href=&quot;https://dagster.io/&quot; target=&quot;_blank&quot;&gt;Dagster&lt;/a&gt;. I like the idea of having ingestion, transformation and stopping/starting services controlled by a consistent and testable process.&lt;/li&gt;
&lt;li&gt;For ingestion, I also considered using the Singer/Meltano&amp;nbsp;&lt;a href=&quot;https://github.com/jwills/target-duckdb&quot; target=&quot;_blank&quot;&gt;target-duckdb&lt;/a&gt;&amp;nbsp;project.&lt;/li&gt;
&lt;li&gt;I also played with &lt;a href=&quot;https://www.rilldata.com/&quot; target=&quot;_blank&quot;&gt;Rill Data&lt;/a&gt;. It is very cool for analyzing the structure of data files (csv, parquet, etc), but it can not read from DuckDB.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Inspiration from:&lt;/h4&gt;

&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://medium.com/datamindedbe/use-dbt-and-duckdb-instead-of-spark-in-data-pipelines-9063a31ea2b5&quot; target=&quot;_blank&quot;&gt;Use dbt and Duckdb instead of Spark in data pipelines&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://duckdb.org/2022/10/12/modern-data-stack-in-a-box.html&quot; target=&quot;_blank&quot;&gt;Modern Data Stack in a Box with DuckDB&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://dagster.io/blog/duckdb-data-lake&quot; target=&quot;_blank&quot;&gt;Build a poor man&amp;rsquo;s data lake from scratch with DuckDB&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Wed, 14 Jun 2023 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/modern-data-stack-in-a-box/</link>
        <guid isPermaLink="true">http://localhost:4000/modern-data-stack-in-a-box/</guid>
        
        <category>data engineering</category>
        
        <category>data warehosue</category>
        
        <category>dbt</category>
        
        <category>hackathon</category>
        
        <category>DuckDB</category>
        
        <category>Airbyte</category>
        
        <category>Metabase</category>
        
        
      </item>
    
      <item>
        <title>The 4 Analytics Questions of Subscription Ecommerce</title>
        <description>&lt;p&gt;I have spent over 20 years building my own subscription based service (&lt;a href=&quot;http://www.SportsCollectors.Net&quot; target=&quot;_blank&quot;&gt;SportsCollectors.Net&lt;/a&gt;) and working for companies with subscription offerings (&lt;a href=&quot;http://www.dell.com&quot; target=&quot;_blank&quot;&gt;Dell&lt;/a&gt;, &lt;a href=&quot;https://www.gettyimages.com/search/photographer?family=creative&amp;amp;photographer=jupiterimages&amp;amp;sort=best&quot; target=&quot;_blank&quot;&gt;Jupiterimages&lt;/a&gt;, &lt;a href=&quot;http://www.ww.com&quot; target=&quot;_blank&quot;&gt;Weight Watchers&lt;/a&gt;, &lt;a href=&quot;http://www.renttherunway.com&quot; target=&quot;_blank&quot;&gt;Rent the Runway&lt;/a&gt;,&amp;nbsp;&lt;a href=&quot;https://www.elysiumhealth.com/&quot; target=&quot;_blank&quot;&gt;ElysiumHelath&lt;/a&gt;). While the business models, value propositions and customer segments of these companies may be very different, there are similarities I recognized as these companies (all with product market fit) looked to accelerate their growth. From this, the 4 questions above are what analytics teams should spend resources trying to answer.&lt;/p&gt;

&lt;style&gt;
table.table_11 td  {
    font-size: 11pt;
    vertical-align: top;
    padding: 2px;
}
&lt;/style&gt;

&lt;p&gt;&lt;strong&gt;Please Note:&lt;/strong&gt;&lt;br /&gt;The order of these questions is in the context that you have an existing subscription based offering (with existing data available to analyze) you want to optimize+grow. If you are starting a new subscription offering/company, the order would likely be the opposite.&lt;/p&gt;

&lt;p&gt;So let&apos;s dig into each one of these topics:&lt;/p&gt;
&lt;h3&gt;1) What do you know about your valuable customers?&lt;/h3&gt;
&lt;p&gt;You are likely in business to serve your customers&apos; needs and/or deliver some value that they are comfortable (or even happy) to pay you for. It helps to understand who are those customers that are most happy and making you money. These folks tend to be your biggest ambassadors (promoters) and support your growth the most.&amp;nbsp;&lt;/p&gt;

&lt;h4&gt;Margin as a measure of value&lt;/h4&gt;
&lt;p&gt;I&apos;ve seen different definitions of &quot;Lifetime Value&quot;, many revolving around top line revenue. I personally like to start with looking at the amount and the details that make up the total margin a specific user represents to the business. So I start by building a ledger for each user showing the credits (increases to equity account) and debits (decreases to equity account).&lt;br /&gt;&lt;br /&gt;Let&apos;s look at a very simple ledger for a subscription for a content site:&lt;/p&gt;

&lt;table class=&quot;table_11&quot;&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Date&lt;/strong&gt;&lt;/td&gt;
&lt;td colspan=&quot;2&quot;&gt;&lt;strong&gt;Debit (money out)&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&amp;nbsp;&amp;nbsp;&lt;/td&gt;
&lt;td colspan=&quot;2&quot;&gt;&lt;strong&gt;Credit (money in)&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&amp;nbsp;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;Margin&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;12/03/2018&lt;/td&gt;
&lt;td&gt;Google AdWords CAC (campaign 892)&lt;/td&gt;
&lt;td&gt;$2.00&lt;/td&gt;
&lt;td&gt;&amp;nbsp;&amp;nbsp;&lt;/td&gt;
&lt;td&gt;&amp;nbsp;&lt;/td&gt;
&lt;td&gt;&amp;nbsp;&lt;/td&gt;
&lt;td&gt;&amp;nbsp;&lt;/td&gt;
&lt;td&gt;-$2.00&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;01/01/2019&amp;nbsp;&lt;/td&gt;
&lt;td&gt;&amp;nbsp;&lt;/td&gt;
&lt;td&gt;&amp;nbsp;&lt;/td&gt;
&lt;td&gt;&amp;nbsp;&amp;nbsp;&lt;/td&gt;
&lt;td&gt;Subscription (order 1987)&lt;/td&gt;
&lt;td&gt;$9.99&lt;/td&gt;
&lt;td&gt;&amp;nbsp;&lt;/td&gt;
&lt;td&gt;&amp;nbsp;$7.99&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;01/01/2019&amp;nbsp;&lt;/td&gt;
&lt;td&gt;Subscription promo (order 1987)&lt;/td&gt;
&lt;td&gt;$9.00&lt;/td&gt;
&lt;td&gt;&amp;nbsp;&amp;nbsp;&lt;/td&gt;
&lt;td&gt;&amp;nbsp;&lt;/td&gt;
&lt;td&gt;&amp;nbsp;&lt;/td&gt;
&lt;td&gt;&amp;nbsp;&lt;/td&gt;
&lt;td&gt;-$1.01&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;01/03/2019&amp;nbsp;&lt;/td&gt;
&lt;td&gt;Monthly operations cost&lt;/td&gt;
&lt;td&gt;$0.14&lt;/td&gt;
&lt;td&gt;&amp;nbsp;&amp;nbsp;&lt;/td&gt;
&lt;td&gt;&amp;nbsp;&lt;/td&gt;
&lt;td&gt;&amp;nbsp;&lt;/td&gt;
&lt;td&gt;&amp;nbsp;&lt;/td&gt;
&lt;td&gt;-$1.15&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;01/03/2019&amp;nbsp;&lt;/td&gt;
&lt;td&gt;&amp;nbsp;&lt;/td&gt;
&lt;td&gt;&amp;nbsp;&lt;/td&gt;
&lt;td&gt;&amp;nbsp;&amp;nbsp;&lt;/td&gt;
&lt;td&gt;Monthly affiliate revenue&lt;/td&gt;
&lt;td&gt;$0.37&lt;/td&gt;
&lt;td&gt;&amp;nbsp;&lt;/td&gt;
&lt;td&gt;-$0.78&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;02/01/2019&amp;nbsp;&lt;/td&gt;
&lt;td&gt;&amp;nbsp;&lt;/td&gt;
&lt;td&gt;&amp;nbsp;&lt;/td&gt;
&lt;td&gt;&amp;nbsp;&amp;nbsp;&lt;/td&gt;
&lt;td&gt;Subscription Renewal - Term 2&lt;/td&gt;
&lt;td&gt;$9.99&lt;/td&gt;
&lt;td&gt;&amp;nbsp;&lt;/td&gt;
&lt;td&gt;&amp;nbsp;$9.21&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;02/03/2019&amp;nbsp;&lt;/td&gt;
&lt;td&gt;Monthly operations cost&lt;/td&gt;
&lt;td&gt;$0.14&lt;/td&gt;
&lt;td&gt;&amp;nbsp;&amp;nbsp;&lt;/td&gt;
&lt;td&gt;&amp;nbsp;&lt;/td&gt;
&lt;td&gt;&amp;nbsp;&lt;/td&gt;
&lt;td&gt;&amp;nbsp;&lt;/td&gt;
&lt;td&gt;&amp;nbsp;$9.07&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;02/03/2019&amp;nbsp;&lt;/td&gt;
&lt;td&gt;&amp;nbsp;&lt;/td&gt;
&lt;td&gt;&amp;nbsp;&lt;/td&gt;
&lt;td&gt;&amp;nbsp;&amp;nbsp;&lt;/td&gt;
&lt;td&gt;Monthly affiliate revenue&lt;/td&gt;
&lt;td&gt;$1.28&lt;/td&gt;
&lt;td&gt;&amp;nbsp;&lt;/td&gt;
&lt;td&gt;&amp;nbsp;$10.35&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;03/01/2019&amp;nbsp;&lt;/td&gt;
&lt;td&gt;&amp;nbsp;&lt;/td&gt;
&lt;td&gt;&amp;nbsp;&lt;/td&gt;
&lt;td&gt;&amp;nbsp;&amp;nbsp;&lt;/td&gt;
&lt;td&gt;Subscription Renewal - Term 3&lt;/td&gt;
&lt;td&gt;$9.99&lt;/td&gt;
&lt;td&gt;&amp;nbsp;&lt;/td&gt;
&lt;td&gt;&amp;nbsp;$20.34&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;03/02/2019&amp;nbsp;&lt;/td&gt;
&lt;td&gt;&amp;nbsp;&lt;/td&gt;
&lt;td&gt;&amp;nbsp;&lt;/td&gt;
&lt;td&gt;&amp;nbsp;&amp;nbsp;&lt;/td&gt;
&lt;td&gt;Pay Per View Event (order 2508)&lt;/td&gt;
&lt;td&gt;$3.99&lt;/td&gt;
&lt;td&gt;&amp;nbsp;&lt;/td&gt;
&lt;td&gt;&amp;nbsp;$24.33&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;03/02/2019&amp;nbsp;&lt;/td&gt;
&lt;td&gt;Royalty for Pay Per View Event&lt;/td&gt;
&lt;td&gt;$1.00&lt;/td&gt;
&lt;td&gt;&amp;nbsp;&amp;nbsp;&lt;/td&gt;
&lt;td&gt;&amp;nbsp;&lt;/td&gt;
&lt;td&gt;&amp;nbsp;&lt;/td&gt;
&lt;td&gt;&amp;nbsp;&lt;/td&gt;
&lt;td&gt;&amp;nbsp;$23.33&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;03/03/2019&amp;nbsp;&lt;/td&gt;
&lt;td&gt;Monthly operations cost&lt;/td&gt;
&lt;td&gt;$0.15&lt;/td&gt;
&lt;td&gt;&amp;nbsp;&amp;nbsp;&lt;/td&gt;
&lt;td&gt;&amp;nbsp;&lt;/td&gt;
&lt;td&gt;&amp;nbsp;&lt;/td&gt;
&lt;td&gt;&amp;nbsp;&lt;/td&gt;
&lt;td&gt;&amp;nbsp;$23.18&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;03/03/2019&amp;nbsp;&lt;/td&gt;
&lt;td&gt;&amp;nbsp;&lt;/td&gt;
&lt;td&gt;&amp;nbsp;&lt;/td&gt;
&lt;td&gt;&amp;nbsp;&amp;nbsp;&lt;/td&gt;
&lt;td&gt;Monthly affiliate revenue&lt;/td&gt;
&lt;td&gt;$2.31&lt;/td&gt;
&lt;td&gt;&amp;nbsp;&lt;/td&gt;
&lt;td&gt;&amp;nbsp;$25.49&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;03/15/2019&amp;nbsp;&lt;/td&gt;
&lt;td&gt;Mid-term cancel - partial refund&lt;/td&gt;
&lt;td&gt;$5.00&lt;/td&gt;
&lt;td&gt;&amp;nbsp;&amp;nbsp;&lt;/td&gt;
&lt;td&gt;&amp;nbsp;&lt;/td&gt;
&lt;td&gt;&amp;nbsp;&lt;/td&gt;
&lt;td&gt;&amp;nbsp;&lt;/td&gt;
&lt;td&gt;&amp;nbsp;$20.49&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;Totals&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&amp;nbsp;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;$17.43&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&amp;nbsp;&amp;nbsp;&lt;/td&gt;
&lt;td&gt;&amp;nbsp;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;$37.92&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;&amp;nbsp;&lt;/td&gt;
&lt;td&gt;&amp;nbsp;&lt;strong style=&quot;color: #ff0000;&quot;&gt;$20.49&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;br /&gt;There&apos;s some interesting questions looking at this kind of data can unearth:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;How does this user&apos;s journal entries compare to others?&amp;nbsp;&lt;/li&gt;
&lt;li&gt;Was their desirable ROI on the $2.00 AdWords attribution?&lt;/li&gt;
&lt;li&gt;Was offering a $9.00 signup promo worthwhile (we won&apos;t see margin on user until second month)?&amp;nbsp;&lt;/li&gt;
&lt;li&gt;Since this user&apos;s affiliate revenue is increasing and they bought a Pay Per View Event, should we incentivize renewal?&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;What do your users tell you?&lt;/h4&gt;

&lt;div&gt;Many companies may ask their users to provide details about themselves with the hope of using this info to provide a better user experience. Here are some examples of the types of industry specific profile information:&lt;/div&gt;
&lt;div&gt;&amp;nbsp;&lt;/div&gt;
&lt;div&gt;
&lt;ul&gt;
&lt;li&gt;WeightWatchers has a goal of helping members get healthier - so it may ask about current weight, weight loss goals, lifestyle or dietary preferences, age, location.&lt;/li&gt;
&lt;li&gt;Rent the Runway has a goal of helping members enjoy fashion options - so it may ask about body type, height, birthdate, location, style preferences.&lt;/li&gt;
&lt;li&gt;EHarmony has the goal of matching users for relationships - so it has users spend 45 minutes to fill a&amp;nbsp;&lt;a href=&quot;https://www.eharmony.com/tour/what-is-the-compatibility-quiz/&quot; target=&quot;_blank&quot;&gt;lengthy questionnaire&lt;/a&gt;&amp;nbsp;about their lifestyles and personal preferences.&amp;nbsp;&amp;nbsp;&lt;/li&gt;
&lt;li&gt;SportsCollectors.net has the goal of helping members enjoy collecting sports autographs - so it may ask about want lists, favorite players/teams, what items they have for trade/sale, their ebay username, year of birth, location.&amp;nbsp;&amp;nbsp;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;h4 id=&quot;user_journey&quot;&gt;Do your users&apos; actions tell you things?&lt;/h4&gt;
&lt;p&gt;In addition to the margin profile, I have also invested time in building a user journey (now being marketed as the&amp;nbsp;&lt;a href=&quot;https://www.activityschema.com/&quot; target=&quot;_blank&quot;&gt;Activity Schema&lt;/a&gt;) -&amp;nbsp; which is a time series of all the actions and communications with this subscriber. A user journey can be represented with the following data elements:&lt;/p&gt;
&lt;table class=&quot;table_11&quot;&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;UserID&lt;/strong&gt;&amp;nbsp;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;Timestamp&lt;/strong&gt;&amp;nbsp;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;User_Event&lt;/strong&gt;&amp;nbsp;&lt;/td&gt;
&lt;td&gt;&lt;strong&gt;Payload&lt;/strong&gt;&amp;nbsp;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td valign=&quot;top&quot;&gt;12345&amp;nbsp;&lt;/td&gt;
&lt;td valign=&quot;top&quot;&gt;01/01/2019 00:00:00&amp;nbsp;&lt;/td&gt;
&lt;td valign=&quot;top&quot;&gt;PAGE_LOAD&amp;nbsp;&lt;/td&gt;
&lt;td&gt;{&quot;device_type&quot;: &quot;desktop&quot;, &quot;marketing_channel&quot;: &quot;google_sem&quot;, &quot;url&quot;: &quot;http://www.hello.com/about-us/?utm=sem_plan1&quot;}&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td valign=&quot;top&quot;&gt;12345&amp;nbsp;&lt;/td&gt;
&lt;td valign=&quot;top&quot;&gt;01/01/2019 00:01:00&amp;nbsp;&lt;/td&gt;
&lt;td valign=&quot;top&quot;&gt;PAGE_LOAD&amp;nbsp;&lt;/td&gt;
&lt;td&gt;{&quot;device_type&quot;: &quot;desktop&quot;, &quot;url&quot;: &quot;http://www.hello.com/plans&quot;}&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td valign=&quot;top&quot;&gt;12345&amp;nbsp;&lt;/td&gt;
&lt;td valign=&quot;top&quot;&gt;01/01/2019 00:02:00&amp;nbsp;&lt;/td&gt;
&lt;td valign=&quot;top&quot;&gt;PAGE_LOAD&amp;nbsp;&lt;/td&gt;
&lt;td&gt;{&quot;device_type&quot;: &quot;desktop&quot;, &quot;url&quot;: &quot;http://www.hello.com/register&quot;}&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td valign=&quot;top&quot;&gt;12345&amp;nbsp;&lt;/td&gt;
&lt;td valign=&quot;top&quot;&gt;01/01/2019 00:03:00&amp;nbsp;&lt;/td&gt;
&lt;td valign=&quot;top&quot;&gt;PAGE_LOAD&amp;nbsp;&lt;/td&gt;
&lt;td&gt;{&quot;device_type&quot;: &quot;desktop&quot;, &quot;url&quot;: &quot;http://www.hello.com/login&quot;}&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td valign=&quot;top&quot;&gt;12345&amp;nbsp;&lt;/td&gt;
&lt;td valign=&quot;top&quot;&gt;01/01/2019 00:04:00&amp;nbsp;&lt;/td&gt;
&lt;td valign=&quot;top&quot;&gt;PAGE_LOAD&amp;nbsp;&lt;/td&gt;
&lt;td&gt;{&quot;device_type&quot;: &quot;desktop&quot;, &quot;url&quot;: &quot;http://www.hello.com/subscribe&quot;}&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td valign=&quot;top&quot;&gt;12345&amp;nbsp;&lt;/td&gt;
&lt;td valign=&quot;top&quot;&gt;01/01/2019 00:04:00&amp;nbsp;&lt;/td&gt;
&lt;td valign=&quot;top&quot;&gt;PLACE_ORDER&amp;nbsp;&lt;/td&gt;
&lt;td&gt;{&quot;order_id&quot;: 1987, &quot;order_status&quot;: &quot;payment_received&quot;, &quot;order_type&quot;: &quot;SUBSCRIPTION&quot;, &quot;amount&quot;: &quot;0.99&quot;}&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td valign=&quot;top&quot;&gt;12345&amp;nbsp;&lt;/td&gt;
&lt;td valign=&quot;top&quot;&gt;01/01/2019 00:07:00&amp;nbsp;&lt;/td&gt;
&lt;td valign=&quot;top&quot;&gt;RECEIVE_EMAIL&amp;nbsp;&lt;/td&gt;
&lt;td&gt;{&quot;template&quot;: &quot;201900101_new_years_cohort&quot;, &quot;list_name&quot;: &quot;hello subscribers&quot;}&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td valign=&quot;top&quot;&gt;12345&amp;nbsp;&lt;/td&gt;
&lt;td valign=&quot;top&quot;&gt;01/01/2019 00:015:00&amp;nbsp;&lt;/td&gt;
&lt;td valign=&quot;top&quot;&gt;OPEN_EMAIL&amp;nbsp;&lt;/td&gt;
&lt;td&gt;{&quot;template&quot;: &quot;201900101_new_years_cohort&quot;, &quot;list_name&quot;: &quot;hello subscribers&quot;}&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td valign=&quot;top&quot;&gt;12345&amp;nbsp;&lt;/td&gt;
&lt;td valign=&quot;top&quot;&gt;01/01/2019 00:16:00&amp;nbsp;&lt;/td&gt;
&lt;td valign=&quot;top&quot;&gt;PAGE_LOAD&amp;nbsp;&lt;/td&gt;
&lt;td&gt;{&quot;device_type&quot;: &quot;mobile&quot;, &quot;url&quot;: &quot;http://www.hello.com/article/hot-investments&quot;}&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td valign=&quot;top&quot;&gt;12345&amp;nbsp;&lt;/td&gt;
&lt;td valign=&quot;top&quot;&gt;01/01/2019 00:21:00&amp;nbsp;&lt;/td&gt;
&lt;td valign=&quot;top&quot;&gt;PAGE_LOAD&amp;nbsp;&lt;/td&gt;
&lt;td&gt;{&quot;device_type&quot;: &quot;mobile&quot;, &quot;url&quot;: &quot;http://www.hello.com/article/trending-for-2019&quot;}&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;br /&gt;This structure allows us to define multiple types of events across our system and the relevant details about each as&amp;nbsp;&lt;a href=&quot;https://en.wikipedia.org/wiki/JSON&quot; target=&quot;_blank&quot;&gt;JSON&lt;/a&gt;&amp;nbsp;in the &quot;Payload&quot; field.&lt;/p&gt;
&lt;p&gt;In order to achieve this, we need to have the ability to track what different users are doing in our system. We have a service that we call to log our users&apos; actions to get the PAGE_LOAD events (there are services like&amp;nbsp;&lt;a href=&quot;https://snowplowanalytics.com/&quot; target=&quot;_blank&quot;&gt;snowplow&lt;/a&gt;,&amp;nbsp;&lt;a href=&quot;https://segment.com/&quot; target=&quot;_blank&quot;&gt;segment&lt;/a&gt;&amp;nbsp;or&amp;nbsp;&lt;a href=&quot;https://analytics.google.com/analytics/web/&quot;&gt;google analytics&lt;/a&gt;&amp;nbsp;for this), we get the PLACE_ORDER events from the order table in our database, and we get EMAIL related information from our email vendor (&lt;a href=&quot;https://mailchimp.com/&quot; target=&quot;_blank&quot;&gt;mailchimp&lt;/a&gt;,&amp;nbsp;&lt;a href=&quot;https://www.sailthru.com&quot; target=&quot;_blank&quot;&gt;sailthru&lt;/a&gt;,&amp;nbsp;&lt;a href=&quot;https://www.cheetahdigital.com&quot; target=&quot;_blank&quot;&gt;cheetahmail&lt;/a&gt;&amp;nbsp;do this kind of thing).&lt;/p&gt;
&lt;p&gt;This very simple example shows that User 12345 has 10 events associated with them:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;He arrives to the site from a google paid search. We can use this to attribute our marketing acquisition cost.&lt;/li&gt;
&lt;li&gt;He views our plans page, then registers for an account, then logs in, and then places an order all on a desktop browser. Seeing the steps taken before an order is placed allows us understand our conversion funnel.&lt;/li&gt;
&lt;li&gt;Our system sends email (through an email vendor) and can track if it is opened or clicked on.&lt;/li&gt;
&lt;li&gt;He reads 2 article pages on their mobile device.&lt;/li&gt;
&lt;/ul&gt;
&lt;div&gt;A real user journey may have many thousands of actions spanning across many visits/sessions from different devices/channels. This can allow us to see how often they interact with our system and how their behavior changes over time.&lt;br /&gt;&lt;br /&gt;&lt;/div&gt;
&lt;h3&gt;Margin + Profile Info + Actions == Potential for Analysis&lt;/h3&gt;
&lt;p&gt;When we combine the margin of a user, their profile information, and the actions in their user journey, we can:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Find who are the high margin users and what makes them so. Develop understanding what actions and features drive higher margins.&amp;nbsp;&lt;/li&gt;
&lt;li&gt;Find out what our members like and want. Develop understanding of their interests across segments of our membership. Understand how should we communicate with them - (effectiveness of email, chat, customer service, retail channels).&lt;/li&gt;
&lt;li&gt;Develop features/content (and possibly create experiments) to further satisfy the user. This can help support goals for longer retention or converting on up-sell opportunities.&lt;/li&gt;
&lt;li&gt;Find out how to describe and categorize our users. Does their profile and actions allow us to classify them into naturally forming groups?&lt;/li&gt;
&lt;li&gt;Try to make our offerings more attractive to future users. What are common paths for conversion and what types of folks convert quickly, slowly, not at all. Where do users get stuck and what actions have worked to de-risk these scenarios.&lt;/li&gt;
&lt;li&gt;Find out how our users at different margin levels find us. This can help optimize our marketing/awareness efforts.&lt;/li&gt;
&lt;li&gt;Understand what happens when we make changes to the offering - such as changing features, content, messaging, pricing, support options, etc.?&amp;nbsp;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;2) How can you keep customers coming back?&lt;/h3&gt;
&lt;div&gt;&amp;nbsp;&lt;/div&gt;
&lt;div&gt;
&lt;div&gt;For subscription based business models, how much users continue to pay for the subscription (also known as retention) is a key concern because:&lt;/div&gt;
&lt;div&gt;&amp;nbsp;&lt;/div&gt;
&lt;div&gt;
&lt;ul&gt;
&lt;li&gt;Assuming that your offering can be profitable (subscription fees are higher than your costs), your revenues and margins scale linearly to the number of subscription terms that users pay for. So figuring out how to maximize retention will increase your earnings.&amp;nbsp;&lt;/li&gt;
&lt;li&gt;It is almost always cheaper to retain current customers than to acquire new ones.&amp;nbsp;&lt;br /&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/4_questions_1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;h4&gt;Measuring retention?&lt;/h4&gt;
&lt;div&gt;Retention rate (as defined by&amp;nbsp;&lt;a href=&quot;https://en.wikipedia.org/wiki/Retention_rate&quot; target=&quot;_blank&quot;&gt;Wikipedia&lt;/a&gt;)&amp;nbsp;is &quot;the ratio of the number of retained customers to the number at risk&quot;. As an example: if you have 1,000 subscribers in term 1 and 950 of those same users are still active in term 2, then your retention rate for that period is 950/1000=0.95 or 95%.&lt;/div&gt;
&lt;div&gt;&amp;nbsp;&lt;/div&gt;
&lt;div&gt;Churn rate (another popular metric) is the inverse of retention, is &quot;the percentage of service subscribers who discontinue their subscriptions within a given time period&quot;. Back to our example: if you have 1,000 subscribers in term 1 and 950 of those same users are still active in term 2, then your churn rate for that period is (1000-950)/1000=0.05 or 5%.&lt;/div&gt;
&lt;div&gt;&amp;nbsp;&lt;/div&gt;
&lt;div&gt;Retention rate for a subscription offering is a proxy metric many folks use to understand its business health and investors use it to compare companies.&lt;br /&gt;&lt;br /&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;h4&gt;Visualizing retention rates by cohorts&lt;/h4&gt;
&lt;p&gt;The most common way people seems to look at retention rate is to segment into related groups - known as cohorts. Each person in a cohort must share a related yet distinguishable trait that separates them from the other cohorts. In these examples, our cohorts will be based on the month that users starts their subscriptions.&lt;/p&gt;
&lt;p&gt;Aaron Chantiles has done a nice job creating these&amp;nbsp;&amp;nbsp;&lt;a href=&quot;https://blog.usejournal.com/how-to-perform-cohort-analysis-calculate-customer-ltv-in-excel-80bfed785ec4&quot; target=&quot;_blank&quot;&gt;3 cohort model reports&lt;/a&gt;:&lt;/p&gt;
&lt;p&gt;1) Survival Analysis - for each month term we can see the percentage of the original users in that cohort that are still active.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/4_questions_2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;2) Average Revenue per User - for each month term we can see the average revenue of the active users in that cohort. (this could be margin instead)&lt;br /&gt;&lt;em&gt;Note: Big variances from month to month likely results from introducing major changes to your subscription plan, adding some great new features, breaking something, or big impact from seasonality.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/4_questions_3.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;3) Total Revenue by Cohort - for each month term we can see the total cumulative revenue of the active users in that cohort. (this could be margin instead)&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/4_questions_4.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h4&gt;Understanding and Predicting user retention&lt;/h4&gt;
&lt;p&gt;KEY QUESTION - If we can somehow experience or &apos;learn&apos; how users have behaved in the past (those who churned and those who stayed), then can we can predict how your current users will behave in the future?&lt;/p&gt;
&lt;p&gt;We can try to leverage our user profile information, ledger, and user journey data sets to discover things about our users and how they behaved. We&apos;ll start by defining some variables (features) that we think may contribute to retention for a subscription content site:&lt;/p&gt;
&lt;table border=&quot;1&quot; cellspacing=&quot;0&quot; cellpadding=&quot;0&quot;&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&quot;padding: 3px;&quot; valign=&quot;top&quot;&gt;&lt;strong&gt;metric&lt;/strong&gt;&lt;/td&gt;
&lt;td style=&quot;padding: 3px;&quot; valign=&quot;top&quot;&gt;&lt;strong&gt;type&lt;/strong&gt;&lt;/td&gt;
&lt;td style=&quot;padding: 3px;&quot; valign=&quot;top&quot;&gt;&lt;strong&gt;source&lt;/strong&gt;&lt;/td&gt;
&lt;td style=&quot;padding: 3px;&quot; valign=&quot;top&quot;&gt;&lt;strong&gt;description&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&quot;padding: 3px;&quot; valign=&quot;top&quot;&gt;median_income&lt;/td&gt;
&lt;td style=&quot;padding: 3px;&quot; valign=&quot;top&quot;&gt;int&lt;/td&gt;
&lt;td style=&quot;padding: 3px;&quot; valign=&quot;top&quot;&gt;profile&lt;/td&gt;
&lt;td style=&quot;padding: 3px;&quot; valign=&quot;top&quot;&gt;we can join user&apos;s location on census data&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&quot;padding: 3px;&quot; valign=&quot;top&quot;&gt;population_density&lt;/td&gt;
&lt;td style=&quot;padding: 3px;&quot; valign=&quot;top&quot;&gt;int&lt;/td&gt;
&lt;td style=&quot;padding: 3px;&quot; valign=&quot;top&quot;&gt;profile&lt;/td&gt;
&lt;td style=&quot;padding: 3px;&quot; valign=&quot;top&quot;&gt;we can join location on census data&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&quot;padding: 3px;&quot; valign=&quot;top&quot;&gt;user_age&lt;/td&gt;
&lt;td style=&quot;padding: 3px;&quot; valign=&quot;top&quot;&gt;int&lt;/td&gt;
&lt;td style=&quot;padding: 3px;&quot; valign=&quot;top&quot;&gt;profile&lt;/td&gt;
&lt;td style=&quot;padding: 3px;&quot; valign=&quot;top&quot;&gt;age in full years&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&quot;padding: 3px;&quot; valign=&quot;top&quot;&gt;first_month_price&lt;/td&gt;
&lt;td style=&quot;padding: 3px;&quot; valign=&quot;top&quot;&gt;int&lt;/td&gt;
&lt;td style=&quot;padding: 3px;&quot; valign=&quot;top&quot;&gt;ledger&lt;/td&gt;
&lt;td style=&quot;padding: 3px;&quot; valign=&quot;top&quot;&gt;amount of first month cost (to determine impact of promos)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&quot;padding: 3px;&quot; valign=&quot;top&quot;&gt;customer_acquisition_cost&lt;/td&gt;
&lt;td style=&quot;padding: 3px;&quot; valign=&quot;top&quot;&gt;int&lt;/td&gt;
&lt;td style=&quot;padding: 3px;&quot; valign=&quot;top&quot;&gt;ledger&lt;/td&gt;
&lt;td style=&quot;padding: 3px;&quot; valign=&quot;top&quot;&gt;amount of acquisition cost (to determine impact of paid marketing sources vs. organic)&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&quot;padding: 3px;&quot; valign=&quot;top&quot;&gt;total_margin&lt;/td&gt;
&lt;td style=&quot;padding: 3px;&quot; valign=&quot;top&quot;&gt;int&lt;/td&gt;
&lt;td style=&quot;padding: 3px;&quot; valign=&quot;top&quot;&gt;ledger&lt;/td&gt;
&lt;td style=&quot;padding: 3px;&quot; valign=&quot;top&quot;&gt;total margin the user has contributed to the company&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&quot;padding: 3px;&quot; valign=&quot;top&quot;&gt;searches_count&lt;/td&gt;
&lt;td style=&quot;padding: 3px;&quot; valign=&quot;top&quot;&gt;int&lt;/td&gt;
&lt;td style=&quot;padding: 3px;&quot; valign=&quot;top&quot;&gt;journey&lt;/td&gt;
&lt;td style=&quot;padding: 3px;&quot; valign=&quot;top&quot;&gt;the number of searches they performed&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&quot;padding: 3px;&quot; valign=&quot;top&quot;&gt;visit_count&lt;/td&gt;
&lt;td style=&quot;padding: 3px;&quot; valign=&quot;top&quot;&gt;int&lt;/td&gt;
&lt;td style=&quot;padding: 3px;&quot; valign=&quot;top&quot;&gt;journey&lt;/td&gt;
&lt;td style=&quot;padding: 3px;&quot; valign=&quot;top&quot;&gt;the number of times they visit&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&quot;padding: 3px;&quot; valign=&quot;top&quot;&gt;articles_per_visit&lt;/td&gt;
&lt;td style=&quot;padding: 3px;&quot; valign=&quot;top&quot;&gt;decimal&lt;/td&gt;
&lt;td style=&quot;padding: 3px;&quot; valign=&quot;top&quot;&gt;journey&lt;/td&gt;
&lt;td style=&quot;padding: 3px;&quot; valign=&quot;top&quot;&gt;the number of articles they view per visit&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&quot;padding: 3px;&quot; valign=&quot;top&quot;&gt;views_per_article&lt;/td&gt;
&lt;td style=&quot;padding: 3px;&quot; valign=&quot;top&quot;&gt;decimal&lt;/td&gt;
&lt;td style=&quot;padding: 3px;&quot; valign=&quot;top&quot;&gt;journey&lt;/td&gt;
&lt;td style=&quot;padding: 3px;&quot; valign=&quot;top&quot;&gt;average number of times they visit the same article&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&quot;padding: 3px;&quot; valign=&quot;top&quot;&gt;favorite_count&lt;/td&gt;
&lt;td style=&quot;padding: 3px;&quot; valign=&quot;top&quot;&gt;int&lt;/td&gt;
&lt;td style=&quot;padding: 3px;&quot; valign=&quot;top&quot;&gt;journey&lt;/td&gt;
&lt;td style=&quot;padding: 3px;&quot; valign=&quot;top&quot;&gt;number of times they favorite content&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&quot;padding: 3px;&quot; valign=&quot;top&quot;&gt;recommendation_count&lt;/td&gt;
&lt;td style=&quot;padding: 3px;&quot; valign=&quot;top&quot;&gt;int&lt;/td&gt;
&lt;td style=&quot;padding: 3px;&quot; valign=&quot;top&quot;&gt;journey&lt;/td&gt;
&lt;td style=&quot;padding: 3px;&quot; valign=&quot;top&quot;&gt;number of times they recommend/share content&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&quot;padding: 3px;&quot; valign=&quot;top&quot;&gt;non_subscription_revenue&lt;/td&gt;
&lt;td style=&quot;padding: 3px;&quot; valign=&quot;top&quot;&gt;decimal&lt;/td&gt;
&lt;td style=&quot;padding: 3px;&quot; valign=&quot;top&quot;&gt;ledger&lt;/td&gt;
&lt;td style=&quot;padding: 3px;&quot; valign=&quot;top&quot;&gt;amount of non-subscription revenue&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&quot;padding: 3px;&quot; valign=&quot;top&quot;&gt;term_number&lt;/td&gt;
&lt;td style=&quot;padding: 3px;&quot; valign=&quot;top&quot;&gt;int&lt;/td&gt;
&lt;td style=&quot;padding: 3px;&quot; valign=&quot;top&quot;&gt;journey&lt;/td&gt;
&lt;td style=&quot;padding: 3px;&quot; valign=&quot;top&quot;&gt;number of terms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&quot;padding: 3px;&quot; valign=&quot;top&quot;&gt;is_churn_next_term&lt;/td&gt;
&lt;td style=&quot;padding: 3px;&quot; valign=&quot;top&quot;&gt;boolean&lt;/td&gt;
&lt;td style=&quot;padding: 3px;&quot; valign=&quot;top&quot;&gt;journey&lt;/td&gt;
&lt;td style=&quot;padding: 3px;&quot; valign=&quot;top&quot;&gt;did the user churn in the next term&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&amp;nbsp;&lt;br /&gt;Once we build a nice data set, we can do some data exploration.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;We can run queries on the top 50-100 highest margin users and see visually view if there obvious patterns that can guide deeper exploration.&amp;nbsp;&lt;/li&gt;
&lt;li&gt;If we see that these folks read lots of content, come in with low customer_acquisition_cost,&amp;nbsp;and/or produce more non-subscription revenue - then we can dig on specifics of the behaviors related users achieving those actions/milestones.&amp;nbsp;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We can plot each of the metrics against number of terms and see which ones correlate well.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;In the past I have built machine learning models to try to identify what variables drive retention and show us the probability that a user is going to churn from their membership in upcoming terms (a.k.a . Churn model). As you can probably imagine, there is a substantial amount of data engineering work (data capture, feature definition, cleanup/transformation/standardization/regularization/labeling, validation) needed and lots of data exploration before we consider running models.&lt;/p&gt;
&lt;p&gt;I may devote a full future blog post to this, but for now I&apos;ll share some outside blog posts with helpful approaches along with python examples how to do this.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;My preference is try to calculate a Subscriber Fragility Score that indicates the likeliness that a subscriber with churn in the upcoming billing term. Many folks use&amp;nbsp; regression and classification models to do this:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://365datascience.com/tutorials/python-tutorials/how-to-build-a-customer-churn-prediction-model-in-python/&quot; target=&quot;_blank&quot;&gt;https://towardsdatascience.com/predict-customer-churn-in-python-e8cd6d3aaa7&lt;br /&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://365datascience.com/tutorials/python-tutorials/how-to-build-a-customer-churn-prediction-model-in-python/&quot; target=&quot;_blank&quot;&gt;https://365datascience.com/tutorials/python-tutorials/how-to-build-a-customer-churn-prediction-model-in-python/&lt;br /&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://neptune.ai/blog/how-to-implement-customer-churn-prediction&quot; target=&quot;_blank&quot;&gt;https://neptune.ai/blog/how-to-implement-customer-churn-prediction&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div&gt;&amp;nbsp;Another approach is to try to predict how long the subscriber will remain with the subscription via Survivor Analysis:&lt;/div&gt;
&lt;div&gt;&amp;nbsp;&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://square.github.io/pysurvival/tutorials/churn.html&quot; target=&quot;_blank&quot;&gt;https://square.github.io/pysurvival/tutorials/churn.html&lt;br /&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&quot;https://thedatascientist.com/customer-churn-machine-learning-data-science-survival-analysis/&quot; target=&quot;_blank&quot;&gt;https://thedatascientist.com/customer-churn-machine-learning-data-science-survival-analysis/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div&gt;I also enjoyed this blog post &quot;Top Ten Mistakes Data Scientists Make While Building Churn Models&quot;:&lt;/div&gt;
&lt;div&gt;&amp;nbsp;&lt;/div&gt;
&lt;div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&quot;https://medium.com/@swansburg.justin/top-ten-mistakes-data-scientists-make-while-building-churn-models-d773bb7deaa5&quot; target=&quot;_blank&quot;&gt;https://medium.com/@swansburg.justin/top-ten-mistakes-data-scientists-make-while-building-churn-models-d773bb7deaa5&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;h3&gt;3) How can you get new visitors to buy things and become customers?&lt;/h3&gt;
&lt;p&gt;This area is a huge focus in many companies and I know quite a few Product Managers who have over a decade each dedicated to working on incrementally improving buyer experiences and getting better conversion. In all those cases, they heavily rely on data (and analysts) to help guide their activites.&lt;/p&gt;
&lt;h4&gt;Let&apos;s start with some questions&amp;nbsp;&lt;/h4&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;1. What is the value proposition of the subscription product? What is the problem that we think it solves and for what groups of people?&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;There are many approaches and tools analysts can use to define a product&apos;s value proposition to users.&lt;/p&gt;
&lt;p&gt;Peter Thompson introduces a diagram view that&amp;nbsp;&lt;a href=&quot;https://www.peterjthomson.com/2013/11/value-proposition-canvas/&quot; target=&quot;_blank&quot;&gt;he calls the Value Proposition Canvas&lt;/a&gt;. He defines as:&lt;/p&gt;
&lt;p style=&quot;padding-left: 30px;&quot;&gt;A value proposition is the place where your company&amp;rsquo;s product intersects with your customer&amp;rsquo;s desires. It&amp;rsquo;s the magic fit between&amp;nbsp;&lt;strong&gt;what&lt;/strong&gt;&amp;nbsp;you make and&amp;nbsp;&lt;strong&gt;why&lt;/strong&gt;&amp;nbsp;people buy it. Your value proposition is the crunch point between business strategy and brand strategy.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/4_questions_5.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;Below is an example illustration for an angel investing syndicate with a co-working space for the member investors and their portfolio of startups.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/4_questions_6.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I&apos;d want to try to quantify the key elements of the value proposition, specifically from the buyer&apos;s perspective (wants, needs, fears and substitutes). We can try to collect factual information, statistics, or research results from credible external sources that support the value proposition. This could include industry expert opinions, awards, or mentions, as well as documented improvements or reductions in attributes like revenue or costs.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;2. How can a user purchase a subscription? More specifically what is the process users generally take to make purchases and what is the messaging about the product?&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;A subscription funnel is an analytical method used to analyze the sequence of events&amp;nbsp;in a subscription-based offering.&amp;nbsp;&amp;nbsp;Funnel analysis tracks user behavior throughout their user journey and calculates how many people make it through each step,&amp;nbsp;allowing marketers / product managers to optimize their strategies and improve user experiences, hopefully leading to subscription growth.&lt;/p&gt;
&lt;p&gt;Typical steps in a subscription funnel for a website can be broken down into the following stages:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;span style=&quot;text-decoration: underline;&quot;&gt;Awareness&lt;/span&gt;: The user becomes aware of the website or service. This can include visiting the website, receiving an email, receiving a referral or seeing an advertisement.&lt;/li&gt;
&lt;li&gt;&lt;span style=&quot;text-decoration: underline;&quot;&gt;Interest&lt;/span&gt;: The user shows interest in the service by signing up for a newsletter, registering for a free trial, or subscribing to receive updates.&lt;/li&gt;
&lt;li style=&quot;box-sizing: border-box;&quot;&gt;&lt;span style=&quot;text-decoration: underline;&quot;&gt;Evaluation&lt;/span&gt;: The user evaluates the service by exploring features, reading reviews, or comparing it with competitors.&lt;/li&gt;
&lt;li style=&quot;box-sizing: border-box;&quot;&gt;&lt;span style=&quot;text-decoration: underline;&quot;&gt;Conversion&lt;/span&gt;: The user decides to subscribe or purchase the service.&lt;/li&gt;
&lt;li style=&quot;box-sizing: border-box;&quot;&gt;&lt;span style=&quot;text-decoration: underline;&quot;&gt;Checkout&lt;/span&gt;: The process and/or steps a user takes to subscribe to the service and make payment.&amp;nbsp;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Being able to leverage each action and signal in the&amp;nbsp;&lt;a href=&quot;#user_journey&quot;&gt;user journey&lt;/a&gt;&amp;nbsp;(time series of events) discussed earlier provides an opportunity to understand the funnel for each user and across user groups.&amp;nbsp; Below is a basic example of funnel visualization on a companies&apos;s checkout process that shows the drop off from each step:&amp;nbsp;&amp;nbsp;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/4_questions_7.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We could next take it further in a BI tool or App (streamlit, Dash, Shiny, etc)&amp;nbsp; we add support for filtering based on user attributes such as demographics, referral method (ads, organic), user behaviors, timing, etc.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;3. What do you we know about the people who become subscribers? Are there consistent differences from the visitors to your site that do not become subscribers?&amp;nbsp;&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;When analyzing users who convert and users who don&apos;t for a subscription product, there are several data elements that can be useful for an analyst / data scientist:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span style=&quot;text-decoration: underline;&quot;&gt;User demographics&lt;/span&gt;: Age, gender, location, and other demographic information can provide insights into the types of users who are more likely to convert. Analyzing these factors can help identify target segments for marketing and user acquisition efforts.&amp;nbsp;&lt;/li&gt;
&lt;li&gt;&lt;span style=&quot;text-decoration: underline;&quot;&gt;User behavior&lt;/span&gt;: Tracking user interactions with the product (via our&amp;nbsp;&lt;a href=&quot;#user_journey&quot;&gt;user journey&lt;/a&gt;), such as features used, frequency of use, and duration of sessions, can help identify patterns and trends among converting users. This information can be used to optimize the user experience and improve conversion rates.&amp;nbsp;&lt;/li&gt;
&lt;li&gt;&lt;span style=&quot;text-decoration: underline;&quot;&gt;Conversion funnel analysis&lt;/span&gt;: Analyzing the steps users take before converting can reveal potential bottlenecks or barriers to conversion. Identifying these issues can help optimize the conversion process and increase overall conversion rates.&lt;/li&gt;
&lt;li&gt;&lt;span style=&quot;text-decoration: underline;&quot;&gt;User feedback&lt;/span&gt;: Collecting and analyzing user feedback, such as ratings, reviews, and survey responses, can provide valuable insights into user preferences, pain points, and areas for improvement. This information can be used to refine the product and marketing strategies to better cater to user needs.&lt;/li&gt;
&lt;li&gt;&lt;span style=&quot;text-decoration: underline;&quot;&gt;Marketing and promotional efforts&lt;/span&gt;: Analyzing the effectiveness of marketing campaigns and promotional efforts can help identify which channels, messages, and targeting strategies are most successful in driving conversions. This information can be used to optimize marketing budgets and improve overall conversion rates.&lt;/li&gt;
&lt;li&gt;&lt;span style=&quot;text-decoration: underline;&quot;&gt;Customer lifetime value (CLV)&lt;/span&gt;: Assessing the long-term value of customers can help determine which user segments are most profitable and inform decisions regarding customer acquisition, retention, and marketing strategies.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;You can likely do some quick exploration to see how much each of these metrics/areas correlate to subscription over different timeframes or cohorts.&amp;nbsp;&lt;/p&gt;
&lt;h4&gt;What can you try to learn more?&lt;/h4&gt;
&lt;p&gt;&lt;strong&gt;Ask users what they want and why they aren&apos;t buying&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;You can ask them (maybe even offer some kind of bribe) to understand specifically what their needs/wants are and why do not think the value proposition of your offering will satisfy them at an acceptable cost.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;I would want to find out if they think:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Do they actually think they need your product?&lt;/li&gt;
&lt;li&gt;Are your prices too high?&lt;/li&gt;
&lt;li&gt;Is there a competitor with a better suited offering or better distribution?&lt;/li&gt;
&lt;li&gt;Are their friends using your product and giving positive feedback about it?&lt;/li&gt;
&lt;/ul&gt;
&lt;div&gt;&lt;strong&gt;Record their sessions and see what they see + do&lt;/strong&gt;&lt;/div&gt;
&lt;div&gt;&amp;nbsp;&lt;/div&gt;
&lt;div&gt;There are a number of&amp;nbsp;&lt;a href=&quot;https://siterecording.com/best-website-session-recording-software&quot; target=&quot;_blank&quot;&gt;SaaS services&lt;/a&gt;&amp;nbsp;that offer the ability integration full session recording directly into your web site. I have used&amp;nbsp;&lt;a href=&quot;https://www.fullstory.com/session-replay/&quot; target=&quot;_blank&quot;&gt;Full Story&lt;/a&gt;&amp;nbsp;in the past and it was helpful in some scenarios.&lt;/div&gt;
&lt;div&gt;&amp;nbsp;&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Look at the Churn Model.&amp;nbsp;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;I have found that it is often the case that the reasons why subscribers churn is also the reasons why users are hesitant to subscribe to your product. You may find overlap from an asset you already have.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Test Things and Run Experiments&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;You can conduct experiments to test different versions of your website or app to identify the most effective design, content, messaging, or features that drive customer engagement and conversion. You can use statistical hypothesis testing to determine the impact of each variation on customer behavior.&lt;/p&gt;
&lt;div&gt;Some specific places that can be effective in terms of learning and potentially impacting conversion:&lt;/div&gt;
&lt;div&gt;
&lt;ul&gt;
&lt;li&gt;Promotions: Impact&amp;nbsp;from offering a trial or promotion (discount) for new subscriptions.&lt;/li&gt;
&lt;li&gt;Pricing: Impact&amp;nbsp;from offering different pricing options or the impact of different display variations of the pricing options.&lt;/li&gt;
&lt;li&gt;Funnel / Checkout: Impact from changing the steps in your checkout process.&lt;/li&gt;
&lt;li&gt;Messaging: The way you message about your product&apos;s benefits (on site, partner sites, email, physical collateral, etc.).&lt;/li&gt;
&lt;li&gt;Channels: Impact from the places your product is promoted and referred from.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Customer Segmentation, Targeting and Personalization&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;/strong&gt;You can analyze customer data to identify different segments and their behavior patterns. This will help you understand which customers are more likely to make a purchase or subscribe to your services. You can use clustering algorithms like K-means or DBSCAN to group customers based on their characteristics, such as demographics, purchase history, and browsing behavior.&lt;/p&gt;
&lt;h3&gt;4) How can you build awareness/traffic to your offerings?&lt;/h3&gt;
&lt;p&gt;I do not claim to be a growth marketing expert, but I have worked with some very smart folks in this domain area and have learned some things along the way.&amp;nbsp;&lt;/p&gt;
&lt;h4&gt;Marketing Channels&lt;/h4&gt;
&lt;p&gt;Here are some of the tactics I have seen used to varying degrees of effectiveness to build awareness and traffic to your subscription offerings as a growth marketer:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Leverage social media platforms&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;/strong&gt;Facebook, Instagram, YouTube, Twitter, TikTok, LinkedIn and others have HUGE highly active audiences. They can be fantastic avenues for attracting potential customers.&lt;/p&gt;
&lt;p&gt;Much of the volume is done as part of running brand-awareness (paid) ads that target specific audiences based on behaviors and preferences. This can help improve reach and recall, and even create &apos;lookalike&apos; audiences similar to your existing followers. These platforms offer many different ad formats and creative display options that can appeal to your potential audience.&lt;/p&gt;
&lt;p&gt;In addition to paid options, many of the social networks provide opportunities to promote your offerings (just be careful not to be too spammy or obnoxious about promoting your business). Facebook and LinkedIN groups are often dedicated to specific niche topics and may be a good avenue to find users.&lt;/p&gt;
&lt;p&gt;Example - I am part of many Facebook groups related to sports autograph collecting, so I often answer questions letting newer collectors know they can find information they seek on&amp;nbsp;&lt;a href=&quot;https://www.sportscollectors.net/&quot; target=&quot;_blank&quot;&gt;SportsCollectors.Net&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Organize contests and giveaways&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;/strong&gt;Host simple content or giveaways to grow your following and drive brand awareness.&lt;/p&gt;
&lt;p&gt;Example - Tech Influencer&amp;nbsp;&lt;a href=&quot;https://www.linkedin.com/in/alexxubyte/&quot; target=&quot;_blank&quot;&gt;Alex Xu&lt;/a&gt;&amp;nbsp;gives away his popular book &quot;System Design Interview&quot; in order to promote his excellent&amp;nbsp;&lt;a href=&quot;https://blog.bytebytego.com/&quot; target=&quot;_blank&quot;&gt;ByteByteGo&lt;/a&gt;&amp;nbsp;paid newsletter.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Give something away for free&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;/strong&gt;Offer a free trial or free samples of your subscription service or product to give potential customers a taste of what you offer.&lt;/p&gt;
&lt;p&gt;Example - I signed up for a free one month trial for&amp;nbsp;&lt;a href=&quot;https://github.com/features/copilot&quot; target=&quot;_blank&quot;&gt;Github&apos;s Copilot&lt;/a&gt;&amp;nbsp;service that offers suggestions as I write code.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Content marketing&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;/strong&gt;Publish blog posts, articles, or other forms of content to establish your expertise and thought leadership in your industry. This can help build brand awareness and drive traffic to your website.&lt;/p&gt;
&lt;p&gt;Example -&amp;nbsp;&lt;a href=&quot;https://www.youtube.com/@SportsCardInvestor&quot; target=&quot;_blank&quot;&gt;Sports Card Investor&lt;/a&gt;&amp;nbsp;hosts regular podcasts talking about the latest high level market trends and features in their MarketMovers subscription product for sports card sales data and analytics.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Influencer marketing&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;/strong&gt;Collaborate with influencers and brand ambassadors to promote your subscription offerings and reach a wider audience.&lt;/p&gt;
&lt;p&gt;Example - This&amp;nbsp;&lt;a href=&quot;https://www.youtube.com/watch?v=aNv1qZ54YzQ&quot; target=&quot;_blank&quot;&gt;youtube video&lt;/a&gt;&amp;nbsp;from Mr. Beast, which has 1M+ views &amp;mdash; it&amp;rsquo;s engaging and encourages the audience to give Honey a try. Companies such&amp;nbsp;&lt;a href=&quot;https://company.shopltk.com/en/company&quot; target=&quot;_blank&quot;&gt;LTK&lt;/a&gt;&amp;nbsp;can even help you find influencers that would work for your company.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Email marketing&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;/strong&gt;Use email marketing to engage with your audience and promote your subscription offerings.&lt;/p&gt;
&lt;p&gt;Example - While I was at ElysiumHealth, their quarterly newsletters (&lt;a href=&quot;https://milled.com/elysium-health/the-abstract-scientific-breakthroughs-of-2022-m44p3qKmgIJQJF5f&quot; target=&quot;_blank&quot;&gt;the Abstract&lt;/a&gt;) were packed with deep scientific dives on longevity topics related that drove interest to their core products.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Employee advocacy&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;/strong&gt;Encourage your employees to share your brand and subscription offerings on their social media platforms. Create a culture where employees proactively want to evangelize your organization.&lt;/p&gt;
&lt;p&gt;Example - I saw that many of the employees at&amp;nbsp;&lt;a href=&quot;https://www.renttherunway.com/&quot; target=&quot;_blank&quot;&gt;Rent the Runway&lt;/a&gt;&amp;nbsp;across business different functions in the company would publicly suggest the company&apos;s subscription offerings as well as specific rental products.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Events&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Hosting Physical events to introduce your product to new audiences.&lt;/p&gt;
&lt;p&gt;Example - Amazon hosts events at their&amp;nbsp;&lt;a href=&quot;https://aws-startup-lofts.com/amer/&quot; target=&quot;_blank&quot;&gt;AWS Startup Lofts&lt;/a&gt;&amp;nbsp;where experienced AWS architects can help you design solutions to solve your technical problems using AWS products.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Search Engine Optimization (SEO)&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Marketing technique that is focused on bringing organic, non-paid traffic to your website by using high quality content to get to the top of a search engine results page.&lt;/p&gt;
&lt;h4&gt;We want to analyze what works (and doesn&apos;t)&lt;/h4&gt;
&lt;p&gt;We will want a way to understand the impacts of traffic and conversion/sales from each of the marketing channels your company employs.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Common Problem - Disparate reporting does not reconcile&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;I have found that many marketers (potentially in different marketing functions) generally use SaaS products for their specific channels that each allow you to set up tagging on your site to track activities within their application.&amp;nbsp;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Your email service provider (like&amp;nbsp;&lt;a href=&quot;https://mailchimp.com/&quot; target=&quot;_blank&quot;&gt;MailChimp&lt;/a&gt;,&amp;nbsp;&lt;a href=&quot;https://iterable.com/&quot; target=&quot;_blank&quot;&gt;Iterable&lt;/a&gt;,&amp;nbsp;&lt;a href=&quot;https://www.klaviyo.com/&quot; target=&quot;_blank&quot;&gt;Klaviyo&lt;/a&gt;, etc) may attribute the full value of customer purchase if the user has opened an email sent from their platform.&lt;/li&gt;
&lt;li&gt;Facebook and Google may attribute the full value of that same customer purchase if users were sent to your site from paid ads you bought on their systems.&amp;nbsp;&lt;/li&gt;
&lt;li&gt;Your influencer or referral partners (like&amp;nbsp;&lt;a href=&quot;https://www.yotpo.com/&quot; target=&quot;_blank&quot;&gt;Yopto&lt;/a&gt;,&amp;nbsp;&lt;a href=&quot;https://www.talkable.com/&quot; target=&quot;_blank&quot;&gt;Talkable&lt;/a&gt;,&amp;nbsp;&lt;a href=&quot;https://www.mention-me.com/&quot; target=&quot;_blank&quot;&gt;Mention Me&lt;/a&gt;, etc) may also attribute the full value of that same customer purchase if users were sent to your site from referral links from their systems.&amp;nbsp;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;So what generally happens is that the leads for email, paid, and influencer marketing download the performance reports from their respective SaaS system and present an overstated ROI for their area. You won&apos;t have a clear picture of how well each channel is contributing and how to prioritize spend between. This invariably leads to questions from your finance team when budgets are being planned out and the numbers don&apos;t all add up.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;How Rules Based Attribution Models Work&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Rules Based (non-model driven) marketing attribution models assign credit to specific marketing channels or touch points based on predetermined rules or assumptions, rather than analyzing data and customer behavior patterns to determine attribution.&lt;/p&gt;
&lt;p&gt;These models are popular because they are easy to understand, relatively straight forward to implement and can provide directional signal around ROI. They can be a good choice for businesses that are just starting out, have limited resources or have less a complex/diverse marketing channel mix. I have personally seen most companies implement these kinds of models.&lt;/p&gt;
&lt;p&gt;There are several types of rules based attribution models that businesses can use. These include:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/4_questions_8.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Our &lt;a href=&quot;#user_journey&quot;&gt;user journey&lt;/a&gt;&amp;nbsp;data contains the full set of actions associated with each user that we are able to track from internal and partner systems. It is industry standard that we set up&amp;nbsp;&lt;a href=&quot;https://en.wikipedia.org/wiki/UTM_parameters&quot; target=&quot;_blank&quot;&gt;UTM parameters&lt;/a&gt;&amp;nbsp;on incoming links that will inform us of the source, medium, and campaign associated with that user&apos;s visit. We will store those parameters as part of payload for each relevant event that we can later use for analysis.&lt;/p&gt;
&lt;p&gt;We will want to use user journey to be able to track individual customer interactions with our brand. The closest method we have to do this is with sessions, defined as discrete periods of activity by a user. The industry standard is to define a session as a series of activities followed by a 30 minutes window without activity. So we can write some code to categorize the actions in our user journey into sessions, with the first action representing channel information we will use for attribution of that session. We can use all of the sessions before a conversion as part of our attribution calculation.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;Based on the preferred attribution methodology shown in the diagram above, we can divide up the sessions for each transaction and attribute the revenue (and return on associated marketing spend) accordingly.&lt;/p&gt;
&lt;p&gt;Claire Carroll wrote a helpful&amp;nbsp;&lt;a href=&quot;https://www.getdbt.com/blog/modeling-marketing-attribution/&quot; target=&quot;_blank&quot;&gt;blog post&lt;/a&gt;&amp;nbsp;detailing an example of how to model out user journey data into sessions with only SQL using the dbt framework. It is very similar to the methodology I have taken in the past do this.&lt;/p&gt;
&lt;p&gt;My personal preference is to use either Time Decay or Position-Based models. I have found the most important thing is that you get consensus with the model that everyone will use up front, and then optimize around the related metrics.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Data Driven Attribution&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Data-driven attribution gives credit for conversions based on how people engage with your various ads and decide to become your customers.&lt;/p&gt;
&lt;p&gt;Unlike the previous discussed rules based models, data-driven attribution gives you more accurate results by analyzing all of the relevant data about the marketing moments that led up to a conversion. Google uses this data-driven attribution in Google Ads&amp;nbsp; as the models takes multiple signals into account, including the ad format and the time between an ad interaction and the conversion. They can drive better conversions because their systems can better predict the incremental impact a specific ad will have on driving a conversion, and adjust bids accordingly to maximize your ROI.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;There are two machine learning models that have become popular to use for data driven attribution:&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;text-decoration: underline;&quot;&gt;1. Shapely Value&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;The Shapley value is a way to fairly distribute credit for a shared outcome among team members by applying an algorithm based on a concept from cooperative game theory called the Shapley Value. In the case of data driven attribution, marketing touchpoints are the &quot;team members&quot;, and the &quot;output&quot; of the team is conversions. The&amp;nbsp; algorithm computes the counterfactual gains of each marketing touchpoint, which means it compares the conversion probability of similar users who were exposed to these touchpoints to the probability when one of the touchpoints does not occur in the path. The actual calculation of conversion credit for each touchpoint depends on comparing all of the different permutations of touchpoints and normalizing across them.&lt;/p&gt;
&lt;p&gt;James Kinney posted a nice explanation of how Game Theory and Shapely Value can be applied to&amp;nbsp;&lt;a href=&quot;https://medium.com/towards-data-science/data-driven-marketing-attribution-1a28d2e613a0&quot; target=&quot;_blank&quot;&gt;Data Driven Marketing Attribution&lt;/a&gt;. He also provides a helpful Jupyter notebook with python code on his&amp;nbsp;&lt;a href=&quot;https://github.com/jrkinley/game-theory-attribution/blob/main/game_theory_attribution.ipynb&quot; target=&quot;_blank&quot;&gt;Github&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;text-decoration: underline;&quot;&gt;2. Markov Model&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;A Markov model is a type of probabilistic model that describes a sequence of events where the probability of each event depends only on the state of the previous event. In the context of marketing attribution, a Markov model can help us model user journeys and how each channel factors into users moving from one channel to another to eventually make a purchase.&lt;/p&gt;
&lt;p&gt;For example, let&apos;s say a user first sees a Facebook ad, then clicks on a Google search result, and finally makes a purchase on your website. A Markov model would help us understand the probability of users moving from Facebook to Google and from Google to your website.&lt;/p&gt;
&lt;p&gt;One advantage of using a Markov model for marketing attribution is that it can account for the structure of your data, which may lead to more accurate results. However, it is more complex than other attribution models, and may require the help of a data scientist to implement at scale.&lt;/p&gt;
&lt;p&gt;To use a Markov model for marketing attribution, we need to estimate a transition matrix that describes the probability of moving from one channel to another. We can then compute the &quot;removal effects&quot; of each channel, which tells us the probability of conversion when a channel is removed from the user journey. This allows us to determine each channel&apos;s contribution to conversion and/or value.&lt;/p&gt;
&lt;p&gt;James Kinney posted Cloudera uses Markov models to solve multi-channel attribution in his post&amp;nbsp;&lt;a href=&quot;https://towardsdatascience.com/multi-channel-marketing-attribution-with-markov-6b744c0b119a&quot; target=&quot;_blank&quot;&gt;Marketing Attribution with Markov&lt;/a&gt;.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Challenges with Attribution&amp;nbsp;&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;It requires set up and discipline to collect all of the necessary data in a central place (like a data warehouse). That means your team needs the data engineering skill sets to do the appropriate pipeline building, transformation and modeling.&amp;nbsp;&lt;/li&gt;
&lt;li&gt;Your user journey may be missing some percentage of actions. Many have found it to be challenging to associate actions with anonymous (non signed in) users using mobile applications and mobile browsers.&amp;nbsp;&lt;/li&gt;
&lt;li&gt;It is challenging to find a proxy for incorporating offline data, such as exposure to a TV, radio or print ad.&lt;/li&gt;
&lt;li&gt;Lack of visibility into external trends that might affect marketing efforts and conversions, such as seasonality, without incorporating aggregate information.&lt;/li&gt;
&lt;li&gt;Attribution models can be subject to correlation-based biases when analyzing the customer journey, causing it to look like one event cause another, when it may not have.&amp;nbsp;&lt;/li&gt;
&lt;li&gt;Consumers who may have been in the market to buy the product and would have purchased it whether they had seen the ad or not. However, the ad gets the attribution for converting this user.&lt;/li&gt;
&lt;li&gt;Bias toward cheap Inventory gives an inaccurate view of how media is performing, making lower cost media appear to perform better due to the natural conversion rate for the targeted consumers, when the ads may not have played a role.&lt;/li&gt;
&lt;li&gt;Attribution models can often overlook the relationship between brand perception and consumer behavior, or will only look at them at a trend regression level.&lt;/li&gt;
&lt;li&gt;The quality of creative and messaging are just as important to consumers as the medium on which they see your ad. One common attribution mistake is evaluating creative in aggregate and determining that one message is ineffective, when in reality it would be effective for a smaller, more targeted audience. This emphasizes the importance of person-level analytics.&amp;nbsp;&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Wed, 07 Jun 2023 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/four-questions-of-subscription-ecommerce/</link>
        <guid isPermaLink="true">http://localhost:4000/four-questions-of-subscription-ecommerce/</guid>
        
        <category>favorites</category>
        
        <category>analysis</category>
        
        <category>data engineering</category>
        
        <category>subscription</category>
        
        <category>marketing</category>
        
        <category>attribution</category>
        
        
      </item>
    
      <item>
        <title>Optimizing spend in Snowflake</title>
        <description>&lt;p&gt;Since about 2016, I have introduced Snowflake at two of my day jobs (&lt;a href=&quot;https://www.renttherunway.com/&quot; target=&quot;_blank&quot;&gt;Rent the Runway&lt;/a&gt; and &lt;a href=&quot;https://lumafintech.com/&quot; target=&quot;_blank&quot;&gt;Luma Financial Technologies&lt;/a&gt;) and have witnessed the platform&apos;s considerable evolution.&amp;nbsp; As with any hosted SaaS platform, it is very important to understand both how the solution is architected, how users interact with it, and how it is priced.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;A very common concern among dats folks is that their Snowflake bills grown considerably as their use cases for it expand. A few months ago I saw this &lt;a href=&quot;https://www.linkedin.com/posts/mattflorian_data-dataengineer-dataanalytics-activity-6995462163326795776-_hzn/&quot; target=&quot;_blank&quot;&gt;post&lt;/a&gt; on LinkedIn:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/sf_spend_1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;I think the suggested action in the post could potentially lead to a very small cost savings, but I suspect there are more impactful areas we can look at to keep our Snowflake costs down. I thought it would be helpful to detail out:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;How Snowflake&apos;s billing works&lt;/li&gt;
&lt;li&gt;Areas where you can look to optimize&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;How Snowflake&apos;s billing works&lt;/h3&gt;
&lt;p&gt;I found an &lt;a href=&quot;https://docs.snowflake.com/en/user-guide/cost-understanding-overall&quot; target=&quot;_blank&quot;&gt;area&lt;/a&gt;&amp;nbsp;and a &lt;a href=&quot;https://www.snowflake.com/wp-content/uploads/2017/03/Understanding-Snowflake-Pricing-1Dec2016.pdf&quot; target=&quot;_blank&quot;&gt;pdf&lt;/a&gt; where Snowflake lays out their costs, so I&apos;ll do my best to summarize with commentary below.&lt;/p&gt;
&lt;p&gt;One of the main selling points for Snowflake is that they do a really good job of separating storage of the data, the compute that runs actions on the data (queries, transformations, etc.), and the cloud services that run the platform&apos;s engine. So their pricing is based on 3 components:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;the volume of data stored in Snowflake&lt;/li&gt;
&lt;li&gt;the compute time used&lt;/li&gt;
&lt;li&gt;the cloud services that are used&amp;nbsp;&amp;nbsp;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;span style=&quot;text-decoration: underline;&quot;&gt;&lt;strong&gt;1. Data Storage Pricing&amp;nbsp;&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Data storage pricing is based on the daily average data volume (in bytes) stored in Snowflake, including compressed or uncompressed files staged for bulk unloading or loading, historical data maintained for File-safe, and data kept in database tables. Snowflake automatically compresses and optimizes all table data, then calculates storage usage based on this compressed file size. The monthly charge for data storage in Snowflake is set at a flat rate per terabyte (TB). However, the precise amount per TB paid depends on the platform (Amazon Web Services (AWS), Azure, or Google Cloud Platform (GCP)). Typically, Snowflake charges a minimum of $25 and up to $40 per terabyte of data stored in its US system.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;For instance, if a Snowflake account is a capacity AWS in us-west-1 snowflake account and the price is $25 per Terabyte per month.&lt;/p&gt;
&lt;p&gt;It is worth noting that the cost of storage in Snowflake is based on the amount of data stored in the account. This includes data stored in database tables, files staged for bulk data loading, and clones of database tables that reference data deleted in the table that owns the clones&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;text-decoration: underline;&quot;&gt;&lt;strong&gt;2. Compute Pricing and Credits&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Snowflake bills compute with credits - the unit used to measure how much billable compute (virtual warehouses) you consume. A Snowflake credit is used only when resources are active, such as when a virtual warehouse is currently running, when loading data with Snowpipe, or serverless features are in use.&lt;/p&gt;
&lt;p&gt;A virtual warehouse contains one or more computing clusters for performing queries, loading data, and other DML operations. Virtual warehouses use Snowflake credits as payment for the processing time they use. How many credits you use depends on the virtual warehouse&amp;rsquo;s size, running duration, and how many they are.&lt;/p&gt;
&lt;p&gt;Warehouses are available in ten sizes now. Each size specifies the amount of computing power a warehouse can access per cluster. By expanding a warehouse to the next larger size, its computing power and credit usage doubles per full operational hour. Check this out:&lt;/p&gt;
&lt;p&gt;The credit pricing rate depends on the Snowflake edition you use: Standard, Enterprise, or Business-Critical. Each edition offers a different set of features.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;The table below shows the Snowflake Credit usage and estimate of the USD cost per hour assuming one credit costs $3.00 (I have found you can get a better rate with a year long commitment):&amp;nbsp;&lt;/p&gt;
&lt;table style=&quot;width: 600px;&quot; border=&quot;1&quot; cellspacing=&quot;1&quot; cellpadding=&quot;3&quot;&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&quot;padding: 5px;&quot;&gt;&lt;strong&gt;Size&lt;/strong&gt;&lt;/td&gt;
&lt;td style=&quot;padding: 5px;&quot;&gt;&lt;strong&gt;Credits/Hour&lt;/strong&gt;&lt;/td&gt;
&lt;td style=&quot;padding: 5px;&quot;&gt;&lt;strong&gt;Cost/Hour&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&quot;padding: 5px;&quot;&gt;XSMALL&lt;/td&gt;
&lt;td style=&quot;padding: 5px;&quot;&gt;1&lt;/td&gt;
&lt;td style=&quot;padding: 5px;&quot;&gt;$3.00&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&quot;padding: 5px;&quot;&gt;SMALL&lt;/td&gt;
&lt;td style=&quot;padding: 5px;&quot;&gt;2&lt;/td&gt;
&lt;td style=&quot;padding: 5px;&quot;&gt;$6.00&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&quot;padding: 5px;&quot;&gt;MEDIUM&lt;/td&gt;
&lt;td style=&quot;padding: 5px;&quot;&gt;4&lt;/td&gt;
&lt;td style=&quot;padding: 5px;&quot;&gt;$12.00&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&quot;padding: 5px;&quot;&gt;LARGE&lt;/td&gt;
&lt;td style=&quot;padding: 5px;&quot;&gt;8&lt;/td&gt;
&lt;td style=&quot;padding: 5px;&quot;&gt;$24.00&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&quot;padding: 5px;&quot;&gt;X-LARGE&lt;/td&gt;
&lt;td style=&quot;padding: 5px;&quot;&gt;16&lt;/td&gt;
&lt;td style=&quot;padding: 5px;&quot;&gt;$48.00&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&quot;padding: 5px;&quot;&gt;2X-LARGE&lt;/td&gt;
&lt;td style=&quot;padding: 5px;&quot;&gt;32&lt;/td&gt;
&lt;td style=&quot;padding: 5px;&quot;&gt;$96.00&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&quot;padding: 5px;&quot;&gt;3X-LARGE&lt;/td&gt;
&lt;td style=&quot;padding: 5px;&quot;&gt;64&lt;/td&gt;
&lt;td style=&quot;padding: 5px;&quot;&gt;$192.00&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&quot;padding: 5px;&quot;&gt;4X-LARGE&lt;/td&gt;
&lt;td style=&quot;padding: 5px;&quot;&gt;128&lt;/td&gt;
&lt;td style=&quot;padding: 5px;&quot;&gt;$384.00&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&quot;padding: 5px;&quot;&gt;5X-LARGE&lt;/td&gt;
&lt;td style=&quot;padding: 5px;&quot;&gt;256&lt;/td&gt;
&lt;td style=&quot;padding: 5px;&quot;&gt;$768.00&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&quot;padding: 5px;&quot;&gt;6X-LARGE&lt;/td&gt;
&lt;td style=&quot;padding: 5px;&quot;&gt;512&lt;/td&gt;
&lt;td style=&quot;padding: 5px;&quot;&gt;$1,536.00&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&lt;br /&gt;Some details to note:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Warehouse only uses credits while running &amp;mdash; not when suspended or idle.&amp;nbsp;Snowflake does not charge for idle compute. In fact, it offers a quick start/stop feature to suspend resource usage whenever you choose or automatically with user-defined rules, such as &amp;ldquo;suspend after five minutes of inactivity&amp;rdquo;.&lt;/li&gt;
&lt;li&gt;Credit usage per hour directly correlates to the number of servers in a warehouse cluster.&lt;/li&gt;
&lt;li&gt;Snowflake bills credits per second &amp;mdash; with a minimum requirement of 60 seconds. Meaning, starting or resuming a suspended warehouse incurs a fee of one minute&amp;rsquo;s worth or usage. Also, resizing a warehouse a size larger costs a full minute&amp;rsquo;s worth of usage. For example, resizing from Medium (four credits per hour) to Large (eight credits per hour) has an additional charge for one minute&amp;rsquo;s worth of four additional credits.&lt;/li&gt;
&lt;li&gt;But after a minute&amp;rsquo;s usage, all subsequent usage resumes on a per-second billing as long as you run the virtual warehouses continuously.&amp;nbsp;Stopping and restarting warehouses within the first minutes leads to multiple charges because the one-minute minimum charge applies each time you restart.&lt;/li&gt;
&lt;li&gt;Operations, such as suspending, resuming, increasing, and decreasing are nearly instantaneous, So, you can precisely match your Snowflake spending to your actual usage, never worrying about capacity planning or unexpected usage.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;span style=&quot;text-decoration: underline;&quot;&gt;&lt;strong&gt;3. Cloud Services Pricing and Credits&lt;/strong&gt;&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Snowflake cloud services comprise a set of tools that support the main data storage and compute functions. The services include metadata and infrastructure management, SQL API, access control and authentication, and query parsing.&lt;/p&gt;
&lt;p&gt;These services are, in turn, powered by compute resources, meaning cloud services consume Snowflake credits, just like virtual warehouses. However, Snowflake only bills cloud services that exceed 10% of your daily compute resources usage. Thus, the 10% adjustment automatically applies on each day, based on that day&amp;rsquo;s credit price.&lt;/p&gt;
&lt;p&gt;For example, if you&amp;rsquo;ve used 120 compute credits and 20 cloud services credits on a particular day, the adjustment will automatically subtract 10% off the compute credits (120 X 10% = 12) for that day. So, your billable credits will be:&lt;/p&gt;
&lt;p&gt;Total used cloud services credits for the day (20) &amp;ndash; adjusted amount (12) = 8 credits billed.&lt;/p&gt;
&lt;p&gt;If your cloud services amount is less than 10% of your daily compute credits amount, Snowflake charges you based on that day&amp;rsquo;s cloud services credits.&lt;/p&gt;
&lt;p&gt;For example, if you&amp;rsquo;ve used 100 compute credits and 8 cloud services credits, you&amp;rsquo;ll have used 8% of your compute credits in cloud services credits. In this case, Snowflake will automatically count 8 cloud services credits as the day&amp;rsquo;s cloud services usage.&lt;/p&gt;
&lt;h3&gt;Areas where to optimize&amp;nbsp;&lt;/h3&gt;
&lt;p&gt;Snowflake is both robust and complex platform. With that, the good and bad news is that there are quite a few options/opportunities to consider that can affect your spend.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;1. Look at your spend &amp;amp; consumption&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Before I look to optimize, I want to understand my current consumption vs. my budget. You can see this in Snowflake&apos;s GUI using the ACCOUNTADMIN role:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/sf_spend_2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;A resource I found for monitoring spend is a dbt package from SELECT:&amp;nbsp;&lt;br /&gt;&lt;a href=&quot;https://github.com/get-select/dbt-snowflake-monitoring&quot; target=&quot;_blank&quot;&gt;https://github.com/get-select/dbt-snowflake-monitoring&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;2. Don&apos;t do things you don&apos;t need to!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;text-decoration: underline;&quot;&gt;Do I have processes running to create calculations, tables, or views&amp;nbsp; that no longer add value?&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;A lot of analytics and data science is doing feature exploration, but we don&apos;t always clean up after our experimentation and disproved hypotheses. This can consume a bunch of credits ongoing basis when they take a lot of resources to create, and morse so if they exist in multiple environments (for testing, CI/CD).&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;text-decoration: underline;&quot;&gt;Am I running processes more often than I need to?&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;As an example - when the models are based off data that we get daily (such as closing stock values), it can become costly to run these many times a day. I have found using a &lt;a href=&quot;https://docs.getdbt.com/reference/resource-configs/tags&quot; target=&quot;_blank&quot;&gt;dbt tags&lt;/a&gt;&amp;nbsp;to limit the models are run in jobs that are limited to running daily.&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;text-decoration: underline;&quot;&gt;Can I decrease sync frequency of my data ingestion tool?&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;There can be a pretty big a credit difference in sync&apos;ing data ever 5 or 15 minutes vs. hourly in FiveTran, AirByte, etc.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;3. Data Warehouse Considerations&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In general, the majority of Snowflake compute cost is the result of automated jobs.&amp;nbsp; This means a huge proportion of cost is the result of transformation jobs and this should be the priority to optimize Snowflake warehouse cost.&amp;nbsp;&amp;nbsp;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;text-decoration: underline;&quot;&gt;Segment your workloads by warehouses&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;My preference is to divide workloads in different warehouses for loading data, transforming data and for end user consumption as I previously described in&amp;nbsp;&lt;a href=&quot;http://jaygrossman.com/post/2023/01/28/Snowflake-Set-up-with-Terraform.aspx&quot; target=&quot;_blank&quot;&gt;My Snowflake Set up with Terraform&lt;/a&gt;. This way I can both configure the permission grants and warehouse size for each of these workload types. You may need to further segment - maybe by business vertical or priority of usage.&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;text-decoration: underline;&quot;&gt;Choose the right size of your warehouses&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;As mentioned, the size of your Snowflake warehouse has a direct impact on your monthly bill.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;Your use case will be key in deciding whether to run large or small warehouses. In general, running heavy queries on large warehouses and light queries on small warehouses is the most cost-effective way to go. Be aware queries should run twice as fast on a larger warehouse but stop increasing warehouse size when the elapsed time improvements drops below 50%.&lt;/p&gt;
&lt;p&gt;I generally prefer to go with smaller warehouses and to define MAX_CLUSTER_COUNT parameter that will bring up additional nodes as scale is needed.&lt;/p&gt;
&lt;p&gt;For warehouses designed to run lower priority batch jobs set the MAX_CLUSTER_COUNT = 3 and SCALING_POLICY = &apos;ECONOMY&apos; to balance the need to maximize throughput with optimizing compute cost.&lt;/p&gt;
&lt;p&gt;For end-user warehouses where performance is a priority set the MAX_CLUSTER_COUNT = 3 and SCALING_POLICY = &apos;STANDARD&apos;.&amp;nbsp; This will automatically allocate additional clusters as the concurrent workload increases.&amp;nbsp; However, set the MAX_CLUSTER_COUNT to the smallest number possible while controlling the time spent queuing.&amp;nbsp; With a SCALING POLICY of STANDARD, avoid setting the MAX_CLUSTER_COUNT = 10 (or higher) unless maximizing performance is a much higher priority than controlling cost.&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;text-decoration-line: underline;&quot;&gt;Suspend warehouses that are sitting idle&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;If you have virtual warehouses that are inactive, you can suspend them to make sure you&apos;re not getting charged for unused compute power.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;Here is an example of how you can create a warehouse (the auto_suspend parameter controls how long until the idle warehouse is suspended):&lt;/p&gt;
&lt;p style=&quot;padding-left: 30px;&quot;&gt;-- Create a multi-cluster warehouse for batch processing&lt;br /&gt;create or replace warehouse transform_wh with&lt;br /&gt;&amp;nbsp; &amp;nbsp; warehouse_size&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;= XSMALL&lt;br /&gt;&amp;nbsp; &amp;nbsp; min_cluster_count&amp;nbsp; &amp;nbsp;= 1&lt;br /&gt;&amp;nbsp; &amp;nbsp; max_cluster_count&amp;nbsp; &amp;nbsp;= 3&lt;br /&gt;&amp;nbsp; &amp;nbsp; scaling_policy&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; = economy&lt;br /&gt;&amp;nbsp; &amp;nbsp; auto_suspend&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; = 60&lt;br /&gt;&amp;nbsp; &amp;nbsp; auto_resume&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;= TRUE;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;text-decoration: underline;&quot;&gt;Update the query timeout default value&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;By default, a Snowflake statement runs for 48 hours (172800 seconds) before the system aborts it.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;This means that Snowflake will charge you for the time it took to compute a query that may have been initiated by mistake.&lt;/p&gt;
&lt;p&gt;Set the STATEMENT_TIMEOUT_IN_SECONDS parameter:&lt;/p&gt;
&lt;p style=&quot;padding-left: 30px;&quot;&gt;alter warehouse transform_wh&amp;nbsp;set&lt;br /&gt;&amp;nbsp; &amp;nbsp;statement_timeout_in_seconds = 3600;&amp;nbsp; &amp;nbsp;-- 3,600 = 1 hour&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;text-decoration: underline;&quot;&gt;Coordinating queries&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;You pay some money for idle warehouses (as defined by the auto_suspend&amp;nbsp; parameter).&amp;nbsp; There can be advantages to submitting multiple SQL jobs in parallel in a different connection running on a shared batch transformation warehouse to maximize throughput.&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;text-decoration: underline;&quot;&gt;Warehouse Observability&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;To make sure you&apos;re staying within your Snowflake budget, you can use a &lt;a href=&quot;https://docs.snowflake.com/en/user-guide/resource-monitors&quot; target=&quot;_blank&quot;&gt;resource monitor&lt;/a&gt; to suspend a warehouse when it reaches its credit limit.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;A great trick is to set credit thresholds at different levels. For example, you could set an alert for when 70% credit consumption is reached and then another for when 90% of the credit consumption is reached.&lt;/p&gt;
&lt;p style=&quot;padding-left: 30px;&quot;&gt;use role accountadmin;&lt;br /&gt; create or replace resource monitor transform_wh_monitor&amp;nbsp;with&lt;br /&gt;&amp;nbsp; &amp;nbsp;credit_quota = 48&amp;nbsp; &lt;br /&gt;&amp;nbsp; &amp;nbsp;triggers on 70 percent&amp;nbsp; do notify&lt;br /&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; on 90 percent&amp;nbsp; do notify&lt;br /&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; on 100 percent do suspend&lt;br /&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; on 110 percent do suspend_immediate;&amp;nbsp;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;4. Understand and Optimize Queries&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Since credits for running queries/transformations are usually the biggest component of my snowflake bill, I generally want to understand:&lt;/p&gt;
&lt;p&gt;1. What queries represent the biggest portion of my consumption?&lt;br /&gt;2. What parts of those of those queries take the longest?&lt;br /&gt;3. Can I refactor those queries better (faster + less resource intensive)?&amp;nbsp;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;text-decoration: underline;&quot;&gt;What queries represent the biggest portion of my consumption?&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;I found a helpful &lt;a href=&quot;https://medium.com/snowflake/monitoring-snowflake-with-snowsight-e9990a2898f1&quot;&gt;blog post &lt;/a&gt;that talks about the &lt;a href=&quot;https://docs.snowflake.com/en/sql-reference/account-usage&quot; target=&quot;_blank&quot;&gt;ACCOUNT_USAGE schema&lt;/a&gt; (that contains details about my Snowflake usage) and how to build a dashboard within Snowflake detailing my usage. Below are some screen shots:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/sf_spend_3.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/sf_spend_4.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;Since I am a frequent dbt user, I generally will look at the standard output after a &quot;dbt run&quot; for models that are taking over 60-120 seconds (shown below). Unless your data is quite large, this is generally a good indicator for a potential refactor.&amp;nbsp;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/sf_spend_5.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;span style=&quot;text-decoration: underline;&quot;&gt;&lt;br /&gt;What parts of those of those queries take the longest?&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;My friend, the Snowflake &lt;a href=&quot;https://docs.snowflake.com/en/user-guide/ui-query-profile&quot; target=&quot;_blank&quot;&gt;Query Profiler&lt;/a&gt;, is very helpful. It&apos;s probably one of my favorite features in the product.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/sf_spend_6.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;The profiler shows you in detail how it breaks down your query in order to build your requested data set. It shows the amount of time it spends on each step. This will let you know where there may be opportunities for refactor your query.&lt;/p&gt;
&lt;p&gt;This is an excellent blog post that details more about the profiler:&amp;nbsp;&lt;br /&gt;&lt;a href=&quot;https://teej.ghost.io/understanding-the-snowflake-query-optimizer/&quot; target=&quot;_blank&quot;&gt;https://teej.ghost.io/understanding-the-snowflake-query-optimizer/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;text-decoration: underline;&quot;&gt;Can I refactor those queries better (faster + less resource intensive)?&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Like with most databases and infra, POORLY WRITTEN CODE MAY BE VERY COSTLY. Some things I look for that could be inefficient code to refactor:&lt;br /&gt;&lt;br /&gt;- Where is query spending its time - on (network, processing, synchronization)?&lt;br /&gt;- Is the query scanning too much data? Could you filter more efficiently or set up cluster keys?&lt;br /&gt;- Is it spilling out of memory and reading from disk?&lt;br /&gt;- Is the query calling too many micro partitions?&lt;br /&gt;- is there an inefficient join? &lt;br /&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; - Could you rewrite it as a CTE or with UNION&apos;ing data sets? &lt;br /&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; - Should you change the order if you are joining two tables ON multiple fields?&lt;br /&gt;- Are you calling a view (or multiple cascading views) where having a materialized table makes more sense?&lt;br /&gt;- Are your UDF&apos;s (functions) running efficiently? Can you do the logic in SQL (more efficient) instead of another supported language?&lt;br /&gt;- Are there places where we can do incremental additions/updates instead of fully rebuilding full tables?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;5. Data Storage Considerations&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;text-decoration: underline;&quot;&gt;Use the right type of table&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;To help manage the storage costs associated with Time Travel and Fail-safe, Snowflake provides two table types, temporary and transient. Temporary and transient tables do not incur the same fees as permanent tables. Transient and temporary tables contribute to the storage charges that Snowflake bills your account until explicitly dropped. Data stored in these table types contributes to the overall storage charges Snowflake bills your account while they exist. Temporary tables are typically used for non-permanent session specific transitory data such as ETL or other session-specific data. Temporary tables only exist for the lifetime or their associated session. On session end, temporary table data is purged and unrecoverable. Temporary tables are not accessible outside the specific session that created them. Transient tables exist until explicitly dropped and are available to all users with appropriate privileges&lt;/p&gt;
&lt;p&gt;For large, high-churn dimension tables that incur overly-excessive costs, Snowflake recommends creating these tables as transient with zero Time Travel retention and then copying these tables on a periodic basis into a permanent table. This effectively creates a full backup of these tables. Because each backup is protected by CDP, when a new backup is created, the old one can be deleted&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;text-decoration: underline;&quot;&gt;Understanding Snowflake Stages&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;To support bulk loading of data into tables, Snowflake utilizes stages where the files containing the data to be loaded are stored. Snowflake supports both internal stages and external stages. Data files staged in Snowflake internal stages are not subject to the additional costs associated with Time Travel and Fail-safe, but they do incur standard data storage costs. As such, to help manage storage costs, Snowflake recommends monitoring these files and removing them from the stages once the data has been loaded and the files are no longer needed&lt;/p&gt;
&lt;p&gt;It is also possible to use Snowflake to access data in on-premises storage devices that expose a highly compliant S3 API. With External Stages and External Tables against on-premises storage, customers can make Snowflake their self-service platform for working with data without having to worry about concurrency issues or the effort of managing compute infrastructure. Data governors can apply consistent policies to tables and monitor usage regardless of where the data is physically stored. Analysts and data scientists have a full view of all relevant data, whether it&apos;s on premises or in the cloud, including first-party or even shared, third-party data sets&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;text-decoration: underline;&quot;&gt;Delete what you don&apos;t need&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Similar to the earlier advice to&amp;nbsp;&quot;Don&apos;t do things you don&apos;t need to!&quot;, I&apos;d suggest you don&apos;t keep data that you will never need and/or that you do not want to continuing keeping fresh. While storage is relatively cheap compared to compute, it still can add on to your bill.&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;text-decoration: underline;&quot;&gt;Split large files to minimize processing overhead&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;To distribute the load across the compute resources in an active warehouse, export large files in smaller chunks using a split utility.&amp;nbsp; This will allow Snowflake to divide the workload into parallel threads and load multiple files simultaneously, which will reduce the compute time of your virtual warehouse.&lt;/p&gt;
&lt;p&gt;&lt;span style=&quot;text-decoration: underline;&quot;&gt;Use zero-copy cloning&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;This unique feature lets you create database, table and schema clones which use pointers to the live data and don&apos;t need additional storage.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;As a result, you can save on storage costs and the time it takes to configure the cloned environment.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;Note that by deleting the original table, storage fees transfer to the cloned table. Always delete both the original and cloned tables you&apos;re not using.&lt;/p&gt;
&lt;h3&gt;Conclusions&lt;/h3&gt;
&lt;p&gt;Snowflake is a great platform for Data Warehouse as a Service. It alleviates many of the more traditional DBA tasks I have had to perform on self hosted OLTP databases (SQL Server, MySQL, PostgreSQL, etc.).&amp;nbsp;&lt;/p&gt;
&lt;p&gt;But like any software platform (especially data platforms), we still need to consider continuous improvement that require commitments to monitoring and then possibly selected optimizations. Hopefully some of the suggestions detailed above will be helpful if you are using Snowflake.&lt;/p&gt;

</description>
        <pubDate>Tue, 25 Apr 2023 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/optimizing-snowfalke/</link>
        <guid isPermaLink="true">http://localhost:4000/optimizing-snowfalke/</guid>
        
        <category>snowflake</category>
        
        <category>optimization</category>
        
        <category>costs</category>
        
        <category>data engineering</category>
        
        <category>credits</category>
        
        <category>profiler</category>
        
        
      </item>
    
      <item>
        <title>Dagster with Python, Singer, and Meltano</title>
        <description>&lt;p&gt;I have been a fan of Dagster for data orchestration for a little while and wanted to share some of the basics. There are a lot of cool things I like about it (compared to airflow and other schedulers):&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;It is Declarative (via their &lt;a href=&quot;https://docs.dagster.io/concepts/assets/software-defined-assets&quot; target=&quot;_blank&quot;&gt;Software-Defined Asset&lt;/a&gt; object). I personally like tools and frameworks that allow me to declare a desired end state (Terraform, dbt, Puppet, Ansible, etc.) vs. frameworks that have me build a bunch of imperative tasks that get daisy chained together.&amp;nbsp; &amp;nbsp;&lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;It makes it very easy to define dependencies as function arguments. It reminds a lot of simplicity I see with &lt;em&gt;ref()&lt;/em&gt; statements in dbt.&lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;It feels like it is a better fit for iterative engineering. It has easy support for &lt;a href=&quot;https://docs.dagster.io/guides/dagster/testing-assets&quot; target=&quot;_blank&quot;&gt;writing unit tests&lt;/a&gt; and running the same code/functionality in different environments.&lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;It pretty easily has integration support with many data related steps I would want to apply as part of an asset building pipeline - including tools like dbt&lt;em&gt;,&amp;nbsp;&lt;/em&gt;airbye, meltano, etc.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Goal for this blog post:&lt;/h3&gt;
&lt;p&gt;In this post, I am going to document different ways how I can build pretty simple common pipelines that take a csv and upload the contents to postgres via Dagster using:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Python Code&lt;/li&gt;
&lt;li&gt;Singer&lt;/li&gt;
&lt;li&gt;Meltano&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;Set Up Steps for this demo:&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;We need to install Dasgter + necessary python packages (I am installing it locally, but we could install it in a docker also):&lt;br /&gt;&lt;br /&gt;&amp;nbsp; &amp;nbsp; &amp;nbsp; pip3 install dagster dagit pandas psycopg2&lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;We need to install postgres (also adding pgadmin for web based admin) in dockers:&lt;br /&gt;&lt;br /&gt;&lt;a href=&quot;https://towardsdatascience.com/how-to-run-postgresql-and-pgadmin-using-docker-3a6a8ae918b5&quot; target=&quot;_blank&quot;&gt;https://towardsdatascience.com/how-to-run-postgresql-and-pgadmin-using-docker-3a6a8ae918b5&lt;br /&gt;&lt;br /&gt;&lt;/a&gt;This creates us a local postgres instance with a database &quot;demo_db&quot;.&lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;We need to create a table in the demo_db database:&lt;br /&gt;&lt;br /&gt;
&lt;pre class=&quot;brush: sql;&quot;&gt;CREATE TABLE IF NOT EXISTS public.sales
(
&amp;nbsp; &amp;nbsp; TransactionID text
&amp;nbsp; &amp;nbsp; , Seller text
&amp;nbsp; &amp;nbsp; , Date text
&amp;nbsp; &amp;nbsp; , Value text
&amp;nbsp; &amp;nbsp; , Title text
&amp;nbsp; &amp;nbsp; , Identifier text
&amp;nbsp; &amp;nbsp; , Condition text
&amp;nbsp; &amp;nbsp; , RetailValue text
&amp;nbsp; &amp;nbsp; , ItemValue text
);&lt;/pre&gt;


&lt;/li&gt;
&lt;li&gt;We need some sample data in csv format to upload, so I cerated some sales data for a 1973 Topps Rich Gossage baseball card (below contains a few records):&lt;br /&gt;&lt;br /&gt; TransactionID,Seller,Date,Value,Title,Identifier,Condition,RetailValue,ItemValue &lt;br /&gt;8094231,comicards990,2023-04-12,14.59,1973 Topps Rich &quot;Goose&quot; Gossage Rookie White Sox HOF #174,134520441986,Ungraded,6.00,8.73 &lt;br /&gt;8094232,916lukey31,2023-04-11,10.95,1973 Topps #174 Rich Gossage RC HOF Vg-Ex *Free Shipping*,275699466365,Ungraded,6.00,8.73 &lt;br /&gt;8094233,jayjay5119,2023-04-11,6.50,1973 Topps Baseball! Rich Gossage rookie card! Card174! Chicago White Sox!,195695255305,Ungraded,6.00,8.73&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;Running Dagster with Python&lt;/h3&gt;
&lt;p&gt;1) The first thing I did was to run the scaffolding command to create a new dagster project:&lt;/p&gt;
&lt;p style=&quot;padding-left: 30px;&quot;&gt;dagster project scaffold --name dagster-project&lt;/p&gt;
&lt;p&gt;It created the following directory and files:&lt;/p&gt;
&lt;table class=&quot;table&quot; style=&quot;border-width: 0px; border-style: solid; border-color: inherit; border-image: initial; --tw-border-opacity: 1; --tw-shadow: 0 0 #0000; --tw-ring-inset: var(--tw-empty, ); --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgba(59,130,246,0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; text-indent: 0px; border-collapse: collapse; width: 720px; table-layout: auto; margin-top: 2em; margin-bottom: 2em; font-size: 0.875em; line-height: 1.71429; color: #524e48; font-family: &apos;Neue Montreal&apos;, ui-sans-serif, system-ui, -apple-system, &apos;system-ui&apos;, &apos;Segoe UI&apos;, Roboto, &apos;Helvetica Neue&apos;, Arial, &apos;Noto Sans&apos;, sans-serif, &apos;Apple Color Emoji&apos;, &apos;Segoe UI Emoji&apos;, &apos;Segoe UI Symbol&apos;, &apos;Noto Color Emoji&apos;; letter-spacing: 0.28px; background-color: #faf9f7;&quot;&gt;
&lt;thead style=&quot;box-sizing: border-box; border-width: 0px 0px 1px; border-style: solid; border-bottom-color: #bdbab7; border-image: initial; --tw-border-opacity: 1; --tw-shadow: 0 0 #0000; --tw-ring-inset: var(--tw-empty, ); --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgba(59,130,246,0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; color: #231f1b; font-weight: 600;&quot;&gt;
&lt;tr style=&quot;box-sizing: border-box; border-width: 0px; border-style: solid; border-color: rgba(218,216,214,var(--tw-border-opacity)); border-image: initial; --tw-border-opacity: 1; --tw-shadow: 0 0 #0000; --tw-ring-inset: var(--tw-empty, ); --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgba(59,130,246,0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000;&quot;&gt;&lt;th style=&quot;box-sizing: border-box; border-width: 0px; border-style: solid; border-color: rgba(218,216,214,var(--tw-border-opacity)); border-image: initial; --tw-border-opacity: 1; --tw-shadow: 0 0 #0000; --tw-ring-inset: var(--tw-empty, ); --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgba(59,130,246,0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; vertical-align: bottom; padding-right: 0.571429em; padding-bottom: 0.571429em; padding-left: 0px; width: 180px;&quot;&gt;File/Directory&lt;/th&gt;&lt;th style=&quot;box-sizing: border-box; border-width: 0px; border-style: solid; border-color: rgba(218,216,214,var(--tw-border-opacity)); border-image: initial; --tw-border-opacity: 1; --tw-shadow: 0 0 #0000; --tw-ring-inset: var(--tw-empty, ); --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgba(59,130,246,0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; vertical-align: bottom; padding-right: 0px; padding-bottom: 0.571429em; padding-left: 0.571429em;&quot;&gt;Description&lt;/th&gt;&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody style=&quot;box-sizing: border-box; border-width: 0px; border-style: solid; border-color: rgba(218,216,214,var(--tw-border-opacity)); border-image: initial; --tw-border-opacity: 1; --tw-shadow: 0 0 #0000; --tw-ring-inset: var(--tw-empty, ); --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgba(59,130,246,0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000;&quot;&gt;
&lt;tr style=&quot;box-sizing: border-box; border-width: 0px 0px 1px; border-style: solid; border-bottom-color: #dad8d6; border-image: initial; --tw-border-opacity: 1; --tw-shadow: 0 0 #0000; --tw-ring-inset: var(--tw-empty, ); --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgba(59,130,246,0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000;&quot;&gt;
&lt;td style=&quot;box-sizing: border-box; border-width: 0px; border-style: solid; border-color: rgba(218,216,214,var(--tw-border-opacity)); border-image: initial; --tw-border-opacity: 1; --tw-shadow: 0 0 #0000; --tw-ring-inset: var(--tw-empty, ); --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgba(59,130,246,0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; vertical-align: top; padding: 0.571429em 0.571429em 0.571429em 0px;&quot;&gt;dagster_project/&lt;/td&gt;
&lt;td style=&quot;box-sizing: border-box; border-width: 0px; border-style: solid; border-color: rgba(218,216,214,var(--tw-border-opacity)); border-image: initial; --tw-border-opacity: 1; --tw-shadow: 0 0 #0000; --tw-ring-inset: var(--tw-empty, ); --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgba(59,130,246,0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; vertical-align: top; padding: 0.571429em 0px 0.571429em 0.571429em;&quot;&gt;A Python package that contains your new Dagster code.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr style=&quot;box-sizing: border-box; border-width: 0px 0px 1px; border-style: solid; border-bottom-color: #dad8d6; border-image: initial; --tw-border-opacity: 1; --tw-shadow: 0 0 #0000; --tw-ring-inset: var(--tw-empty, ); --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgba(59,130,246,0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000;&quot;&gt;
&lt;td style=&quot;box-sizing: border-box; border-width: 0px; border-style: solid; border-color: rgba(218,216,214,var(--tw-border-opacity)); border-image: initial; --tw-border-opacity: 1; --tw-shadow: 0 0 #0000; --tw-ring-inset: var(--tw-empty, ); --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgba(59,130,246,0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; vertical-align: top; padding: 0.571429em 0.571429em 0.571429em 0px;&quot;&gt;dagster_project_tests/&lt;/td&gt;
&lt;td style=&quot;box-sizing: border-box; border-width: 0px; border-style: solid; border-color: rgba(218,216,214,var(--tw-border-opacity)); border-image: initial; --tw-border-opacity: 1; --tw-shadow: 0 0 #0000; --tw-ring-inset: var(--tw-empty, ); --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgba(59,130,246,0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; vertical-align: top; padding: 0.571429em 0px 0.571429em 0.571429em;&quot;&gt;A Python package that contains tests for&lt;code style=&quot;box-sizing: border-box; border-width: 0px; border-style: solid; border-color: rgba(218,216,214,var(--tw-border-opacity)); border-image: initial; --tw-border-opacity: 1; --tw-shadow: 0 0 #0000; --tw-ring-inset: var(--tw-empty, ); --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgba(59,130,246,0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; font-family: &apos;Diatype Mono&apos;; font-size: 14px; background: #f5f4f2; color: #231f1b; padding: 4px 6px; border-radius: 4px; overflow-wrap: break-word;&quot;&gt;dagster_project&lt;/code&gt;.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr style=&quot;box-sizing: border-box; border-width: 0px 0px 1px; border-style: solid; border-bottom-color: #dad8d6; border-image: initial; --tw-border-opacity: 1; --tw-shadow: 0 0 #0000; --tw-ring-inset: var(--tw-empty, ); --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgba(59,130,246,0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000;&quot;&gt;
&lt;td style=&quot;box-sizing: border-box; border-width: 0px; border-style: solid; border-color: rgba(218,216,214,var(--tw-border-opacity)); border-image: initial; --tw-border-opacity: 1; --tw-shadow: 0 0 #0000; --tw-ring-inset: var(--tw-empty, ); --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgba(59,130,246,0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; vertical-align: top; padding: 0.571429em 0.571429em 0.571429em 0px;&quot;&gt;README.md&lt;/td&gt;
&lt;td style=&quot;box-sizing: border-box; border-width: 0px; border-style: solid; border-color: rgba(218,216,214,var(--tw-border-opacity)); border-image: initial; --tw-border-opacity: 1; --tw-shadow: 0 0 #0000; --tw-ring-inset: var(--tw-empty, ); --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgba(59,130,246,0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; vertical-align: top; padding: 0.571429em 0px 0.571429em 0.571429em;&quot;&gt;A description and starter guide for your new Dagster project.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr style=&quot;box-sizing: border-box; border-width: 0px 0px 1px; border-style: solid; border-bottom-color: #dad8d6; border-image: initial; --tw-border-opacity: 1; --tw-shadow: 0 0 #0000; --tw-ring-inset: var(--tw-empty, ); --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgba(59,130,246,0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000;&quot;&gt;
&lt;td style=&quot;box-sizing: border-box; border-width: 0px; border-style: solid; border-color: rgba(218,216,214,var(--tw-border-opacity)); border-image: initial; --tw-border-opacity: 1; --tw-shadow: 0 0 #0000; --tw-ring-inset: var(--tw-empty, ); --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgba(59,130,246,0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; vertical-align: top; padding: 0.571429em 0.571429em 0.571429em 0px;&quot;&gt;pyproject.toml&lt;/td&gt;
&lt;td style=&quot;box-sizing: border-box; border-width: 0px; border-style: solid; border-color: rgba(218,216,214,var(--tw-border-opacity)); border-image: initial; --tw-border-opacity: 1; --tw-shadow: 0 0 #0000; --tw-ring-inset: var(--tw-empty, ); --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgba(59,130,246,0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; vertical-align: top; padding: 0.571429em 0px 0.571429em 0.571429em;&quot;&gt;A file that specifies package core metadata in a static, tool-agnostic way.&lt;br style=&quot;box-sizing: border-box; border-width: 0px; border-style: solid; border-color: rgba(218,216,214,var(--tw-border-opacity)); border-image: initial; --tw-border-opacity: 1; --tw-shadow: 0 0 #0000; --tw-ring-inset: var(--tw-empty, ); --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgba(59,130,246,0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000;&quot; /&gt;&lt;br style=&quot;box-sizing: border-box; border-width: 0px; border-style: solid; border-color: rgba(218,216,214,var(--tw-border-opacity)); border-image: initial; --tw-border-opacity: 1; --tw-shadow: 0 0 #0000; --tw-ring-inset: var(--tw-empty, ); --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgba(59,130,246,0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000;&quot; /&gt;This file includes a&amp;nbsp;&lt;code style=&quot;box-sizing: border-box; border-width: 0px; border-style: solid; border-color: rgba(218,216,214,var(--tw-border-opacity)); border-image: initial; --tw-border-opacity: 1; --tw-shadow: 0 0 #0000; --tw-ring-inset: var(--tw-empty, ); --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgba(59,130,246,0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; font-family: &apos;Diatype Mono&apos;; font-size: 14px; background: #f5f4f2; color: #231f1b; padding: 4px 6px; border-radius: 4px; overflow-wrap: break-word;&quot;&gt;tool.dagster&lt;/code&gt;&amp;nbsp;section which references the Python package with your Dagster definitions defined and discoverable at the top level. This allows you to use the&lt;code style=&quot;box-sizing: border-box; border-width: 0px; border-style: solid; border-color: rgba(218,216,214,var(--tw-border-opacity)); border-image: initial; --tw-border-opacity: 1; --tw-shadow: 0 0 #0000; --tw-ring-inset: var(--tw-empty, ); --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgba(59,130,246,0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; font-family: &apos;Diatype Mono&apos;; font-size: 14px; background: #f5f4f2; color: #231f1b; padding: 4px 6px; border-radius: 4px; overflow-wrap: break-word;&quot;&gt;dagster dev&lt;/code&gt;&amp;nbsp;command to load your Dagster code without any parameters. Refer to the&amp;nbsp;&lt;a style=&quot;box-sizing: border-box; border-width: 0px; border-style: solid; border-color: rgba(218,216,214,var(--tw-border-opacity)); border-image: initial; --tw-border-opacity: 1; --tw-shadow: 0 0 #0000; --tw-ring-inset: var(--tw-empty, ); --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgba(59,130,246,0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; color: #4f43dd; text-decoration-line: none; transition: all 0.3s ease 0s; overflow-wrap: break-word;&quot; href=&quot;https://docs.dagster.io/concepts/code-locations&quot;&gt;Code locations documentation&lt;/a&gt;&amp;nbsp;to learn more.&lt;br style=&quot;box-sizing: border-box; border-width: 0px; border-style: solid; border-color: rgba(218,216,214,var(--tw-border-opacity)); border-image: initial; --tw-border-opacity: 1; --tw-shadow: 0 0 #0000; --tw-ring-inset: var(--tw-empty, ); --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgba(59,130,246,0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000;&quot; /&gt;&lt;br style=&quot;box-sizing: border-box; border-width: 0px; border-style: solid; border-color: rgba(218,216,214,var(--tw-border-opacity)); border-image: initial; --tw-border-opacity: 1; --tw-shadow: 0 0 #0000; --tw-ring-inset: var(--tw-empty, ); --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgba(59,130,246,0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000;&quot; /&gt;&lt;span style=&quot;box-sizing: border-box; border-width: 0px; border-style: solid; border-color: rgba(218,216,214,var(--tw-border-opacity)); border-image: initial; --tw-border-opacity: 1; --tw-shadow: 0 0 #0000; --tw-ring-inset: var(--tw-empty, ); --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgba(59,130,246,0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; color: #231f1b;&quot;&gt;Note:&lt;/span&gt;&amp;nbsp;&lt;code style=&quot;box-sizing: border-box; border-width: 0px; border-style: solid; border-color: rgba(218,216,214,var(--tw-border-opacity)); border-image: initial; --tw-border-opacity: 1; --tw-shadow: 0 0 #0000; --tw-ring-inset: var(--tw-empty, ); --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgba(59,130,246,0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; font-family: &apos;Diatype Mono&apos;; font-size: 14px; background: #f5f4f2; color: #231f1b; padding: 4px 6px; border-radius: 4px; overflow-wrap: break-word;&quot;&gt;pyproject.toml&lt;/code&gt;&amp;nbsp;was introduced in&amp;nbsp;&lt;a style=&quot;box-sizing: border-box; border-width: 0px; border-style: solid; border-color: rgba(218,216,214,var(--tw-border-opacity)); border-image: initial; --tw-border-opacity: 1; --tw-shadow: 0 0 #0000; --tw-ring-inset: var(--tw-empty, ); --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgba(59,130,246,0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; color: #4f43dd; text-decoration-line: none; transition: all 0.3s ease 0s; overflow-wrap: break-word;&quot; href=&quot;https://peps.python.org/pep-0518/https://peps.python.org/pep-0518/&quot;&gt;PEP-518&lt;/a&gt;&amp;nbsp;and meant to replace&amp;nbsp;&lt;code style=&quot;box-sizing: border-box; border-width: 0px; border-style: solid; border-color: rgba(218,216,214,var(--tw-border-opacity)); border-image: initial; --tw-border-opacity: 1; --tw-shadow: 0 0 #0000; --tw-ring-inset: var(--tw-empty, ); --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgba(59,130,246,0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; font-family: &apos;Diatype Mono&apos;; font-size: 14px; background: #f5f4f2; color: #231f1b; padding: 4px 6px; border-radius: 4px; overflow-wrap: break-word;&quot;&gt;setup.py&lt;/code&gt;, but we may still include a&amp;nbsp;&lt;code style=&quot;box-sizing: border-box; border-width: 0px; border-style: solid; border-color: rgba(218,216,214,var(--tw-border-opacity)); border-image: initial; --tw-border-opacity: 1; --tw-shadow: 0 0 #0000; --tw-ring-inset: var(--tw-empty, ); --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgba(59,130,246,0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; font-family: &apos;Diatype Mono&apos;; font-size: 14px; background: #f5f4f2; color: #231f1b; padding: 4px 6px; border-radius: 4px; overflow-wrap: break-word;&quot;&gt;setup.py&lt;/code&gt;&amp;nbsp;for compatibility with tools that do not use this spec.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr style=&quot;box-sizing: border-box; border-width: 0px 0px 1px; border-style: solid; border-bottom-color: #dad8d6; border-image: initial; --tw-border-opacity: 1; --tw-shadow: 0 0 #0000; --tw-ring-inset: var(--tw-empty, ); --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgba(59,130,246,0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000;&quot;&gt;
&lt;td style=&quot;box-sizing: border-box; border-width: 0px; border-style: solid; border-color: rgba(218,216,214,var(--tw-border-opacity)); border-image: initial; --tw-border-opacity: 1; --tw-shadow: 0 0 #0000; --tw-ring-inset: var(--tw-empty, ); --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgba(59,130,246,0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; vertical-align: top; padding: 0.571429em 0.571429em 0.571429em 0px;&quot;&gt;setup.py&lt;/td&gt;
&lt;td style=&quot;box-sizing: border-box; border-width: 0px; border-style: solid; border-color: rgba(218,216,214,var(--tw-border-opacity)); border-image: initial; --tw-border-opacity: 1; --tw-shadow: 0 0 #0000; --tw-ring-inset: var(--tw-empty, ); --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgba(59,130,246,0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; vertical-align: top; padding: 0.571429em 0px 0.571429em 0.571429em;&quot;&gt;A build script with Python package dependencies for your new project as a package.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr style=&quot;box-sizing: border-box; border-width: 0px; border-style: solid; border-bottom-color: #dad8d6; border-image: initial; --tw-border-opacity: 1; --tw-shadow: 0 0 #0000; --tw-ring-inset: var(--tw-empty, ); --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgba(59,130,246,0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000;&quot;&gt;
&lt;td style=&quot;box-sizing: border-box; border-width: 0px; border-style: solid; border-color: rgba(218,216,214,var(--tw-border-opacity)); border-image: initial; --tw-border-opacity: 1; --tw-shadow: 0 0 #0000; --tw-ring-inset: var(--tw-empty, ); --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgba(59,130,246,0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; vertical-align: top; padding: 0.571429em 0.571429em 0.571429em 0px;&quot;&gt;setup.cfg&lt;/td&gt;
&lt;td style=&quot;box-sizing: border-box; border-width: 0px; border-style: solid; border-color: rgba(218,216,214,var(--tw-border-opacity)); border-image: initial; --tw-border-opacity: 1; --tw-shadow: 0 0 #0000; --tw-ring-inset: var(--tw-empty, ); --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgba(59,130,246,0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; vertical-align: top; padding: 0.571429em 0px 0.571429em 0.571429em;&quot;&gt;An ini file that contains option defaults for&amp;nbsp;&lt;code style=&quot;box-sizing: border-box; border-width: 0px; border-style: solid; border-color: rgba(218,216,214,var(--tw-border-opacity)); border-image: initial; --tw-border-opacity: 1; --tw-shadow: 0 0 #0000; --tw-ring-inset: var(--tw-empty, ); --tw-ring-offset-width: 0px; --tw-ring-offset-color: #fff; --tw-ring-color: rgba(59,130,246,0.5); --tw-ring-offset-shadow: 0 0 #0000; --tw-ring-shadow: 0 0 #0000; font-family: &apos;Diatype Mono&apos;; font-size: 14px; background: #f5f4f2; color: #231f1b; padding: 4px 6px; border-radius: 4px; overflow-wrap: break-word;&quot;&gt;setup.py&lt;/code&gt;&amp;nbsp;commands.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;2) In that directory we save our sales csv file as:&amp;nbsp;&lt;/p&gt;
&lt;p style=&quot;padding-left: 30px;&quot;&gt;1973_topps_gossage_sales.csv&lt;/p&gt;
&lt;p&gt;3) In that directory, create a file called python_assets.py:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;from dagster import asset # import the `dagster` library

# python libraries we need for these assets
import numpy as np
import psycopg2
import psycopg2.extras as extras
import pandas as pd

# get sales from a csv file

@asset
def get_sales():
    csv_file_path = &apos;./1973_topps_gossage_sales.csv&apos;
    df = pd.read_csv(csv_file_path)
    return df

# write sales to postgres table, calling the dataframe from get_sales
@asset
def write_sales(get_sales):
    conn = psycopg2.connect(
        database=&quot;demo_db&quot;,
        user=&apos;root&apos;,
        password=&apos;root&apos;,
        host=&apos;localhost&apos;,
        port=&apos;5432&apos;,
        options=&quot;-c search_path=dbo,public&quot;
    )

    table = &apos;sales&apos;
    tuples = [tuple(x) for x in get_sales.to_numpy()]

    cols = &apos;,&apos;.join(list(get_sales.columns))
    query = &quot;INSERT INTO %s(%s) VALUES %%s&quot; % (table, cols)
    cursor = conn.cursor()
    try:
        extras.execute_values(cursor, query, tuples)
        conn.commit()
    except (Exception, psycopg2.DatabaseError) as error:
        print(&quot;Error: %s&quot; % error)
        conn.rollback()
        cursor.close()
        return 1
    print(&quot;the dataframe is inserted&quot;)
    cursor.close() 
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;In the above code, we have functions preceded by the &lt;strong&gt;@asset&lt;/strong&gt; decorator. This tells Dagster to identify them as software-defined assets that can be materialized.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;The &lt;strong&gt;get_sales&lt;/strong&gt; asset will read the csv file and populate the dataframe. The&amp;nbsp;&lt;strong&gt;write_sales&lt;/strong&gt; asset writes the contents of dataframe to our Postgres table. Notice the&amp;nbsp;&lt;strong&gt;write_sales(&lt;span style=&quot;color: #ff0000;&quot;&gt;get_sales&lt;/span&gt;)&lt;/strong&gt;&amp;nbsp;signature, that is how Dagster can recognize that&amp;nbsp;&lt;strong&gt;get_sales&lt;/strong&gt;&amp;nbsp;is a dependency for&amp;nbsp;&lt;strong&gt;write_sales&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;PLEASE NOTE: We would never want to hard code database credentials in our python code, below is how you can use environment variables to make it more secure:&amp;nbsp;&amp;nbsp;&lt;br /&gt;&lt;span style=&quot;color: #0000ee; text-decoration-line: underline;&quot;&gt;https://docs.dagster.io/guides/dagster/using-environment-variables-and-secrets&lt;/span&gt;&amp;nbsp;&lt;/p&gt;
&lt;p&gt;4) We can now run the asset pipeline in the Dagster dashboard. You can type the following on the command line to launch the dagit dashboard:&lt;/p&gt;
&lt;p style=&quot;padding-left: 30px;&quot;&gt;dagster dev -f python_assets.py&lt;/p&gt;
&lt;p&gt;The you can visit the following address in your browser to view the dashboard:&lt;/p&gt;
&lt;p&gt;http://127.0.0.1:3000/assets&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/dagster_fun_1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;4) Select the checkboxes in front of both assets and click the &quot;Materialize selected&quot; button. This will execute the assets in the correct order.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;The screen below show the successful materialization of the assets:&amp;nbsp;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/dagster_fun_2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;5) We can log into Postgres (via pgadmin) and see our records written into the &lt;strong&gt;public.sales&lt;/strong&gt; table:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/dagster_fun_3.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h3&gt;&lt;br /&gt;Running Dagster with Singer&lt;/h3&gt;
&lt;p&gt;&lt;a href=&quot;https://www.singer.io/&quot; target=&quot;_blank&quot;&gt;Singer&lt;/a&gt; is an open-source ETL tool from Stitch that lets you write scripts to move data from your sources to their destinations. Singer has two types of scripts&amp;mdash;taps and targets.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A tap is a script, or a piece of code, that connects to your data sources and outputs the data in JSON format.&lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;A target script pipes these data streams from input sources and store them in your data destinations.&amp;nbsp;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;1) In order to migrate our dagster flow to use singer, we will need a tap to read our csv and a target to write to postgres. We can install them into python virtual environments with the below commands:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;python -m venv tap-csv-venv
source tap-csv-venv/bin/activate
pip3 install git+https://github.com/MeltanoLabs/tap-csv.git
alias tap-csv=&quot;tap-csv-venv/bin/tap-csv&quot;
deactivate

python -m venv target-postgres-venv
source target-postgres-venv/bin/activate
pip3 install git+https://github.com/datamill-co/target-postgres.git
alias target-postgres=&quot;target-postgres-venv/bin/target-postgres&quot;
deactivate
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;2) Configure the tap-csv (docs at &lt;a href=&quot;https://github.com/MeltanoLabs/tap-csv&quot; target=&quot;_blank&quot;&gt;https://github.com/MeltanoLabs/tap-csv&lt;/a&gt;):&lt;/p&gt;
&lt;p&gt;- Create a file singer/config.json with this content:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;{
    &quot;csv_files_definition&quot;: &quot;./singer/files_def.json&quot;
}
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;- Create a file singer/files_def.json with this content:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;[
    {   &quot;entity&quot; : &quot;sales&quot;,
        &quot;path&quot; : &quot;1973_topps_gossage_sales.csv&quot;,
        &quot;keys&quot; : [&quot;TransactionID&quot;]
    }
]
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;You can test that the tap will read in your file and convert it to json:&lt;/p&gt;
&lt;p style=&quot;padding-left: 30px;&quot;&gt;&lt;strong&gt;tap-csv --config singer/config.json&lt;/strong&gt;&lt;/p&gt;
&lt;p style=&quot;padding-left: 30px;&quot;&gt;2023-04-03 05:23:13,706 | INFO&amp;nbsp; &amp;nbsp; &amp;nbsp;| tap-csv&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; | Beginning full_table sync of &apos;sales&apos;...&lt;br /&gt;2023-04-03 05:23:13,706 | INFO&amp;nbsp; &amp;nbsp; &amp;nbsp;| tap-csv&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; | Tap has custom mapper. Using 1 provided map(s).&lt;br /&gt;{&quot;type&quot;: &quot;SCHEMA&quot;, &quot;stream&quot;: &quot;sales&quot;, &quot;schema&quot;: {&quot;properties&quot;: {&quot;TransactionID&quot;: {&quot;type&quot;: [&quot;string&quot;, &quot;null&quot;]}, &quot;Seller&quot;: {&quot;type&quot;: [&quot;string&quot;, &quot;null&quot;]}, &quot;Date&quot;: {&quot;type&quot;: [&quot;string&quot;, &quot;null&quot;]}, &quot;Value&quot;: {&quot;type&quot;: [&quot;string&quot;, &quot;null&quot;]}, &quot;Title&quot;: {&quot;type&quot;: [&quot;string&quot;, &quot;null&quot;]}, &quot;Identifier&quot;: {&quot;type&quot;: [&quot;string&quot;, &quot;null&quot;]}, &quot;Condition&quot;: {&quot;type&quot;: [&quot;string&quot;, &quot;null&quot;]}, &quot;RetailValue&quot;: {&quot;type&quot;: [&quot;string&quot;, &quot;null&quot;]}, &quot;ItemValue&quot;: {&quot;type&quot;: [&quot;string&quot;, &quot;null&quot;]}}, &quot;type&quot;: &quot;object&quot;}, &quot;key_properties&quot;: [&quot;TransactionID&quot;]}&lt;br /&gt;{&quot;type&quot;: &quot;RECORD&quot;, &quot;stream&quot;: &quot;sales&quot;, &quot;record&quot;: {&quot;TransactionID&quot;: &quot;8094231&quot;, &quot;Seller&quot;: &quot;comicards990&quot;, &quot;Date&quot;: &quot;2023-04-12&quot;, &quot;Value&quot;: &quot;14.59&quot;, &quot;Title&quot;: &quot;1973 Topps Rich \&quot;Goose\&quot; Gossage Rookie White Sox HOF #174&quot;, &quot;Identifier&quot;: &quot;134520441986&quot;, &quot;Condition&quot;: &quot;Ungraded&quot;, &quot;RetailValue&quot;: &quot;6.00&quot;, &quot;ItemValue&quot;: &quot;8.73&quot;}, &quot;time_extracted&quot;: &quot;2023-04-03T09:23:13.706647+00:00&quot;}&lt;br /&gt;{&quot;type&quot;: &quot;STATE&quot;, &quot;value&quot;: {&quot;bookmarks&quot;: {&quot;sales&quot;: {&quot;starting_replication_value&quot;: null}}}}&lt;br /&gt;{&quot;type&quot;: &quot;RECORD&quot;, &quot;stream&quot;: &quot;sales&quot;, &quot;record&quot;: {&quot;TransactionID&quot;: &quot;8094232&quot;, &quot;Seller&quot;: &quot;916lukey31&quot;, &quot;Date&quot;: &quot;2023-04-11&quot;, &quot;Value&quot;: &quot;10.95&quot;, &quot;Title&quot;: &quot;1973 Topps #174 Rich Gossage RC HOF Vg-Ex *Free Shipping*&quot;, &quot;Identifier&quot;: &quot;275699466365&quot;, &quot;Condition&quot;: &quot;Ungraded&quot;, &quot;RetailValue&quot;: &quot;6.00&quot;, &quot;ItemValue&quot;: &quot;8.73&quot;}, &quot;time_extracted&quot;: &quot;2023-04-03T09:23:13.706848+00:00&quot;}&lt;br /&gt;{&quot;type&quot;: &quot;RECORD&quot;, &quot;stream&quot;: &quot;sales&quot;, &quot;record&quot;: {&quot;TransactionID&quot;: &quot;8094233&quot;, &quot;Seller&quot;: &quot;jayjay5119&quot;, &quot;Date&quot;: &quot;2023-04-11&quot;, &quot;Value&quot;: &quot;6.50&quot;, &quot;Title&quot;: &quot;1973 Topps Baseball! Rich Gossage rookie card! Card174! Chicago White Sox!&quot;, &quot;Identifier&quot;: &quot;195695255305&quot;, &quot;Condition&quot;: &quot;Ungraded&quot;, &quot;RetailValue&quot;: &quot;6.00&quot;, &quot;ItemValue&quot;: &quot;8.73&quot;}, &quot;time_extracted&quot;: &quot;2023-04-03T09:23:13.707571+00:00&quot;}&lt;br /&gt;2023-04-03 05:23:13,707 | INFO&amp;nbsp; &amp;nbsp; &amp;nbsp;| singer_sdk.metrics&amp;nbsp; &amp;nbsp;| INFO METRIC: {&quot;metric_type&quot;: &quot;timer&quot;, &quot;metric&quot;: &quot;sync_duration&quot;, &quot;value&quot;: 0.0012621879577636719, &quot;tags&quot;: {&quot;stream&quot;: &quot;sales&quot;, &quot;context&quot;: {}, &quot;status&quot;: &quot;succeeded&quot;}}&lt;br /&gt;2023-04-03 05:23:13,708 | INFO&amp;nbsp; &amp;nbsp; &amp;nbsp;| singer_sdk.metrics&amp;nbsp; &amp;nbsp;| INFO METRIC: {&quot;metric_type&quot;: &quot;counter&quot;, &quot;metric&quot;: &quot;record_count&quot;, &quot;value&quot;: 3, &quot;tags&quot;: {&quot;stream&quot;: &quot;sales&quot;, &quot;context&quot;: {}}}&lt;br /&gt;{&quot;type&quot;: &quot;STATE&quot;, &quot;value&quot;: {&quot;bookmarks&quot;: {&quot;sales&quot;: {}}}}&lt;br /&gt;{&quot;type&quot;: &quot;STATE&quot;, &quot;value&quot;: {&quot;bookmarks&quot;: {&quot;sales&quot;: {}}}}&lt;/p&gt;
&lt;p&gt;2) Configure the taraget-postgres (docs at&amp;nbsp;&lt;a href=&quot;https://github.com/datamill-co/target-postgres&quot; target=&quot;_blank&quot;&gt;https://github.com/datamill-co/target-postgres&lt;/a&gt;):&lt;/p&gt;

&lt;p&gt;- Create a file singer/target_postgres_config.json with this content:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;{
    &quot;postgres_host&quot;: &quot;localhost&quot;,
    &quot;postgres_port&quot;: 5432,
    &quot;postgres_database&quot;: &quot;demo_db&quot;,
    &quot;postgres_username&quot;: &quot;root&quot;,
    &quot;postgres_password&quot;: &quot;root&quot;,
    &quot;postgres_schema&quot;: &quot;public&quot;
}
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;You can test that the tap and target will read in your file and upload it to postgres:&lt;/p&gt;
&lt;p style=&quot;padding-left: 30px;&quot;&gt;&lt;strong&gt;tap-csv --config singer/config.json | target-postgres --config singer/target_postgres_config.json&amp;nbsp;&lt;br /&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p style=&quot;padding-left: 30px;&quot;&gt;INFO PostgresTarget created with established connection: `user=root password=xxx dbname=demo_db host=localhost port=5432 application_name=target-postgres`, PostgreSQL schema: `public`&lt;/p&gt;
&lt;p style=&quot;padding-left: 30px;&quot;&gt;INFO Sending version information to singer.io. To disable sending anonymous usage data, set the config parameter &quot;disable_collection&quot; to true&lt;/p&gt;
&lt;p style=&quot;padding-left: 30px;&quot;&gt;2023-04-03 13:09:44,466 | INFO&amp;nbsp; &amp;nbsp; &amp;nbsp;| tap-csv&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; | Beginning full_table sync of &apos;sales&apos;...&lt;br /&gt;2023-04-03 13:09:44,467 | INFO&amp;nbsp; &amp;nbsp; &amp;nbsp;| tap-csv&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; | Tap has custom mapper. Using 1 provided map(s).&lt;br /&gt;2023-04-03 13:09:44,467 | INFO&amp;nbsp; &amp;nbsp; &amp;nbsp;| singer_sdk.metrics&amp;nbsp; &amp;nbsp;| INFO METRIC: {&quot;metric_type&quot;: &quot;timer&quot;, &quot;metric&quot;: &quot;sync_duration&quot;, &quot;value&quot;: 0.000537872314453125, &quot;tags&quot;: {&quot;stream&quot;: &quot;sales&quot;, &quot;context&quot;: {}, &quot;status&quot;: &quot;succeeded&quot;}}&lt;br /&gt;2023-04-03 13:09:44,467 | INFO&amp;nbsp; &amp;nbsp; &amp;nbsp;| singer_sdk.metrics&amp;nbsp; &amp;nbsp;| INFO METRIC: {&quot;metric_type&quot;: &quot;counter&quot;, &quot;metric&quot;: &quot;record_count&quot;, &quot;value&quot;: 3, &quot;tags&quot;: {&quot;stream&quot;: &quot;sales&quot;, &quot;context&quot;: {}}}&lt;br /&gt;INFO Mapping: test to None&lt;br /&gt;INFO Mapping: sales to [&apos;sales&apos;]&lt;br /&gt;INFO Mapping: tp_sales_transactionid__sdc_sequence_idx to None&lt;br /&gt;INFO Stream sales (sales) with max_version None targetting None&lt;br /&gt;INFO Root table name sales&lt;br /&gt;INFO Writing batch with 3 records for `sales` with `key_properties`: `[&apos;TransactionID&apos;]`&lt;br /&gt;INFO Writing table batch schema for `(&apos;sales&apos;,)`...&lt;br /&gt;INFO METRIC: {&quot;type&quot;: &quot;timer&quot;, &quot;metric&quot;: &quot;job_duration&quot;, &quot;value&quot;: 0.048573970794677734, &quot;tags&quot;: {&quot;job_type&quot;: &quot;upsert_table_schema&quot;, &quot;path&quot;: [&quot;sales&quot;], &quot;database&quot;: &quot;demo_db&quot;, &quot;schema&quot;: &quot;public&quot;, &quot;table&quot;: &quot;sales&quot;, &quot;status&quot;: &quot;succeeded&quot;}}&lt;br /&gt;INFO Writing table batch with 3 rows for `(&apos;sales&apos;,)`...&lt;br /&gt;INFO METRIC: {&quot;type&quot;: &quot;counter&quot;, &quot;metric&quot;: &quot;record_count&quot;, &quot;value&quot;: 3, &quot;tags&quot;: {&quot;count_type&quot;: &quot;table_rows_persisted&quot;, &quot;path&quot;: [&quot;sales&quot;], &quot;database&quot;: &quot;demo_db&quot;, &quot;schema&quot;: &quot;public&quot;, &quot;table&quot;: &quot;sales&quot;}}&lt;br /&gt;INFO METRIC: {&quot;type&quot;: &quot;timer&quot;, &quot;metric&quot;: &quot;job_duration&quot;, &quot;value&quot;: 0.10874700546264648, &quot;tags&quot;: {&quot;job_type&quot;: &quot;table&quot;, &quot;path&quot;: [&quot;sales&quot;], &quot;database&quot;: &quot;demo_db&quot;, &quot;schema&quot;: &quot;public&quot;, &quot;table&quot;: &quot;sales&quot;, &quot;status&quot;: &quot;succeeded&quot;}}&lt;br /&gt;INFO METRIC: {&quot;type&quot;: &quot;counter&quot;, &quot;metric&quot;: &quot;record_count&quot;, &quot;value&quot;: 3, &quot;tags&quot;: {&quot;count_type&quot;: &quot;batch_rows_persisted&quot;, &quot;path&quot;: [&quot;sales&quot;], &quot;database&quot;: &quot;demo_db&quot;, &quot;schema&quot;: &quot;public&quot;}}&lt;br /&gt;INFO METRIC: {&quot;type&quot;: &quot;timer&quot;, &quot;metric&quot;: &quot;job_duration&quot;, &quot;value&quot;: 0.10944700241088867, &quot;tags&quot;: {&quot;job_type&quot;: &quot;batch&quot;, &quot;path&quot;: [&quot;sales&quot;], &quot;database&quot;: &quot;demo_db&quot;, &quot;schema&quot;: &quot;public&quot;, &quot;status&quot;: &quot;succeeded&quot;}}&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;span style=&quot;text-decoration: underline;&quot;&gt;Some things to take note of:&lt;/span&gt;&lt;br /&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;- We defined the entity = &quot;sales&quot; in the tap.&amp;nbsp; So that dictates the table_name for the target.&lt;/p&gt;
&lt;p&gt;- We see in the output &lt;strong&gt;&quot;job_type&quot;: &quot;upsert_table_schema&quot;&lt;/strong&gt;, this means that write to postgres will do an upsert of the record based on the keys we defined in tap (&quot;keys&quot; : [&quot;TransactionID&quot;]).&lt;/p&gt;
&lt;p&gt;3)&amp;nbsp;In that directory, create a file called python_assets.py:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;# import the `dagster` library
from dagster import asset

# python libraries we need for these assets
import subprocess

# get sales from a csv file and write to postgres via singer tap and target
@asset
def get_and_write_sales_with_singer():
    ps = subprocess.Popen([&apos;~/dagster/dagster-project/tap-csv-venv/bin/tap-csv&apos;, &apos;--config&apos;, &apos;singer/config.json&apos;],
    stdout=subprocess.PIPE) output = subprocess.run([&apos;~/dagster/dagster-project/target-postgres-venv/bin/target-postgres&apos;, &apos;--config&apos;, &apos;singer/target_postgres_config.json&apos;], stdin=ps.stdout)
    ps.wait() print(output.stdout)
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;4) We can now run the asset pipeline in the Dagster dashboard. You can type the following on the command line to launch the dagit dashboard:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;dagster dev -f singer_assets.py
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The you can visit the following address in your browser to view the dashboard:&lt;/p&gt;
&lt;p&gt;http://127.0.0.1:3000/assets&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/dagster_fun_4.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;5) Select the checkboxes in front of both assets and click the &quot;Materialize selected&quot; button. This will execute the assets in the correct order.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;The screen below show the successful materialization of the assets:&amp;nbsp;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/dagster_fun_5.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&amp;nbsp;&lt;br /&gt;6) We can log into Postgres (via pgadmin) and see our records written into the&amp;nbsp;&lt;strong&gt;public.sales&lt;/strong&gt;&amp;nbsp;table:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/dagster_fun_6.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h3&gt;&lt;br /&gt;Running Dagster with Meltano&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;https://meltano.com/&quot; target=&quot;_blank&quot;&gt;Meltano&lt;/a&gt; is an open source tool which can be used to extract from data sources and load it to destinations like your data warehouse. It uses extractors and loaders written in the Singer open source standard.&lt;/p&gt;

&lt;p&gt;1) In order to migrate our dagster flow to use meltano, we will need to install meltano:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;pip3 install meltano
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;2) Configure a new meltano project and switch into the directory:&amp;nbsp;&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;meltano init meltano
cd meltano
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;3) We need to install the tap and target:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;meltano add extractor tap-csv
meltano add loader target-postgres --variant&amp;amp;nbsp;datamill-co
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;4) Adding a job that include the tap and target:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;meltano job add demo_job --tasks &quot;tap-csv target-postgres&quot;
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;4) Configure the following in meltano.yml (it should look very similar to the singer configuration from the Singer section):&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;version: 1
default_environment: dev
project_id: fb9afb3c-5560-406e-b977-d86eef949779
environments:
- name: dev
- name: staging
- name: prod
plugins:
extractors:
- name: tap-csv
    variant: meltanolabs
    pip_url: git+https://github.com/MeltanoLabs/tap-csv.git
    config:
    files:
        - entity: sales
        file: ../1973_topps_gossage_sales.csv
        keys:
            - TransactionID
loaders:
- name: target-postgres
    variant: datamill-co
    pip_url: git+https://github.com/datamill-co/target-postgres.git
    config:
    host: localhost
    port: 5432
    user: root
    password: root
    dbname: demo_db
    default_target_schema: public
jobs:
- name: demo_job
tasks:
- tap-csv target-postgres
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;5) Run meltano on the command line:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;meltano run demo_job
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p style=&quot;padding-left: 30px;&quot;&gt;2023-04-03T18:06:17.336263Z [info&amp;nbsp; &amp;nbsp; &amp;nbsp;] Environment &apos;dev&apos; is active&lt;br /&gt;2023-04-03T18:06:19.212684Z [info&amp;nbsp; &amp;nbsp; &amp;nbsp;] INFO Starting sync&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;cmd_type=elb consumer=False name=tap-csv producer=True stdio=stderr string_id=tap-csv&lt;br /&gt;2023-04-03T18:06:19.212939Z [info&amp;nbsp; &amp;nbsp; &amp;nbsp;] INFO Syncing entity &apos;sales&apos; from file: &apos;../1973_topps_gossage_sales.csv&apos; cmd_type=elb consumer=False name=tap-csv producer=True stdio=stderr string_id=tap-csv&lt;br /&gt;2023-04-03T18:06:19.213097Z [info&amp;nbsp; &amp;nbsp; &amp;nbsp;] INFO Sync completed&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; cmd_type=elb consumer=False name=tap-csv producer=True stdio=stderr string_id=tap-csv&lt;br /&gt;2023-04-03T18:06:19.577999Z [info&amp;nbsp; &amp;nbsp; &amp;nbsp;] time=2023-04-03 14:06:19 name=target_postgres level=INFO message=Table &apos;&quot;sales&quot;&apos; exists cmd_type=elb consumer=True name=target-postgres producer=False stdio=stderr string_id=target-postgres&lt;br /&gt;2023-04-03T18:06:19.711365Z [info&amp;nbsp; &amp;nbsp; &amp;nbsp;] time=2023-04-03 14:06:19 name=target_postgres level=INFO message=Loading 3 rows into &apos;public.&quot;sales&quot;&apos; cmd_type=elb consumer=True name=target-postgres producer=False stdio=stderr string_id=target-postgres&lt;br /&gt;2023-04-03T18:06:19.840375Z [info&amp;nbsp; &amp;nbsp; &amp;nbsp;] time=2023-04-03 14:06:19 name=target_postgres level=INFO message=Loading into public.&quot;sales&quot;: {&quot;inserts&quot;: 0, &quot;updates&quot;: 3, &quot;size_bytes&quot;: 452} cmd_type=elb consumer=True name=target-postgres producer=False stdio=stderr string_id=target-postgres&lt;br /&gt;2023-04-03T18:06:19.862895Z [info&amp;nbsp; &amp;nbsp; &amp;nbsp;] Incremental state has been updated at 2023-04-03 18:06:19.862840.&lt;br /&gt;2023-04-03T18:06:19.871775Z [info&amp;nbsp; &amp;nbsp; &amp;nbsp;] Block run completed.&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;block_type=ExtractLoadBlocks err=None set_number=0 success=True&lt;/p&gt;

&lt;p&gt;6) Install the Dagster-Meltano library&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;cd ../
pip3 install dagster-meltano
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;7) In that directory, create a file called meltano_assets.py:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
2
3
4
5
6
7
8
9
10
11
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;from dagster import Definitions, job
from dagster_meltano import meltano_resource, meltano_run_op 

@job(resource_defs={&quot;meltano&quot;: meltano_resource})
def run_job():
tap_done = meltano_run_op(&quot;demo_job&quot;)() 

# alternatively we could run this
# tap_done = meltano_run_op(&quot;tap-csv target-postgres&quot;)()

defs = Definitions(jobs=[run_job])
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;8) We can now run the asset pipeline in the Dagster dashboard. You can type the following on the command line to launch the dagit dashboard:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;table class=&quot;rouge-table&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;rouge-gutter gl&quot;&gt;&lt;pre class=&quot;lineno&quot;&gt;1
&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;rouge-code&quot;&gt;&lt;pre&gt;dagster dev -f meltano_assets.py
&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/tbody&gt;&lt;/table&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
</description>
        <pubDate>Tue, 04 Apr 2023 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/dagster-fun/</link>
        <guid isPermaLink="true">http://localhost:4000/dagster-fun/</guid>
        
        <category>orchestration</category>
        
        <category>data engineering</category>
        
        <category>dagster</category>
        
        <category>python</category>
        
        <category>singer</category>
        
        <category>meltano</category>
        
        <category>workflow</category>
        
        
      </item>
    
      <item>
        <title>My Snowflake Set up with Terraform</title>
        <description>&lt;p&gt;I have been working with &lt;a href=&quot;https://www.snowflake.com/en/&quot; target=&quot;_blank&quot;&gt;Snowflake&lt;/a&gt; since 2016 when I proposed and chose to bring it into Rent the Runway to replace our very painful on premise Vertica implementation (yes we had it in a data center in NJ). Since then, Snowflake has grown considerably and is now one of the leading Data Warehouse offerings.&lt;br /&gt;&lt;br /&gt;After implementing data warehouses at several companies and having lots of conversations with some very smart folks, I&apos;ve learned some things along the way. I&apos;ve found it is a really good idea to think about up front how I want to segment responsibilities and permissions for databases. I want to decide what types of data to store in different places and to build roles + permission grants to enforce those decisions.&amp;nbsp;&lt;/p&gt;
&lt;h3&gt;My Best Practices Assumptions for segmenting data:&lt;/h3&gt;
&lt;p&gt;Best practice for running a cloud data warehouse is to build separate repositories for:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;raw data from source systems and providers&lt;/li&gt;
&lt;li&gt;schemas where data is regularly transformed/generated and documented to be consumed for reporting and analysis&lt;/li&gt;
&lt;li&gt;area to data exploration, modeling, and experimentation&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;To ensure that these logical areas are used for their defined purposes, we can create specific roles and permissions for each.&amp;nbsp;&lt;/p&gt;
&lt;h3&gt;Logical set up of Databases with Role Permissions&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/sf_terrafrom_1.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;
&lt;table style=&quot;width: 600px;&quot; border=&quot;1&quot; cellpadding=&quot;5&quot;&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td style=&quot;padding: 5px; vertical-align: top;&quot;&gt;&lt;strong&gt;database&lt;/strong&gt;&lt;/td&gt;
&lt;td style=&quot;padding: 5px; vertical-align: top;&quot;&gt;&lt;strong&gt;functional description&lt;/strong&gt;&lt;/td&gt;
&lt;td style=&quot;padding: 5px; vertical-align: top;&quot;&gt;&lt;strong&gt;privileges&lt;/strong&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&quot;padding: 5px; vertical-align: top;&quot;&gt;Raw&lt;/td&gt;
&lt;td style=&quot;padding: 5px; vertical-align: top;&quot;&gt;
&lt;ul&gt;
&lt;li&gt;Data is loaded from source systems and providers in its original (non transformed) format&lt;/li&gt;
&lt;li&gt;No users can directly query this data&lt;/li&gt;
&lt;/ul&gt;
&lt;/td&gt;
&lt;td style=&quot;padding: 5px; vertical-align: top;&quot;&gt;
&lt;ul&gt;
&lt;li&gt;load_role - full access&amp;nbsp;&lt;/li&gt;
&lt;li&gt;transform_role - read&lt;/li&gt;
&lt;/ul&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&quot;padding: 5px; vertical-align: top;&quot;&gt;Analytics&lt;/td&gt;
&lt;td style=&quot;padding: 5px; vertical-align: top;&quot;&gt;
&lt;ul&gt;
&lt;li&gt;Data is transformed or generated (on a regular cadence) and documented to support reporting and analysis needs.&lt;/li&gt;
&lt;li&gt;Users can directly query this data, but can not write/update&lt;/li&gt;
&lt;li&gt;Metabase (and other reporting tools) would read from this database&lt;/li&gt;
&lt;/ul&gt;
&lt;/td&gt;
&lt;td style=&quot;padding: 5px; vertical-align: top;&quot;&gt;
&lt;ul&gt;
&lt;li&gt;transform_role - full access&lt;/li&gt;
&lt;li&gt;report_role - read&lt;/li&gt;
&lt;/ul&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td style=&quot;padding: 5px; vertical-align: top;&quot;&gt;Work&lt;/td&gt;
&lt;td style=&quot;padding: 5px; vertical-align: top;&quot;&gt;
&lt;ul&gt;
&lt;li&gt;Serves as an area to data exploration, modeling, and experimentation&lt;/li&gt;
&lt;li&gt;Users can access, create/load, and change data in this area&lt;/li&gt;
&lt;/ul&gt;
&lt;/td&gt;
&lt;td style=&quot;padding: 5px; vertical-align: top;&quot;&gt;
&lt;ul&gt;
&lt;li&gt;analyst_role - full access&amp;nbsp;&lt;/li&gt;
&lt;/ul&gt;
&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;
&lt;p&gt;Other key assumptions with this set up:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Each Snowflake Role has its own dedicated compute (Snowflake Warehouse). &lt;br /&gt;For instance the LOAD_ROLE can only run on a warehouse named LOAD_WH.&lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;The Raw database should contain a schema for each source system.&amp;nbsp;&lt;br /&gt;For example, if we are syncing data from our ecommerce site with a Mongo backend, we would name the schema as MONGO_ECOMMERCE.&amp;nbsp;&lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;In the Analytics database, we define 5 schemas:&lt;br /&gt;- &lt;strong&gt;src&lt;/strong&gt;:&amp;nbsp; holding dbt models of data that has been lightly transformed from the original source data. Here we may flatten nested variant data into a relational model.&lt;br /&gt;- &lt;strong&gt;trans&lt;/strong&gt;:&amp;nbsp;schema for intermediate dbt models &lt;br /&gt;- &lt;strong&gt;rpt&lt;/strong&gt;: holding dbt dimensional models that drive our BI reporting&lt;br /&gt;- &lt;strong&gt;export&lt;/strong&gt;: holding tables/views that are for data to be exported to systems outside of Snowflake &lt;br /&gt;-&amp;nbsp;&lt;strong&gt;db_stats&lt;/strong&gt;: metadata about our dbt models like total runtime, last runtime, number of rows added incrementally, etc&lt;/li&gt;
&lt;li&gt;The specific tools in the diagram above are meant to be examples for certain roles:&lt;br /&gt;- Fivetran to sync raw data from source systems into RAW (could be meltano, airbyte)&lt;br /&gt;- dbt to create models in ANALYTICS (could be ML scripts via dagster/airflow)&lt;br /&gt;- Metabase to read from ANALYTICS for BI (could be Looker, Tableau, Superset)&lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;You need to consider the processes for Development and Testing of Pipelines + Models. This diagram does not describe how I would approach these topics.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3&gt;Snowflake configuration management with Terraform&lt;/h3&gt;
&lt;p&gt;&lt;a href=&quot;https://www.terraform.io/&quot; target=&quot;_blank&quot;&gt;Terraform&lt;/a&gt; (created by Hashicorp) is an open-source infrastructure as code software tool that enables you to safely and predictably create, change, and improve infrastructure.&lt;/p&gt;
&lt;h3&gt;Chan Zuckerberg terraform provider for Snowflake&lt;/h3&gt;
&lt;p&gt;This is a terraform provider plugin for managing Snowflake accounts: &lt;br /&gt;GitHub -&amp;nbsp;&lt;a href=&quot;https://github.com/Snowflake-Labs/terraform-provider-snowflake&quot; target=&quot;_blank&quot;&gt;https://github.com/Snowflake-Labs/terraform-provider-snowflake&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Documentation available here: &lt;br /&gt;&lt;a href=&quot;https://registry.terraform.io/providers/chanzuckerberg/snowflake/latest/docs&quot; target=&quot;_blank&quot;&gt;Terraform Registry&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;We can use this framework to manage objects in snowflake such as:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;databases&lt;/li&gt;
&lt;li&gt;roles&lt;/li&gt;
&lt;li&gt;schemas&lt;/li&gt;
&lt;li&gt;user accounts&lt;/li&gt;
&lt;li&gt;permission grants&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Github repo with my terraform configuration for this set up&lt;/h3&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/jaygrossman/snowflake_terraform_setup&quot; target=&quot;_blank&quot;&gt;https://github.com/jaygrossman/snowflake_terraform_setup&lt;/a&gt;&amp;nbsp;&amp;nbsp;&lt;/p&gt;
&lt;p&gt;PLEASE NOTE: This scope for this repo does not include the terrafom configuration to set up snowflake stages, functions, and file formatters. These objects often have more dependencies and require more advanced configuration, so I may plan to dedicate future blog posts to explaining the details.&amp;nbsp;&lt;/p&gt;
&lt;h3&gt;Folks who this would not have been possible without:&lt;/h3&gt;
&lt;table&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;img src=&quot;/assets/images/collin.jpg&quot; alt=&quot;&quot; width=&quot;200&quot; /&gt;&lt;/td&gt;
&lt;td&gt;&lt;img src=&quot;/assets/images/rob.jpg&quot; alt=&quot;&quot; width=&quot;200&quot; /&gt;&lt;/td&gt;
&lt;td&gt;&lt;img src=&quot;/assets/images/tim.jpg&quot; alt=&quot;&quot; width=&quot;200&quot; /&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;a href=&quot;https://www.linkedin.com/in/collinmeyers/&quot; target=&quot;_blank&quot;&gt;Collin Meyers&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=&quot;https://www.linkedin.com/in/rob-sokolowski-35b25318/&quot; target=&quot;_blank&quot;&gt;Rob Sokolowski&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;&lt;a href=&quot;https://www.linkedin.com/in/timricablanca/&quot; target=&quot;_blank&quot;&gt;Tim Ricablanca&lt;/a&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
</description>
        <pubDate>Sat, 28 Jan 2023 00:00:00 -0500</pubDate>
        <link>http://localhost:4000/snowflake-setup-with-terraform/</link>
        <guid isPermaLink="true">http://localhost:4000/snowflake-setup-with-terraform/</guid>
        
        <category>snowflake</category>
        
        <category>terraform</category>
        
        <category>data engineering</category>
        
        
      </item>
    
      <item>
        <title>Fun First Project with my 3D Printer</title>
        <description>&lt;p&gt;I have long been curious about 3D printers and my daughter has been playing with CAD programs for her middle school STEM Club.&amp;nbsp; So I decided to do some research and buy the &lt;a href=&quot;https://www.amazon.com/gp/product/B07D18L9K6/ref=as_li_tl?ie=UTF8&amp;amp;camp=1789&amp;amp;creative=9325&amp;amp;creativeASIN=B07D18L9K6&amp;amp;linkCode=as2&amp;amp;tag=hipstir-20&amp;amp;linkId=e0567b354d0dc701ea13d6a8607c88d5&quot; target=&quot;_blank&quot;&gt;Creality Ender 3 3D Printer Fully Open Source with Resume Printing DIY FDM Printer 220x220x250mm&lt;/a&gt; off Amazon for $165. I also bought 2 1KG spools of PLA Filament so I could print things.&lt;/p&gt;
&lt;p&gt;I am completely new to 3D printers, but i have put together my share of projects and I do engineering for my day job. I decided that 10pm Christmas Eve would be a good time to start assembling the printer, probably not the best decision. I open the box and there were a lot of pieces+tools and a single sheet with directions with only pictures. Two and a half hours later I had the printer assembled (I am pretty sure Santa&apos;s elves or someone experienced would have done it an hour)&lt;/p&gt;

&lt;h3&gt;&lt;br /&gt;First Project - Toothpaste Squeezer&lt;/h3&gt;
&lt;p&gt;You ever get near the end of a tube of toothpaste and struggle squeezing out the last few uses? This is an annoyance for members of my family, so I figured this could be a good first project. So here&apos;s what I did:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;I went on Thingiverse.com and they have all sorts of amazing things you can 3D print (including the CAD files). They had several projects to choose from when I searched for toothpaste, and I chose the &lt;a href=&quot;https://www.thingiverse.com/thing:867811&quot; target=&quot;_blank&quot;&gt;Simple Toothpaste Squeezer&lt;/a&gt;. Since it was my first project, I wanted something that was solid and with only 1 part to print.&lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;I download the .slt file from the project.&lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;I needed to convert the .slt file to a .gcode to use with my printer. I downloaded a great piece of free software that it seems like many people use - &lt;a href=&quot;https://ultimaker.com/software/ultimaker-cura&quot; target=&quot;_blank&quot;&gt;Ultimaker Cura&lt;/a&gt;. In Cura, I was able to choose my printer (the Ember 3) to automatically set up the file with correct settings and then &quot;Slice&quot; the file to convert it to .gcode. It also shows you how much filament will be used and how long the print will take.&lt;br /&gt;&lt;br /&gt;

&lt;img src=&quot;/assets/images/3d_printer_1.png&quot; alt=&quot;&quot; /&gt;&lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;I saved the converted .gcode file to the provided SD card, inserted the card into the printer, and started my print.&lt;/li&gt;
&lt;/ol&gt;
&lt;table cellspacing=&quot;5&quot; cellspacinging=&quot;3&quot; cellpadding=&quot;0&quot;&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;img src=&quot;/assets/images/3d_printer_2.jpg&quot; alt=&quot;&quot; /&gt;&lt;/td&gt;
&lt;td&gt;&amp;nbsp;&lt;/td&gt;
&lt;td valign=&quot;top&quot;&gt;The printer begins drawing the outline of the project. It went around the edges and the holes many times.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;img src=&quot;/assets/images/3d_printer_3.jpg&quot; alt=&quot;&quot; /&gt;&lt;/td&gt;
&lt;td&gt;&amp;nbsp;&lt;/td&gt;
&lt;td valign=&quot;top&quot;&gt;After the base layer was done, the printer began creating a honeycomb pattern as the Squeezer is not fully solid inside. I wasn&apos;t expecting this (being new to 3D printing) and thought it was worth sharing.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td colspan=&quot;3&quot;&gt;
&lt;h3&gt;&lt;br /&gt;The Finished Product:&lt;/h3&gt;
The printing took about 45 minutes. When it was done, I popped the squeezer off the bed and the plastic felt pretty solid.&amp;nbsp; So here is a picture of me trying it out (there&apos;s no hand modeling in my future):&lt;br /&gt;&lt;br /&gt; &lt;img src=&quot;/assets/images/3d_printer_4.jpg&quot; alt=&quot;&quot; /&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;
&lt;h3&gt;My Very Early Learnings:&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;I &lt;strong&gt;REALLY&lt;/strong&gt; wish I had found this video before starting this adventure. It would have saved me so much time and answered so many of my questions. If you buy an Ember 3, watch this:&lt;br /&gt;&lt;br /&gt;&lt;iframe src=&quot;https://www.youtube.com/embed/dQ0q9zLygTY&quot; frameborder=&quot;0&quot; width=&quot;560&quot; height=&quot;315&quot;&gt;&lt;/iframe&gt;&lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;There is a setting for the Ember 3 for voltage settings. I was a little disturbed when the machine would not power up until I switched it from 230V to 115V.&lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;Make sure everything is clean between prints. Make sure the extruder (where the filament comes out of)&amp;nbsp;is cleaned out, especially if you are changing colors.&amp;nbsp;&lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;Although it is a tedious pain, aligning the bed to the proper height before every print is really important (the video shows how to do this). I botched some subsequent print attempts when I didn&apos;t get the extruder close enough to the bed and the plastic moved all over the place.&lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;I love &lt;a href=&quot;https://www.thingiverse.com/&quot; target=&quot;_blank&quot;&gt;Thingiverse&lt;/a&gt;!!! The designs, comments, and remixes are just awesome. There is such nerdy awesomeness because people 3D print &lt;a href=&quot;https://www.thingiverse.com/search?type=things&amp;amp;q=&amp;amp;sort=popular&amp;amp;posted_after=now-30d&amp;amp;category_id=127&amp;amp;page=1&quot; target=&quot;_blank&quot;&gt;accessories to pimp out their 3D printers&lt;/a&gt;. I am going to add some things this week.&lt;br /&gt;&lt;br /&gt;&lt;/li&gt;
&lt;li&gt;My daughter made a few cool smaller designs with &lt;a href=&quot;https://www.tinkercad.com/&quot; target=&quot;_blank&quot;&gt;TinkerCAD&lt;/a&gt;.&amp;nbsp;She saved her designs as .slt files, we converted them with Cura, and they were great when we printed them out.&lt;/li&gt;
&lt;/ol&gt;
</description>
        <pubDate>Sat, 26 Dec 2020 00:00:00 -0500</pubDate>
        <link>http://localhost:4000/fun-first-project-with-my-3d-printer/</link>
        <guid isPermaLink="true">http://localhost:4000/fun-first-project-with-my-3d-printer/</guid>
        
        
        <category>code</category>
        
      </item>
    
      <item>
        <title>Automating Image Processing with Python</title>
        <description>&lt;p&gt;If you have been reading this blog, you&apos;ll know I collect sports cards. It&apos;s fun to share what I have with other collectors (by posting scans on sportscollectors.net, facebook, etc.).&lt;/p&gt;
&lt;p&gt;A few years ago I bought a &lt;a title=&quot;Brother MFC-9130CW printer&quot; href=&quot;https://www.amazon.com/gp/product/B00C6MNP52/ref=as_li_tl?ie=UTF8&amp;amp;camp=1789&amp;amp;creative=9325&amp;amp;creativeASIN=B00C6MNP52&amp;amp;linkCode=as2&amp;amp;tag=hipstir-20&amp;amp;linkId=53bf0932815fceb1355ecfab28f5c264&quot; target=&quot;_blank&quot;&gt;Brother MFC-9130CW&lt;/a&gt; all in one printer/scanner that I use to do my scanning. I usually set it to scan documents in legal format so I can fit 9 cards on a scan (3 rows by 3 columns). And since I want to do this most efficiently, I generally save them as a 200dpi pdf file with multiple pages.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The requirements:&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;We will have a multipage pdf, with each page containing a 3x3 grid of equal sized card images. I will use this sample file for this exercise:&amp;nbsp;&lt;a href=&quot;/FILES%2f2020%2f07%2fsample_cards_file.pdf.axdx&quot; target=&quot;_blank&quot;&gt;sample_cards_file.pdf (3.02 mb)&lt;br /&gt;&lt;br /&gt;&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;We will need each image cropped to show the full card (we can have some extra space around it) and saved to it&apos;s own jpeg file. An example is show below:&lt;br /&gt;&lt;br /&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/1989_hoops_english.jpg&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;br /&gt;&amp;nbsp;&lt;/li&gt;
&lt;li&gt;I will supply a directory path containing the outputted images. Since I will put them in my inventory database, I&apos;ll need to supply the starting number for the image names and have each subsequent image increment by one.&lt;br /&gt;&lt;br /&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/1989_hoops.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;The process:&lt;/strong&gt;&amp;nbsp;&lt;/p&gt;
&lt;p&gt;I like using Python for automating things, as it seems to have libraries for most things I want to do.&amp;nbsp; So I figured it would be a good candidate for this project.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The first thing we will want to be able to define some global variables&lt;br /&gt;&lt;br /&gt;
&lt;pre class=&quot;brush: py;&quot;&gt;#paths
source_file = &apos;/tmp/sample_cards_file.pdf&apos;
out_dir = &apos;/tmp/process/&apos;

# page setup
rows = 3
cols = 3

# defines where the last run of this left off on (first item will be 1.jpg if 0)
starting_count = 0

#spacing offsets
top_offest = 0
left_offset = 260
bottom_offset = 0
right_offset = 0
spacer = 40
vert_spacer=0&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;We will need to do is convert the each page of the pdf to a jpeg file.&lt;br /&gt;&lt;br /&gt;Python has a library called pdf2image to do this:&lt;br /&gt;&lt;br /&gt;
&lt;pre class=&quot;brush: py;&quot;&gt;pip install pdf2image
pip install poppler

# I used this syntax instead do of pip when running this in a jupyter notebook
# conda install -c conda-forge pdf2image
# conda install -c conda-forge poppler

from pdf2image import convert_from_path

# function that converts multipage pdf to individual
# jpeg images. Function returns list of image paths.
def convert_pdf_to_jpegs(pdf_path, out_dir):
    file_paths=[]
    pages = convert_from_path(pdf_path, 500)
    page_count = 1
    for page in pages:
        image_path=&quot;{}temp_page_{}.jpg&quot;.format(out_dir, str(page_count))
        #add to the list
        file_paths.append(image_path)
        #save converted file
        page.save(image_path, &apos;JPEG&apos;)
        page_count += 1
    return file_paths&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;We will have a function that breaks up an image with our 9 cards and save each individually to a specified directory. We will need the ability to seed the first image name. The function can use the spacing offset variables&lt;br /&gt;&lt;br /&gt;
&lt;pre class=&quot;brush: py;&quot;&gt;from PIL import Image

def split_images_from_page(image_path, out_dir, row_count, col_count, start_number):
    # opens the image file 
    Im = Image.open(image_path)

    # calculates height and width of image
    full_width = Im.width
    full_height = Im.height
    image_height = int((full_height - vert_spacer - top_offest - bottom_offset) / rows)
    image_width = int((full_width - spacer - left_offset - right_offset) / cols)
    
    image_count = 0
    row_current=1

    #iterates through rows and columns
    while row_current &amp;lt;= row_count:
        col_current = 1
        while col_current &amp;lt;= col_count:

            # calculates the coordinates on the image to crop            
            croppedIm = Im.crop((left_offset + ((col_current - 1) * image_width) + spacer, top_offest + ((row_current - 1) * image_height), min(left_offset + (col_current * image_width) + ((col_current - 1) * spacer), Im.width), top_offest + (row_current * image_height) + vert_spacer))

            # if you wanted to resize the image to 300 width and 420 height 
            # croppedIm = croppedIm.resize((300, 420))
            
            # saves the image to specified directory
            croppedIm.save(&quot;{}/{}.jpg&quot;.format(out_dir, start_number+image_count))
       
            col_current += 1
            image_count += 1        
        row_current+=1&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;Calling the functions:&lt;br /&gt;&lt;br /&gt;
&lt;pre class=&quot;brush: py;&quot;&gt;import os

page_count = 1

# convert pdf to jpeg file for each page
file_paths = convert_pdf_to_jpegs(source_file, out_dir)

# split each page into images
for file_path in file_paths:
    number_start = starting_count + (page_count-1) * (rows * cols) + 1 
    split_images_from_page(file_path, out_dir, rows, cols, number_start)
    page_count += 1

# clean up delete jpeg files for each page
for file_path in file_paths:
    os.remove(file_path)&lt;/pre&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;
&lt;p&gt;Here is my ipython notebook with the code detailed above:&amp;nbsp;&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;/assets/files/pdf_scan_process.ipynb&quot; target=&quot;_blank&quot;&gt;pdf_scan_process.ipynb&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Tue, 28 Jul 2020 00:00:00 -0400</pubDate>
        <link>http://localhost:4000/image-processing-python/</link>
        <guid isPermaLink="true">http://localhost:4000/image-processing-python/</guid>
        
        <category>python</category>
        
        <category>scans</category>
        
        <category>automation</category>
        
        
        <category>code</category>
        
      </item>
    
  </channel>
</rss>
